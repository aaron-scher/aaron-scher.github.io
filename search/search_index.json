{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome","text":"<p>A place for my future self to quickly get back up to speed (and hopefully be useful to others).</p>"},{"location":"#physics-quantum","title":"Physics &amp; Quantum","text":"<ul> <li>Foundations of Quantum Mechanics</li> <li>Quantum Basis Representation</li> <li>Quantum Operators and Measurement</li> <li>Entanglement Isn't Just for Spin</li> <li>Velocity in Quantum Mechanics</li> <li>Why Massive Particles Stay Localized</li> <li>Band Theory of Solids</li> <li>Electrons in Metals: From Bloch Waves to Wave Packets</li> <li>Phonons: What They Are and How They Hit Electrons</li> </ul>"},{"location":"#electronics-energy","title":"Electronics &amp; Energy","text":"<ul> <li>Voltaic Cells: How Batteries Work</li> </ul>"},{"location":"#programming-low-level-computing","title":"Programming &amp; Low-Level Computing","text":"<ul> <li>ARM Assembly: Fundamentals</li> <li>ARM Assembly: Control Flow &amp; Functions</li> <li>C Toolchain: From Source to Binary</li> <li>C Memory Model: Objects &amp; Pointers</li> <li>Organizing C Programs: Scope, Linkage &amp; Storage</li> <li>Visual Reference: ASCII Diagrams</li> </ul>"},{"location":"#dsp-teaching-materials","title":"DSP Teaching Materials","text":"<ul> <li>DSP Lab Materials \u2014 Five hands-on labs covering signals, sampling, convolution, filtering, and DTMF tone generation/detection.</li> </ul>"},{"location":"arm-assembly/arm-assembly-control-flow/","title":"ARM Assembly: Control Flow &amp; Functions","text":"<p>Continuing the Olive Stem ARM Assembly tutorial series (tutorials 6-10), covering branching, loops, conditional execution, functions, and the stack.</p> <p>Try it yourself</p> <p>Open the ARM Emulator and paste any code example below to run it.</p>"},{"location":"arm-assembly/arm-assembly-control-flow/#1-comparisons-branching","title":"1. Comparisons &amp; Branching","text":"<p> Watch Tutorial 6</p> <p><code>CMP</code> compares two values by subtracting them and setting the CPSR flags \u2014 but it discards the result. Branch instructions then check those flags to decide whether to jump.</p> GAS<pre><code>.global _start\n_start:\n    MOV R0, #1\n    MOV R1, #2\n\n    CMP R0, R1           @ Computes R0 - R1, sets flags, discards result\n\n    BGT greater          @ Branch if R0 &gt; R1 (signed: Z=0 and N=V)\n\n    BAL default          @ Branch always (unconditional jump)\n\ngreater:\n    MOV R2, #1\n\ndefault:\n    MOV R2, #2\n</code></pre> <p>This is the ARM equivalent of an if/else in C:</p> C<pre><code>// C equivalent\nif (R0 &gt; R1) {\n    R2 = 1;       // greater\n} else {\n    R2 = 2;       // default\n}\n</code></pre>"},{"location":"arm-assembly/arm-assembly-control-flow/#branch-conditions","title":"Branch Conditions","text":"Instruction Meaning Flags Checked C Equivalent <code>BGT</code> Branch if Greater Than Z=0 and N=V <code>if (a &gt; b)</code> <code>BGE</code> Branch if Greater or Equal N=V <code>if (a &gt;= b)</code> <code>BLT</code> Branch if Less Than N!=V <code>if (a &lt; b)</code> <code>BLE</code> Branch if Less or Equal Z=1 or N!=V <code>if (a &lt;= b)</code> <code>BEQ</code> Branch if Equal Z=1 <code>if (a == b)</code> <code>BNE</code> Branch if Not Equal Z=0 <code>if (a != b)</code> <code>BAL</code> Branch Always (none) <code>goto label</code> <p>CMP is just subtraction</p> <p><code>CMP R0, R1</code> is identical to <code>SUBS</code>, except the result is thrown away. Only the flags matter. This is why you need the <code>S</code> suffix on arithmetic instructions if you want to branch based on results \u2014 without flags, branches have nothing to check.</p> <p>Try it: Change the values of R0 and R1 and predict which branch will be taken. Verify in the emulator by stepping through.</p>"},{"location":"arm-assembly/arm-assembly-control-flow/#2-loops","title":"2. Loops","text":"<p> Watch Tutorial 7</p> <p>Loops in assembly are just branches that jump backwards. The pattern is: do work, check a condition, branch back to the top if not done.</p>"},{"location":"arm-assembly/arm-assembly-control-flow/#example-summing-a-list","title":"Example: Summing a List","text":"<p>Here are three versions of the same program, each handling the end-of-list check differently.</p>"},{"location":"arm-assembly/arm-assembly-control-flow/#version-1-using-equ-constant-and-explicit-endlist-in-memory","title":"Version 1 \u2014 Using <code>.equ</code> constant and explicit endlist in memory","text":"GAS<pre><code>.global _start\n.equ endlist_value, 0xaaaaaaaa   @ Define a constant for end-of-list marker\n\n_start:\n    LDR R0, =list          @ R0 = pointer to start of list\n    LDR R3, =endlist        @ R3 = pointer to endlist marker in memory\n    LDR R3, [R3]            @ R3 = value of endlist marker (0xaaaaaaaa)\n    LDR R1, [R0]            @ R1 = first element\n    MOV R2, #0              @ R2 = sum (initialized to 0)\n    ADD R2, R2, R1          @ Add first element to sum\n\nloop:\n    LDR R1, [R0, #4]!      @ Advance pointer, load next element\n    CMP R1, R3              @ Is it the end marker?\n    BEQ exit                @ If yes, we're done\n    ADD R2, R2, R1          @ Otherwise, add to sum\n    BAL loop                @ Repeat\n\nexit:\n\n.data\nlist:\n    .word 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n\nendlist:\n    .word endlist_value     @ End marker stored in memory\n</code></pre>"},{"location":"arm-assembly/arm-assembly-control-flow/#version-2-using-equ-directly-no-endlist-in-data","title":"Version 2 \u2014 Using <code>.equ</code> directly (no endlist in .data)","text":"GAS<pre><code>.global _start\n.equ endlist, 0xaaaaaaaa    @ End marker as assembler constant\n\n_start:\n    LDR R0, =list\n    LDR R3, =endlist        @ R3 = 0xaaaaaaaa (loaded directly)\n    LDR R1, [R0]\n    ADD R2, R2, R1\n\nloop:\n    LDR R1, [R0, #4]!\n    CMP R1, R3\n    BEQ exit\n    ADD R2, R2, R1\n    BAL loop\n\nexit:\n\n.data\nlist:\n    .word 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n</code></pre>"},{"location":"arm-assembly/arm-assembly-control-flow/#version-3-endlist-explicitly-in-memory-no-equ","title":"Version 3 \u2014 Endlist explicitly in memory (no <code>.equ</code>)","text":"GAS<pre><code>.global _start\n\n_start:\n    LDR R0, =list\n    LDR R3, =endlist        @ R3 = address of endlist\n    LDR R3, [R3]            @ R3 = value at endlist (0xaaaaaaaa)\n    LDR R1, [R0]\n    MOV R2, #0\n    ADD R2, R2, R1\n\nloop:\n    LDR R1, [R0, #4]!\n    CMP R1, R3\n    BEQ exit\n    ADD R2, R2, R1\n    BAL loop\n\nexit:\n\n.data\nlist:\n    .word 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n\nendlist:\n    .word 0xaaaaaaaa\n</code></pre>"},{"location":"arm-assembly/arm-assembly-control-flow/#the-loop-pattern","title":"The Loop Pattern","text":"<p>All three versions follow the same structure, which maps to a C <code>while</code> loop:</p> C<pre><code>// C equivalent\nint list[] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10};\nint sum = 0;\nint *ptr = list;\nwhile (*ptr != SENTINEL) {\n    sum += *ptr;\n    ptr++;\n}\n</code></pre> <p>Sentinel values</p> <p>The value <code>0xAAAAAAAA</code> is used as an end-of-list sentinel because uninitialized memory in the emulator defaults to this value. In a real program, you'd typically use the list length instead. The <code>.equ</code> directive creates a named constant at assembly time \u2014 similar to <code>#define</code> in C.</p> <p>Try it: Add more numbers to the list and verify the sum in R2. Try removing the endlist marker \u2014 what happens?</p>"},{"location":"arm-assembly/arm-assembly-control-flow/#3-conditional-execution","title":"3. Conditional Execution","text":"<p> Watch Tutorial 8</p> <p>ARM has a feature that most architectures don't: you can make any instruction conditional by adding a condition suffix. This avoids the overhead of branching for simple if/else logic.</p>"},{"location":"arm-assembly/arm-assembly-control-flow/#the-old-way-with-branches","title":"The Old Way (with branches)","text":"GAS<pre><code>.global _start\n_start:\n    MOV R0, #2\n    MOV R1, #4\n    CMP R0, R1\n\n    BLT addR2            @ Branch if R0 &lt; R1\n    BAL exit\n\naddR2:\n    ADD R2, #1\n\nexit:\n</code></pre>"},{"location":"arm-assembly/arm-assembly-control-flow/#the-better-way-conditional-execution","title":"The Better Way (conditional execution)","text":"GAS<pre><code>.global _start\n_start:\n    MOV R0, #2\n    MOV R1, #4\n    CMP R0, R1\n\n    ADDLT R2, #1         @ Add 1 to R2 ONLY IF R0 &lt; R1\n</code></pre>"},{"location":"arm-assembly/arm-assembly-control-flow/#another-example","title":"Another Example","text":"GAS<pre><code>.global _start\n_start:\n    MOV R0, #6\n    MOV R1, #4\n    CMP R0, R1\n\n    MOVGE R2, #5         @ Move 5 into R2 ONLY IF R0 &gt;= R1\n</code></pre>"},{"location":"arm-assembly/arm-assembly-control-flow/#why-arm-can-do-this","title":"Why ARM Can Do This","text":"<p>Every ARM instruction has a 4-bit condition field in its encoding. This is called predicated execution. Instead of branching (which flushes the pipeline and costs cycles), the CPU simply skips the instruction if the condition isn't met. For short if/else blocks, this is more efficient.</p> Suffix Meaning Example <code>LT</code> Less than <code>ADDLT R2, #1</code> \u2014 add only if less than <code>GE</code> Greater or equal <code>MOVGE R2, #5</code> \u2014 move only if greater or equal <code>EQ</code> Equal <code>MOVEQ R2, #0</code> \u2014 move only if equal <code>NE</code> Not equal <code>ADDNE R3, R3, #1</code> \u2014 add only if not equal <code>GT</code> Greater than <code>SUBGT R4, R4, #1</code> \u2014 subtract only if greater <code>LE</code> Less or equal <code>MOVLE R5, #0</code> \u2014 move only if less or equal <p>When to use conditional execution vs branches</p> <p>Use conditional suffixes for 1-2 instructions (simple if/else). Use branches for longer blocks or loops. Conditional execution saves pipeline flushes but clutters the code if overused.</p> <p>Try it: Rewrite the \"old way\" example using conditional execution. Then try the reverse: implement <code>MOVGE</code> using branches instead.</p>"},{"location":"arm-assembly/arm-assembly-control-flow/#4-functions-the-link-register","title":"4. Functions &amp; the Link Register","text":"<p> Watch Tutorial 9</p> <p><code>BL</code> (Branch with Link) is how you call a function. It works like <code>BAL</code> (branch always) but also saves the return address in the Link Register (LR). The function returns by branching back to LR with <code>BX LR</code>.</p> GAS<pre><code>.global _start\n_start:\n    MOV R0, #1\n    MOV R1, #3\n    BL add2              @ Call function: saves return address in LR\n\n    MOV R3, #4           @ This executes AFTER add2 returns\n\nadd2:\n    ADD R2, R0, R1       @ R2 = R0 + R1 = 4\n    BX LR                @ Return to caller (address stored in LR)\n</code></pre>"},{"location":"arm-assembly/arm-assembly-control-flow/#how-it-maps-to-c","title":"How It Maps to C","text":"C<pre><code>// C equivalent\nint add2(int a, int b) {\n    return a + b;\n}\n\nint main() {\n    int result = add2(1, 3);  // BL add2\n    int x = 4;                // MOV R3, #4\n}\n</code></pre> ARM x86 Meaning <code>BL label</code> <code>CALL label</code> Call function, save return address <code>BX LR</code> <code>RET</code> Return to caller <code>LR</code> (R14) Stack Where the return address is stored <p>LR gets overwritten</p> <p>If your function calls another function with <code>BL</code>, the second <code>BL</code> overwrites LR. You'll lose your return address! The solution: save LR to the stack before making nested calls (covered in the next section).</p> <p>Try it: Step through the code and watch LR update when <code>BL</code> executes. Note the address it stores \u2014 it's the instruction right after <code>BL</code>.</p>"},{"location":"arm-assembly/arm-assembly-control-flow/#5-stack-memory","title":"5. Stack Memory","text":"<p> Watch Tutorial 10</p> <p>The stack is how you preserve register values across function calls. <code>PUSH</code> saves registers onto the stack (decrements SP), and <code>POP</code> restores them (increments SP). The stack grows downward \u2014 pushing moves SP to a lower address.</p> GAS<pre><code>.global _start\n_start:\n    MOV R0, #1           @ R0 = 1\n    MOV R1, #3           @ R1 = 3\n\n    PUSH {R0, R1}        @ Save R0 and R1 onto the stack\n\n    BL get_value          @ Call function (overwrites R0, R1)\n\n    POP {R0, R1}         @ Restore original R0 and R1\n\n    B end                @ Jump to end\n\nget_value:\n    MOV R0, #5           @ R0 = 5 (overwrites the caller's value)\n    MOV R1, #7           @ R1 = 7\n    ADD R2, R0, R1       @ R2 = 12 (this is the \"return value\")\n    BX LR                @ Return to caller\n\nend:\n</code></pre>"},{"location":"arm-assembly/arm-assembly-control-flow/#what-happens-step-by-step","title":"What Happens Step by Step","text":"Step SP R0 R1 R2 Stack Contents After MOV instructions 0 1 3 0 (empty) After PUSH SP-8 1 3 0 [R0=1, R1=3] Inside get_value SP-8 5 7 12 [R0=1, R1=3] After POP 0 1 3 12 (stale data remains)"},{"location":"arm-assembly/arm-assembly-control-flow/#the-full-function-call-pattern","title":"The Full Function Call Pattern","text":"<p>This is the standard pattern for calling functions in ARM \u2014 and it's essentially what the C compiler generates:</p> GAS<pre><code>@ Caller side:\nPUSH {R0, R1}        @ 1. Save registers you need to preserve\nBL function           @ 2. Call function (sets LR)\nPOP {R0, R1}         @ 3. Restore registers after function returns\n\n@ Function side:\nfunction:\n    PUSH {LR}        @ Save return address (if making nested calls)\n    @ ... do work ...\n    POP {LR}         @ Restore return address\n    BX LR            @ Return to caller\n</code></pre> <p>C calling convention connection</p> <p>In the ARM calling convention (AAPCS), R0-R3 are used for passing arguments and return values, while R4-R11 are \"callee-saved\" \u2014 if a function uses them, it must PUSH them first and POP them before returning. This is exactly the pattern above, just standardized.</p> <p>Try it: Step through the code and watch the Stack Pointer (SP) change. After <code>PUSH</code>, check the memory at the SP address \u2014 you'll see R0 and R1's values stored there. After <code>POP</code>, note that the old data is still in memory \u2014 it's just considered \"garbage\" now.</p>"},{"location":"arm-assembly/arm-assembly-control-flow/#6-going-further","title":"6. Going Further","text":"<p>These tutorials continue the series but don't have detailed notes yet. Watch the videos to explore these topics.</p>"},{"location":"arm-assembly/arm-assembly-control-flow/#tutorial-11-hardware-interactions","title":"Tutorial 11 \u2014 Hardware Interactions","text":"<p> Watch Tutorial 11</p> <p>Interacting with hardware peripherals through memory-mapped I/O.</p>"},{"location":"arm-assembly/arm-assembly-control-flow/#tutorial-12-setting-up-qemu-for-arm","title":"Tutorial 12 \u2014 Setting up QEMU for ARM","text":"<p> Watch Tutorial 12</p> <p>Running ARM assembly on your local machine using QEMU emulation.</p>"},{"location":"arm-assembly/arm-assembly-control-flow/#tutorial-13-printing-strings-to-terminal","title":"Tutorial 13 \u2014 Printing Strings to Terminal","text":"<p> Watch Tutorial 13</p> <p>Using syscalls to output strings \u2014 the ARM equivalent of <code>printf</code>.</p>"},{"location":"arm-assembly/arm-assembly-control-flow/#tutorial-14-debugging-with-gdb","title":"Tutorial 14 \u2014 Debugging with GDB","text":"<p> Watch Tutorial 14</p> <p>Setting breakpoints, inspecting registers, and stepping through ARM assembly with GDB.</p>"},{"location":"arm-assembly/arm-assembly-control-flow/#7-quick-reference","title":"7. Quick Reference","text":""},{"location":"arm-assembly/arm-assembly-control-flow/#condition-codes","title":"Condition Codes","text":"Suffix Meaning Flags Signed/Unsigned <code>EQ</code> Equal Z=1 Either <code>NE</code> Not equal Z=0 Either <code>GT</code> Greater than Z=0, N=V Signed <code>GE</code> Greater or equal N=V Signed <code>LT</code> Less than N!=V Signed <code>LE</code> Less or equal Z=1 or N!=V Signed <code>HI</code> Higher C=1, Z=0 Unsigned <code>LS</code> Lower or same C=0 or Z=1 Unsigned <code>AL</code> Always (none) Either"},{"location":"arm-assembly/arm-assembly-control-flow/#common-patterns","title":"Common Patterns","text":"<p>If/else: GAS<pre><code>    CMP R0, R1\n    BGT if_true\n    @ else block\n    BAL end_if\nif_true:\n    @ if block\nend_if:\n</code></pre></p> <p>Loop (while-style): GAS<pre><code>loop:\n    @ ... do work ...\n    CMP R0, R1\n    BNE loop             @ Keep looping if not equal\n</code></pre></p> <p>Function call with register preservation: GAS<pre><code>    PUSH {R4-R6, LR}    @ Save callee-saved registers and return address\n    @ ... function body ...\n    POP {R4-R6, LR}     @ Restore everything\n    BX LR                @ Return\n</code></pre></p>"},{"location":"arm-assembly/arm-assembly-control-flow/#instructions-covered-in-part-2","title":"Instructions Covered in Part 2","text":"Instruction Syntax Description <code>CMP</code> <code>CMP Rn, Rm</code> Compare (subtract without storing result) <code>B</code> <code>B label</code> Unconditional branch <code>BAL</code> <code>BAL label</code> Branch always (same as B) <code>BEQ</code> <code>BEQ label</code> Branch if equal <code>BNE</code> <code>BNE label</code> Branch if not equal <code>BGT</code> <code>BGT label</code> Branch if greater than <code>BGE</code> <code>BGE label</code> Branch if greater or equal <code>BLT</code> <code>BLT label</code> Branch if less than <code>BLE</code> <code>BLE label</code> Branch if less or equal <code>BL</code> <code>BL label</code> Branch with link (function call) <code>BX</code> <code>BX LR</code> Branch to address in register (return) <code>PUSH</code> <code>PUSH {R0, R1}</code> Save registers to stack <code>POP</code> <code>POP {R0, R1}</code> Restore registers from stack"},{"location":"arm-assembly/arm-assembly-control-flow/#resources","title":"Resources","text":"<ul> <li>ARM Emulator (CPUlator)</li> <li>Azeria Labs \u2014 Writing ARM Assembly</li> <li>ARM ASM Basics</li> <li>Olive Stem YouTube Playlist</li> </ul>"},{"location":"arm-assembly/arm-assembly-fundamentals/","title":"ARM Assembly: Fundamentals","text":"<p>These notes follow the Olive Stem ARM Assembly tutorial series on YouTube (tutorials 1-5). They're designed as a quick-review reference with runnable code examples.</p> <p>Try it yourself</p> <p>Open the ARM Emulator and paste any code example below to run it.</p>"},{"location":"arm-assembly/arm-assembly-fundamentals/#1-your-first-program","title":"1. Your First Program","text":"<p> Watch Tutorial 1</p> <p>Every ARM assembly program needs a <code>_start</code> label \u2014 this is where execution begins. The <code>.global</code> directive makes <code>_start</code> visible to the linker so it knows where to begin. This is analogous to <code>int main()</code> in C: the linker needs an entry point, and <code>.global _start</code> is how you provide one.</p> GAS<pre><code>.global _start\n_start:\n    MOV R0, #30      @ Load the value 30 into register R0\n\n    MOV R7, #1       @ Syscall number 1 = exit\n    SWI 0            @ Trigger the syscall (Software Interrupt)\n</code></pre> <p>What's happening:</p> Instruction Meaning <code>.global _start</code> Export <code>_start</code> so the linker can find the entry point <code>_start:</code> Label marking where execution begins <code>MOV R0, #30</code> Move the immediate value 30 into register R0 <code>MOV R7, #1</code> Load syscall number 1 (exit) into R7 <code>SWI 0</code> Execute the syscall \u2014 program exits with code 30 <p>Why <code>SWI 0</code>?</p> <p>ARM uses software interrupts to make system calls. <code>R7</code> holds the syscall number (1 = exit), and <code>R0</code> holds the return value. This is how your assembly program talks to the operating system.</p> <p>Try it: Change <code>#30</code> to a different value and check the exit code. In the emulator, step through each instruction and watch the registers update.</p>"},{"location":"arm-assembly/arm-assembly-fundamentals/#2-memory-addressing-modes","title":"2. Memory &amp; Addressing Modes","text":"<p> Watch Tutorial 2</p> <p>ARM is a load/store architecture \u2014 you can't operate on memory directly. You must first load data into registers, work on it, then store it back. The <code>LDR</code> instruction is how you get data from memory into a register.</p>"},{"location":"arm-assembly/arm-assembly-fundamentals/#the-data-section","title":"The <code>.data</code> Section","text":"<p>The <code>.data</code> section holds your program's variables and constants. The <code>.word</code> directive allocates 32-bit integers in memory, stored sequentially.</p>"},{"location":"arm-assembly/arm-assembly-fundamentals/#addressing-modes","title":"Addressing Modes","text":"GAS<pre><code>.global _start\n\n_start:\n    LDR R0, =list       @ R0 = address of 'list' (pointer to the array)\n\n    LDR R1, [R0]        @ R1 = *R0 (load value at address in R0)\n\n    LDR R2, [R0, #4]    @ R2 = *(R0 + 4) (offset: skip 4 bytes = 1 word)\n\n    LDR R3, [R0, #4]!   @ Pre-increment: R0 += 4, then R3 = *R0\n\n    LDR R0, =list       @ Reset R0 to start of list\n\n    LDR R4, [R0], #4    @ Post-increment: R4 = *R0, then R0 += 4\n\n    MOV R7, #1\n    SWI 0\n\n.data\nlist:\n    .word 4, 5, -9, 1, 0, 2, -3\n</code></pre>"},{"location":"arm-assembly/arm-assembly-fundamentals/#addressing-modes-summary","title":"Addressing Modes Summary","text":"Mode Syntax Effect C Equivalent Register indirect <code>[R0]</code> Load from address in R0 <code>*ptr</code> Offset <code>[R0, #4]</code> Load from R0 + 4 (R0 unchanged) <code>*(ptr + 1)</code> Pre-increment <code>[R0, #4]!</code> R0 += 4, then load from R0 <code>*++ptr</code> Post-increment <code>[R0], #4</code> Load from R0, then R0 += 4 <code>*ptr++</code> <p>Why offsets are multiples of 4</p> <p>Each <code>.word</code> is 32 bits = 4 bytes. To access the second element, offset by 4. Third element: offset by 8. This is just like pointer arithmetic in C where <code>ptr + 1</code> on an <code>int*</code> advances by <code>sizeof(int)</code>.</p> <p>Try it: Step through the code in the emulator. After each <code>LDR</code>, check both the destination register and R0 \u2014 notice how pre-increment and post-increment change R0 at different times.</p>"},{"location":"arm-assembly/arm-assembly-fundamentals/#3-arithmetic-cpsr-flags","title":"3. Arithmetic &amp; CPSR Flags","text":"<p> Watch Tutorial 3</p> <p>ARM has two versions of arithmetic instructions: with and without the <code>S</code> suffix. The <code>S</code> suffix tells the CPU to update the CPSR (Current Program Status Register) flags based on the result. Without <code>S</code>, the flags are untouched.</p> GAS<pre><code>.global _start\n_start:\n    MOV R0, #5\n    MOV R1, #7\n\n    ADD R2, R0, R1       @ R2 = R0 + R1 = 12. No flags updated.\n\n    SUBS R2, R0, R1      @ R2 = R0 - R1 = -2. Flags updated!\n                         @ N flag set (result is negative)\n\n    ADDS R3, R2, R2      @ R3 = R2 + R2 = -4. Flags updated!\n                         @ Check V flag for signed overflow\n</code></pre>"},{"location":"arm-assembly/arm-assembly-fundamentals/#cpsr-flags","title":"CPSR Flags","text":"Flag Name Set When N Negative Result is negative (bit 31 = 1) Z Zero Result is exactly zero C Carry Unsigned operation overflowed (carry out of bit 31) V Overflow Signed operation overflowed (result doesn't fit in 32 bits) <p>ADD vs ADDS</p> <p><code>ADD</code> performs addition but does not update flags. <code>ADDS</code> performs the same addition and updates flags. This matters because branching instructions (covered in Part 2) check these flags. If you forget the <code>S</code>, your branches won't work as expected.</p> <p>Try it: Run the code and inspect the CPSR register after each instruction. Try <code>SUBS R2, R0, R0</code> \u2014 you should see the Z flag set.</p>"},{"location":"arm-assembly/arm-assembly-fundamentals/#4-logical-operations","title":"4. Logical Operations","text":"<p> Watch Tutorial 4</p> <p>Bitwise operations work on individual bits of a value. These are essential for working with hardware registers, flags, and bit manipulation.</p> GAS<pre><code>.global _start\n_start:\n    MOV R0, #0xFF         @ R0 = 0xFF (binary: 11111111)\n    MOV R1, #22           @ R1 = 22  (binary: 00010110)\n\n    AND R2, R1, R0        @ Bitwise AND: keep bits that are 1 in both\n    ORR R3, R1, R0        @ Bitwise OR: set bits that are 1 in either\n    EOR R4, R1, R0        @ Exclusive OR: set bits that differ\n\n    MVN R0, R0            @ Move Negation: flip all bits (~R0)\n    AND R0, R0, #0x000000FF   @ Bit mask: keep only lowest 8 bits\n</code></pre>"},{"location":"arm-assembly/arm-assembly-fundamentals/#logical-operations-truth-table","title":"Logical Operations Truth Table","text":"A B AND ORR EOR 0 0 0 0 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 <p>Bit Masking Pattern</p> <p><code>AND</code> with a mask keeps only the bits you want. For example, <code>AND R0, R0, #0xFF</code> zeroes out everything except the lowest 8 bits. This is how you extract specific fields from a hardware register or protocol word \u2014 the same idea as <code>value &amp; 0xFF</code> in C.</p> <p>Try it: Change <code>R1</code> to different values and predict the results of each operation before stepping through. Verify <code>MVN</code> flips every bit.</p>"},{"location":"arm-assembly/arm-assembly-fundamentals/#5-shifts-rotations","title":"5. Shifts &amp; Rotations","text":"<p> Watch Tutorial 5</p> <p>Shifting bits left or right is equivalent to multiplying or dividing by powers of 2 \u2014 and it's much faster than actual multiplication.</p> GAS<pre><code>.global _start\n_start:\n    @ Simple shifts\n    MOV R0, #10          @ R0 = 10 (binary: 1010)\n    LSL R0, #1           @ Shift left by 1 = multiply by 2. R0 = 20\n    LSR R0, #2           @ Shift right by 2 = divide by 4. R0 = 5\n\n    @ Move and shift (preserves original)\n    MOV R1, #10          @ R1 = 10\n    MOV R2, R1, LSL #1   @ R2 = R1 &lt;&lt; 1 = 20 (R1 stays 10)\n\n    @ Rotation vs shift\n    MOV R3, #15          @ R3 = 15 (binary: ...0000 1111)\n    ROR R3, #1           @ Rotate right: bits wrap around\n                         @ Result: 0x80000007 (MSB gets the shifted-out bit)\n\n    MOV R4, #15          @ R4 = 15\n    LSR R4, #1           @ Logical shift right: bits fall off\n                         @ Result: 0x00000007 (shifted-out bit is lost)\n</code></pre>"},{"location":"arm-assembly/arm-assembly-fundamentals/#final-register-values","title":"Final Register Values","text":"Register Value Explanation R0 <code>0x00000005</code> 10 &lt;&lt; 1 = 20, then 20 &gt;&gt; 2 = 5 R1 <code>0x0000000A</code> Unchanged (10) R2 <code>0x00000014</code> 10 &lt;&lt; 1 = 20 R3 <code>0x80000007</code> 15 rotated right 1 \u2014 bit wraps to MSB R4 <code>0x00000007</code> 15 shifted right 1 \u2014 bit lost"},{"location":"arm-assembly/arm-assembly-fundamentals/#shifts-vs-rotation","title":"Shifts vs Rotation","text":"Operation What Happens to Shifted-Out Bits Use Case <code>LSL #n</code> Discarded (zeros fill from right) Multiply by 2^n <code>LSR #n</code> Discarded (zeros fill from left) Unsigned divide by 2^n <code>ROR #n</code> Wrap around to the other side Cryptography, checksums <p>No ROL instruction</p> <p>ARM has <code>ROR</code> (rotate right) but no rotate-left instruction. This is intentional \u2014 to minimize the instruction set. You can achieve a left rotation by rotating right by (32 - n). For example, rotating left by 4 is the same as <code>ROR #28</code>.</p> <p>Try it: Experiment with <code>ROR</code> on different values. Watch how bits that \"fall off\" the right side reappear on the left \u2014 like pac-man wrapping around the screen.</p>"},{"location":"arm-assembly/arm-assembly-fundamentals/#6-quick-reference","title":"6. Quick Reference","text":""},{"location":"arm-assembly/arm-assembly-fundamentals/#instructions-covered","title":"Instructions Covered","text":"Instruction Syntax Description <code>MOV</code> <code>MOV Rd, #imm</code> / <code>MOV Rd, Rs</code> Move value into register <code>LDR</code> <code>LDR Rd, =label</code> Load address of label into register <code>LDR</code> <code>LDR Rd, [Rn]</code> Load value from memory address <code>LDR</code> <code>LDR Rd, [Rn, #off]</code> Load with offset <code>LDR</code> <code>LDR Rd, [Rn, #off]!</code> Load with pre-increment <code>LDR</code> <code>LDR Rd, [Rn], #off</code> Load with post-increment <code>ADD</code> <code>ADD Rd, Rn, Rm</code> Rd = Rn + Rm <code>SUB</code> <code>SUB Rd, Rn, Rm</code> Rd = Rn - Rm <code>AND</code> <code>AND Rd, Rn, Rm</code> Bitwise AND <code>ORR</code> <code>ORR Rd, Rn, Rm</code> Bitwise OR <code>EOR</code> <code>EOR Rd, Rn, Rm</code> Bitwise XOR <code>MVN</code> <code>MVN Rd, Rs</code> Bitwise NOT (move negated) <code>LSL</code> <code>LSL Rd, #n</code> Logical shift left (multiply by 2^n) <code>LSR</code> <code>LSR Rd, #n</code> Logical shift right (divide by 2^n) <code>ROR</code> <code>ROR Rd, #n</code> Rotate right <code>SWI</code> <code>SWI 0</code> Software interrupt (syscall)"},{"location":"arm-assembly/arm-assembly-fundamentals/#key-concepts","title":"Key Concepts","text":"Concept Meaning <code>.global _start</code> Makes entry point visible to linker <code>.data</code> Section for variables and constants <code>.word</code> Allocates 32-bit integers <code>S</code> suffix Updates CPSR flags (e.g., <code>ADDS</code>, <code>SUBS</code>) CPSR Holds N, Z, C, V flags after <code>S</code>-suffixed instructions"},{"location":"arm-assembly/arm-assembly-fundamentals/#resources","title":"Resources","text":"<ul> <li>ARM Emulator (CPUlator)</li> <li>Azeria Labs \u2014 Writing ARM Assembly</li> <li>ARM ASM Basics</li> </ul>"},{"location":"batteries/voltaic-cell-tutorial/","title":"Voltaic Cells: How Batteries Work","text":""},{"location":"batteries/voltaic-cell-tutorial/#1-introduction","title":"1. Introduction","text":"<p>A lemon, a nail, and a penny can power an LED. How?</p> <p> Figure 1: The lemon battery demonstrates all the principles of electrochemistry. The lemon provides an acidic electrolyte, while the zinc nail and copper penny serve as electrodes.</p> <p>This tutorial explains the electrochemistry behind batteries using the lemon battery as our central example. We'll introduce the key players (electrons and ions), explain what happens when metal meets electrolyte, and build up to understanding complete voltaic cells.</p> <p>Important: The lemon is NOT the energy source. The energy comes from zinc oxidation - the lemon just provides the acidic environment where the chemistry can happen.</p>"},{"location":"batteries/voltaic-cell-tutorial/#2-the-key-players","title":"2. The Key Players","text":"<p>Before diving into batteries, let's meet our cast of characters and understand their movement rules.</p>"},{"location":"batteries/voltaic-cell-tutorial/#21-electrons","title":"2.1 Electrons","text":"<p>Electrons are negatively charged particles found in the outer shells of atoms. In metals, the outermost electrons aren't bound to individual atoms - they form a \"sea\" of mobile electrons due to metallic bonding.</p> <p>Critical rule: Electrons can travel through metals but CANNOT travel through electrolytes. Why? There's an energy barrier preventing electrons from leaving the metal and entering solution. This isn't just \"difficult\" - it's energetically forbidden under normal conditions.</p>"},{"location":"batteries/voltaic-cell-tutorial/#22-ions","title":"2.2 Ions","text":"<p>Ions are atoms that have gained or lost electrons:</p> <ul> <li>Cations (+): Atoms that have lost electrons (positive charge)</li> <li>Anions (-): Atoms that have gained electrons (negative charge)</li> </ul> <p>When ionic compounds dissolve in water, they dissociate into ions:</p> <p> Figure 2: Common electrolytes dissociating into ions. NaCl \u2192 Na\u207a + Cl\u207b, H\u2082SO\u2084 \u2192 2H\u207a + SO\u2084\u00b2\u207b, KCl \u2192 K\u207a + Cl\u207b</p> <p>Critical rule: Ions CAN travel through electrolytes but CANNOT travel through metals. The crystalline structure of metals doesn't allow ions to pass through.</p>"},{"location":"batteries/voltaic-cell-tutorial/#summary-table","title":"Summary Table","text":"Player What it is Where it moves Electron (e\u207b) Negative particle Metals ONLY Cation (+) Atom missing electrons Electrolyte only Anion (-) Atom with extra electrons Electrolyte only <p>Key insight: A battery works because electrons and ions have different \"allowed paths.\" This separation is what creates useful current - electrons must go through an external wire while ions complete the circuit through the electrolyte.</p>"},{"location":"batteries/voltaic-cell-tutorial/#3-what-happens-when-metal-meets-electrolyte","title":"3. What Happens When Metal Meets Electrolyte?","text":"<p>This is the most important section. Understanding what happens at the metal-electrolyte interface is the foundation of all electrochemistry.</p> <p> Figure 3: When a metal bar is placed in an electrolyte, it spontaneously becomes negatively charged.</p>"},{"location":"batteries/voltaic-cell-tutorial/#the-key-phenomenon","title":"The Key Phenomenon","text":"<p>When you place a zinc bar in sulfuric acid, the metal bar becomes negatively charged. This happens spontaneously, with no external power source.</p>"},{"location":"batteries/voltaic-cell-tutorial/#why-does-this-happen","title":"Why Does This Happen?","text":"<p>Let's trace through what happens when zinc meets sulfuric acid:</p> <p>The starting point: The acid is already dissociated in water (this happens the moment you make the solution, before any metal is involved):</p> \\[\\text{H}_2\\text{SO}_4 \\rightarrow 2\\text{H}^+ + \\text{SO}_4^{2-}\\] <p>When zinc enters the solution:</p> <ol> <li>Zinc oxidizes - Zinc atoms leave the metal surface and enter solution as Zn\u00b2\u207a ions:</li> </ol> \\[\\text{Zn}(s) \\rightarrow \\text{Zn}^{2+}(aq) + 2e^-\\] <p>Crucially, the electrons stay behind in the metal - they cannot enter the electrolyte.</p> <ol> <li> <p>Negative charge builds - As more zinc atoms oxidize, electrons accumulate on the zinc surface. The metal becomes increasingly negatively charged.</p> </li> <li> <p>Double layer forms - The negative surface attracts positive ions (Zn\u00b2\u207a, H\u207a) from solution, forming what's called the electric double layer.</p> </li> <li> <p>Equilibrium reached - Eventually, the electric field from the accumulated charge opposes further oxidation. The system reaches equilibrium.</p> </li> </ol> <p> Figure 4: The electric double layer. Here's what you're seeing: The brown bar on the left is the metal (negatively charged). The blue \u2295 circles are all the cations in solution - this includes both the Zn\u00b2\u207a ions that dissolved from the metal AND the H\u207a ions that were already in the acid. The red \u2296 circles are anions (like SO\u2084\u00b2\u207b). Because the metal surface is negative, positive ions (cations) cluster near it - these are called \"counterions.\" The green curve shows how the electric potential drops from \u03c8\u2080 at the metal surface to zero in the bulk solution. The \"Debye length\" marks roughly where this transition happens.</p> <p>Key analogy: Metal in electrolyte acts like a \"self-charging capacitor.\" The oxidation reaction provides the energy to build up and maintain the negative charge on the metal surface.</p>"},{"location":"batteries/voltaic-cell-tutorial/#the-electrode-potential","title":"The Electrode Potential","text":"<p>Each metal-electrolyte interface develops a characteristic voltage called the standard electrode potential. For zinc, this is \\(E_{0,\\text{Zn}} = -0.76\\) V. The negative sign indicates zinc has a strong tendency to oxidize (lose electrons). We'll use this number later to calculate battery voltage.</p>"},{"location":"batteries/voltaic-cell-tutorial/#4-oxidation-and-reduction","title":"4. Oxidation and Reduction","text":"<p> Figure 5: Oxidation is loss of electrons; reduction is gain of electrons.</p>"},{"location":"batteries/voltaic-cell-tutorial/#the-oilrig-mnemonic","title":"The OILRIG Mnemonic","text":"<ul> <li>Oxidation Is Loss (of electrons)</li> <li>Reduction Is Gain (of electrons)</li> </ul>"},{"location":"batteries/voltaic-cell-tutorial/#historical-origin-of-terms","title":"Historical Origin of Terms","text":"<ul> <li> <p>\"Oxidation\" - Originally meant \"combining with oxygen.\" Why does oxygen take electrons? Oxygen atoms have 6 electrons in their outer shell but \"want\" 8 (a full shell). This makes oxygen highly electronegative - it strongly attracts electrons from other atoms. When iron rusts (Fe + O\u2082 \u2192 Fe\u2082O\u2083), the oxygen is pulling electrons away from the iron. Later, chemists realized the key event was losing electrons, whether oxygen was involved or not.</p> </li> <li> <p>\"Reduction\" - Originally a metallurgy term. An ore is rock containing metal bonded to other elements (usually oxygen) - like iron ore (Fe\u2082O\u2083) or copper ore (CuO). Ancient metalworkers would heat ore with charcoal, and the pure metal would emerge. They called this \"reduction\" because the ore's mass literally reduced (the oxygen left as CO\u2082 gas). What's happening chemically? The oxygen had been hogging the metal's electrons. When oxygen leaves, the metal gets those electrons back. So \"reduction\" = gaining electrons.</p> </li> </ul>"},{"location":"batteries/voltaic-cell-tutorial/#in-our-battery","title":"In Our Battery","text":"<p>At the anode (oxidation):</p> \\[\\text{Zn}(s) \\rightarrow \\text{Zn}^{2+}(aq) + 2e^-\\] <p>At the cathode (reduction):</p> \\[2\\text{H}^+(aq) + 2e^- \\rightarrow \\text{H}_2(g)\\] <p>Full reaction:</p> \\[\\text{Zn}(s) + 2\\text{H}^+(aq) \\rightarrow \\text{Zn}^{2+}(aq) + \\text{H}_2(g)\\]"},{"location":"batteries/voltaic-cell-tutorial/#where-does-the-energy-come-from","title":"Where Does the Energy Come From?","text":"<p>Let's build up an analogy that captures what's really happening.</p> <p>The Setup: Imagine a compressed spring that can launch balls up onto a ledge. The ledge sits above a turbine - when balls roll off, they spin the turbine and do useful work.</p> <p>The Analogy:</p> Analogy Battery Compressed spring Chemical energy stored in zinc metal Spring releases, launches ball up Zinc oxidizes: Zn \u2192 Zn\u00b2\u207a + 2e\u207b Ball lands on ledge Electron arrives at high electrical potential (in the double layer) Ball rolls off through turbine Electron flows through external circuit, doing work <p>But here's what makes this analogy actually useful - the feedback mechanism:</p> <ul> <li>As balls accumulate on the ledge, their weight presses on a lock that prevents the spring from firing</li> <li>When the ledge is \"full,\" the spring is locked - no more balls can be launched</li> <li>This is equilibrium: zinc in acid with no circuit connected. Oxidation stops because electrons have accumulated.</li> </ul> <p>Now connect the circuit:</p> <ul> <li>Balls start rolling off the ledge (electrons flow through wire)</li> <li>The ledge gets lighter \u2192 the lock releases \u2192 the spring can fire again</li> <li>More balls get launched up (more zinc oxidizes)</li> <li>This sustains the current until the spring is exhausted (zinc consumed)</li> </ul> <p>Where was the energy originally? In the compressed spring - which represents the chemical bonds of zinc metal. It took energy to produce metallic zinc in the first place (extracted from ore in a factory). That energy is now stored in the zinc, waiting to be released.</p> <p>Where does the energy go?</p> <ol> <li>First into electrical potential - the spring launches balls up to the ledge (electrons get pumped to high voltage)</li> <li>Then into useful work - balls roll through the turbine (electrons flow through your LED, motor, etc.)</li> <li>Some into heat - friction in the system (the battery warms slightly)</li> <li>Some into forming H\u2082 - at the cathode, energy goes into making hydrogen gas</li> </ol> <p>One more detail: Why can't the electron just follow the Zn\u00b2\u207a into solution? Because electrons cannot travel through electrolytes - there's an energy barrier. The electron is trapped in the metal. Its \"parent\" Zn\u00b2\u207a has left, and the electron \"wants\" to follow the positive charge, but it can only escape through the external wire. This is what creates voltage.</p> <p>(The Zn\u00b2\u207a does meet negative charges in solution - water molecules and anions surround and stabilize it. This \"solvation energy\" is part of why the reaction is favorable. But the electron can't follow - it's stuck.)</p> <p> Figure 6: The zinc oxidizing acts as an energy source (EMF), while hydrogen reduction acts as an energy sink.</p>"},{"location":"batteries/voltaic-cell-tutorial/#5-why-zinc-and-not-copper","title":"5. Why Zinc and Not Copper?","text":"<p>Not all metals behave the same in electrolytes. Why does zinc oxidize readily while copper doesn't react with dilute acid?</p>"},{"location":"batteries/voltaic-cell-tutorial/#the-galvanic-series","title":"The Galvanic Series","text":"<p> Figure 7: The galvanic series ranks metals by their tendency to oxidize. Metals at the anodic end (top) oxidize easily; metals at the cathodic end (bottom) are \"noble\" and resist oxidation.</p> <ul> <li>Anodic end (top): Mg, Zn, Al - more reactive, easily oxidize</li> <li>Cathodic end (bottom): Cu, Ag, Au - less reactive, noble metals</li> </ul> <p>Why the difference? It comes down to how tightly each metal holds its outer electrons:</p> <ul> <li>Every metal atom has a positive nucleus pulling on its electrons</li> <li>But outer electrons are shielded from the nucleus by inner electron shells</li> <li>The balance between nuclear pull and shielding determines how easily an electron can escape</li> </ul> <p>Zinc's outer electrons sit in orbitals that are relatively exposed and loosely held - they're easy to remove. Gold's outer electrons, despite gold having way more protons, are held very tightly due to how its many electron shells are arranged (plus relativistic effects that contract its orbitals). So gold stubbornly keeps its electrons while zinc readily gives them up.</p>"},{"location":"batteries/voltaic-cell-tutorial/#zinc-vs-copper-comparison","title":"Zinc vs Copper Comparison","text":"<p> Figure 8: Zinc actively dissolves in dilute acid, forming a double layer and releasing electrons. Copper does not react.</p> Property Zinc Copper Standard potential E\u2080 -0.76 V +0.34 V In H\u2082SO\u2084 Oxidizes, forms double layer Does NOT react Battery role Anode (oxidizes) Cathode (reduction site)"},{"location":"batteries/voltaic-cell-tutorial/#why-the-difference","title":"Why the Difference?","text":"<p>This comes down to quantum mechanics and electron configuration. In metallic bonding, electrons move freely through the metal. However, there's an energy barrier preventing electrons from entering the electrolyte.</p> <p>Zinc's electronic structure makes it \"want\" to release electrons more readily than copper. This isn't simply about having more protons - it's about the energy levels of the valence electrons and how they interact with the metal's band structure.</p> <p>Key insight: Battery voltage equals the difference in standard potentials:</p> \\[V_{\\text{batt}} = |\\mathcal{E}_{\\text{Zn,ox}}| - |\\mathcal{E}_{\\text{H,red}}| = 0.76 \\text{ V}\\]"},{"location":"batteries/voltaic-cell-tutorial/#6-the-complete-voltaic-cell","title":"6. The Complete Voltaic Cell","text":""},{"location":"batteries/voltaic-cell-tutorial/#61-basic-setup","title":"6.1 Basic Setup","text":"<p> Figure 9: Basic voltaic cell setup with zinc and copper electrodes in sulfuric acid.</p> <p> Figure 10: When the circuit closes, electrons flow from zinc through the wire to copper, while ions move through the electrolyte to complete the circuit.</p> <p>When the switch closes, three things happen simultaneously (not sequentially - they all enable each other):</p> <ul> <li>Electrons flow through the wire: Zn \u2192 wire \u2192 Cu</li> <li>Reduction occurs at Cu surface: H\u207a + e\u207b \u2192 \u00bdH\u2082 (bubbles form!)</li> <li>Ions move through the electrolyte to balance the charge</li> </ul> <p>Why simultaneous? Because each process depends on the others. Electrons can only keep flowing if reduction consumes them at copper. Reduction can only continue if electrons arrive. And ions must move to prevent charge buildup that would stop everything.</p> <p> Figure 11: Ion movement in the electrolyte completes the circuit. Cations move toward the cathode; anions move toward the anode.</p>"},{"location":"batteries/voltaic-cell-tutorial/#62-the-furnace-metaphor","title":"6.2 The Furnace Metaphor","text":"<p>The voltaic cell is like a furnace where zinc is the fuel burned to furnish energy. The electrolyte is the fiery blaze facilitating transformation. Hydrogen bubbles are the exhaust. The copper electrode is the anvil - unyielding, providing structure to harness current, remaining chemically unchanged.</p>"},{"location":"batteries/voltaic-cell-tutorial/#63-circuit-representation","title":"6.3 Circuit Representation","text":"<p> Figure 12: The complete circuit has two parallel paths: electrons through the external wire/resistor (where useful work is done), and ions through the electrolyte (completing the circuit).</p> <p>Two parallel paths: - External: Electrons through wire/resistor (where work is done) - Internal: Ions through electrolyte (completes the circuit)</p> <p> Figure 13: Energy flows from zinc oxidation through the external circuit to hydrogen reduction.</p>"},{"location":"batteries/voltaic-cell-tutorial/#64-why-must-electrodes-share-electrolyte","title":"6.4 Why Must Electrodes Share Electrolyte?","text":"<p>Imagine putting the electrodes in separate containers, connected only by a wire (no shared electrolyte):</p> <ul> <li>At the zinc container: Zinc oxidizes, releasing Zn\u00b2\u207a ions into solution. The solution becomes increasingly positive.</li> <li>At the copper container: H\u207a ions get reduced to H\u2082, depleting positive ions. The solution becomes increasingly negative.</li> <li>Result: This charge imbalance creates a voltage that opposes electron flow. Current stops almost instantly!</li> </ul> <p>The electrolyte (or a salt bridge) solves this by allowing ions to flow between containers, neutralizing the charge buildup so electrons can keep flowing.</p>"},{"location":"batteries/voltaic-cell-tutorial/#65-why-does-voltage-decrease-over-time","title":"6.5 Why Does Voltage Decrease Over Time?","text":"<p>Le Chatelier's Principle: The system opposes changes to equilibrium.</p> <p>As the battery discharges: - [Zn\u00b2\u207a] increases \u2192 zinc becomes less \"willing\" to oxidize \u2192 \\(\\mathcal{E}_{\\text{Zn,ox}}\\) decreases - [H\u207a] decreases \u2192 harder to reduce hydrogen \u2192 \\(\\mathcal{E}_{\\text{H,red}}\\) decreases</p> <p>Net effect: \\(V_{\\text{batt}}\\) drops over time.</p>"},{"location":"batteries/voltaic-cell-tutorial/#66-the-cathode-reaction-why-it-matters-and-how-it-works","title":"6.6 The Cathode Reaction: Why It Matters and How It Works","text":"<p>This section answers three connected questions: 1. Why does H\u2082 form at copper instead of zinc? 2. Is the bubbling essential for the battery to work? 3. What's the difference between a battery and a capacitor?</p>"},{"location":"batteries/voltaic-cell-tutorial/#the-multi-step-mechanism-of-hydrogen-evolution","title":"The Multi-Step Mechanism of Hydrogen Evolution","text":"<p>Thermodynamics says H\u207a reduction is allowed on BOTH metals. The difference is kinetics - how fast it happens. Hydrogen evolution requires multiple steps:</p> <p>Step 1 (Volmer): H\u207a approaches the metal, receives an electron, and adsorbs:</p> \\[\\text{H}^+ + e^- + \\text{M} \\rightarrow \\text{M-H}\\] <p>Step 2 (Tafel or Heyrovsky): Adsorbed hydrogen atoms combine to form H\u2082: - Tafel: M-H + M-H \u2192 H\u2082 + 2M - Heyrovsky: M-H + H\u207a + e\u207b \u2192 H\u2082 + M</p> <p>For this to work efficiently, the metal-hydrogen bond must be \"just right\" (Sabatier principle): - Too weak \u2192 H can't adsorb (Step 1 fails) - Too strong \u2192 H won't let go (Step 2 fails)</p> <p>Zinc has the wrong bond strength. The Zn-H bond is too weak - hydrogen barely adsorbs. This gives zinc a high overpotential (~0.7 V): you'd need 0.7 V extra to force H\u2082 formation at any reasonable rate.</p> <p>Copper is closer to optimal. The Cu-H bond is strong enough for adsorption but weak enough for release. Lower overpotential (~0.5 V).</p> <p>Think of it as matchmaking: zinc can't get H\u207a and electrons together properly. Copper holds hydrogen just long enough for the chemistry to happen.</p>"},{"location":"batteries/voltaic-cell-tutorial/#why-the-reduction-reaction-is-essential","title":"Why the Reduction Reaction is Essential","text":"<p>What if electrons arrived at copper but didn't react? They'd accumulate, the electrode would go negative, and current would stop. You'd have a capacitor, not a battery.</p> <p>Double layer vs. chemical reaction - what's the difference?</p> <p>In a double layer, electrons on the metal and H\u207a ions in solution sit nanometers apart. They're close but don't combine. This acts like a tiny capacitor: - Limited capacity (~microcoulombs) - Fills in microseconds - The built-up voltage opposes further current</p> <p>In the chemical reaction, electrons and H\u207a actually combine into neutral H\u2082 that floats away. The charge is gone from the system, making room for more electrons.</p> <p>But isn't a neutral H\u2082 molecule just charges close together?</p> <p>Yes, but three things make it fundamentally different from a double layer:</p> Property Double Layer Neutral H\u2082 Molecule Separation Nanometers Fractions of an \u00c5ngstr\u00f6m (1000\u00d7 closer) Mobility Stuck at interface Diffuses away, bubbles out Field orientation All aligned \u2192 fields add up Random tumbling \u2192 fields cancel <p>The double layer creates a coherent voltage that blocks current. Neutral molecules have no net field and leave the system entirely.</p>"},{"location":"batteries/voltaic-cell-tutorial/#concentration-changes-over-time","title":"Concentration Changes Over Time","text":"<p>As the battery runs: - [H\u207a] drops (consumed at copper) - diffusion from bulk replaces it - [Zn\u00b2\u207a] rises (produced at zinc) - diffuses into bulk</p> <p>This is why voltage decreases over time (Nernst equation / Le Chatelier effects).</p>"},{"location":"batteries/voltaic-cell-tutorial/#comparison-our-cell-vs-lithium-ion-battery","title":"Comparison: Our Cell vs. Lithium-Ion Battery","text":"Zn/Cu/Acid Cell Lithium-Ion Battery Cathode reaction H\u207a + e\u207b \u2192 \u00bdH\u2082 Li\u207a + e\u207b + CoO\u2082 \u2192 LiCoO\u2082 Product fate Gas escapes Stored in electrode Rechargeable? No Yes (reverse the reaction) <p>Both require a reduction reaction that consumes electrons. The difference: our cell's product (H\u2082) leaves, so it's not rechargeable. Li-ion's product stays put, so you can reverse it.</p>"},{"location":"batteries/voltaic-cell-tutorial/#battery-vs-capacitor","title":"Battery vs. Capacitor","text":"<p>Could you skip the chemistry and just grow double layers? That's a capacitor: - Stores energy by charge separation - Low capacity, fills instantly - No sustained current</p> <p>Batteries store energy in chemical bonds. The reaction is what gives them high energy density and sustained current.</p>"},{"location":"batteries/voltaic-cell-tutorial/#7-the-lemon-battery-revisited","title":"7. The Lemon Battery Revisited","text":"<p>Let's return to our opening example (Figure 1) with new understanding.</p> <p>Components: - Anode: Zinc nail (oxidizes) - Cathode: Copper penny (reduction site) - Electrolyte: Citric acid: \\(\\text{C}_6\\text{H}_8\\text{O}_7 \\rightarrow \\text{H}^+ + \\text{C}_6\\text{H}_5\\text{O}_7^{3-}\\)</p> <p>The same principles apply: - Zinc oxidizes at the nail: \\(\\text{Zn} \\rightarrow \\text{Zn}^{2+} + 2e^-\\) - Electrons flow through external circuit - Hydrogen reduces at the copper: \\(2\\text{H}^+ + 2e^- \\rightarrow \\text{H}_2\\) - Voltage: ~0.5V typical (lower than theoretical 0.76V due to internal resistance)</p> <p>Key insight: The lemon is NOT the energy source. The energy comes from zinc oxidation - essentially recovering some of the energy that was used in industrial zinc production. The lemon merely provides the acidic environment.</p>"},{"location":"batteries/voltaic-cell-tutorial/#8-zinc-granules-experiment","title":"8. Zinc Granules Experiment","text":"<p> Figure 14: Zinc granules dissolving in dilute sulfuric acid. Note the hydrogen bubbles forming directly on the zinc surface.</p> <p>Observation: Zinc granules in H\u2082SO\u2084 dissolve with vigorous H\u2082 bubbling - but we just said zinc has high hydrogen overpotential!</p>"},{"location":"batteries/voltaic-cell-tutorial/#why-do-bubbles-form-on-zinc-itself","title":"Why Do Bubbles Form on Zinc Itself?","text":"<p>The answer is impurities, and this connects back to everything we learned in Section 6.6.</p> <p>What are impurities? Real zinc isn't pure. During mining and refining, tiny amounts of other metals get trapped in the zinc - typically iron (Fe) and copper (Cu) particles, sometimes just a few atoms wide, scattered throughout the zinc.</p> <p>Why do impurities matter? Remember: - Zinc has HIGH hydrogen overpotential (bad at making H\u2082) - Copper and iron have LOWER hydrogen overpotential (better at making H\u2082)</p> <p>The impurity particles are like tiny copper or iron electrodes embedded in the zinc surface!</p> <p> Figure 15: Impurities create microscopic galvanic cells. Each impurity site is a tiny cathode where H\u2082 can form efficiently.</p> <p>The mechanism:</p> <p>Each impurity creates a local short-circuited galvanic cell right on the zinc surface:</p> <ol> <li>At the zinc (anode): Zn \u2192 Zn\u00b2\u207a + 2e\u207b (zinc dissolves)</li> <li>Electrons flow through the metal from zinc regions to impurity sites (no external wire needed - it's all connected!)</li> <li>At the impurity (cathode): 2H\u207a + 2e\u207b \u2192 H\u2082 (bubbles form)</li> </ol> <p>The impurity site acts like a tiny copper electrode. It has the right Cu-H or Fe-H bond characteristics to catalyze the Volmer and Tafel/Heyrovsky steps efficiently. The hydrogen adsorbs, combines, and leaves as bubbles.</p> <p> Figure 16: Equivalent circuit: thousands of tiny short-circuited batteries on the zinc surface.</p> <p>Why pure zinc reacts slowly: With no impurities, there's no good site for H\u2082 to form. The zinc can oxidize a little (building up the double layer), but without somewhere for electrons to go, it stops. The high hydrogen overpotential of pure zinc (~0.7 V) almost cancels out zinc's standard potential (-0.76 V), making the net driving force tiny.</p> <p>The feedback loop that sustains the reaction:</p> <ol> <li>H\u207a reduces at impurity sites, consuming electrons from the zinc metal</li> <li>This decreases the negative charge built up on zinc</li> <li>With less charge barrier, more zinc can oxidize, releasing more electrons</li> <li>These electrons flow to impurity sites and get consumed... cycle continues</li> </ol> <p>This is why chemists use impure zinc for H\u2082 production in labs - pure zinc would barely react!</p>"},{"location":"batteries/voltaic-cell-tutorial/#9-historical-development","title":"9. Historical Development","text":""},{"location":"batteries/voltaic-cell-tutorial/#91-voltas-pile-1800","title":"9.1 Volta's Pile (1800)","text":"<p> Figure 17: Alessandro Volta's pile - the first true battery capable of producing continuous current.</p> <p>Alessandro Volta's invention revolutionized science: - Alternating discs of copper and zinc - Cardboard or cloth soaked in saltwater between each pair - Stacked to increase voltage</p> <p>Problem: H\u2082 bubbles accumulated at copper surfaces, increasing internal resistance and causing unstable output.</p>"},{"location":"batteries/voltaic-cell-tutorial/#92-daniell-cell-1836","title":"9.2 Daniell Cell (1836)","text":"<p> Figure 18: John Frederic Daniell's improved design using separate electrolytes connected by a salt bridge.</p> <p>John Frederic Daniell's solution: - Innovation: Separate electrolytes + salt bridge - ZnSO\u2084 solution on zinc side - CuSO\u2084 solution on copper side - Salt bridge allows ion flow without mixing solutions - Result: More stable, consistent voltage</p> <p>The copper ions in CuSO\u2084 get reduced instead of H\u207a, depositing copper metal and eliminating the bubble problem entirely.</p>"},{"location":"batteries/voltaic-cell-tutorial/#10-galvanic-corrosion-the-dark-side","title":"10. Galvanic Corrosion: The Dark Side","text":"<p>The same electrochemistry that powers batteries can destroy structures when it happens where you don't want it.</p> <p> Figure 19: When dissimilar metals contact in a wet environment, the more anodic metal corrodes.</p> <p> Figure 20: Classic example: aluminum structure with copper bolt. Rainwater acts as electrolyte. Aluminum (anodic) corrodes while copper (cathodic) is protected.</p> <p>Example: Aluminum sculpture with copper bolt - Rainwater = electrolyte - Al is anodic (corrodes) - white corrosion products appear - Cu is cathodic (protected) - Result: Structural failure at the joint</p> <p> Figure 21: Severe galvanic corrosion damage where dissimilar metals were in contact in a marine environment.</p> <p>Key insight: The same electrochemistry that powers batteries destroys structures when dissimilar metals contact in wet environments. The more anodic metal sacrifices itself. Engineers must carefully consider material compatibility!</p>"},{"location":"batteries/voltaic-cell-tutorial/#11-summary","title":"11. Summary","text":""},{"location":"batteries/voltaic-cell-tutorial/#component-comparison","title":"Component Comparison","text":"Component Lemon Battery Sulfuric Acid Cell Role Anode Zinc nail Zinc strip Oxidizes: Zn \u2192 Zn\u00b2\u207a + 2e\u207b Cathode Copper penny Copper strip Reduction site: 2H\u207a + 2e\u207b \u2192 H\u2082 Electrolyte Citric acid H\u2082SO\u2084 Provides H\u207a, ion transport V_batt ~0.5V ~0.76V |\\(\\mathcal{E}_{\\text{Zn,ox}}\\)| - |\\(\\mathcal{E}_{\\text{H,red}}\\)|"},{"location":"batteries/voltaic-cell-tutorial/#half-reaction-standard-potentials-25c","title":"Half-Reaction Standard Potentials (25\u00b0C)","text":"Half-Reaction E\u00b0 (V) Zn\u00b2\u207a + 2e\u207b \u21cc Zn(s) -0.7618 2H\u207a + 2e\u207b \u21cc H\u2082(g) 0.00 (by definition)"},{"location":"batteries/voltaic-cell-tutorial/#key-concepts","title":"Key Concepts","text":"<ol> <li>Metal really becomes negatively charged - electrons physically accumulate, creating a real surface charge and electric field</li> <li>\"Self-charging capacitor\" - metal in electrolyte spontaneously builds up charge</li> <li>Furnace metaphor - zinc is fuel, H\u2082 is exhaust, copper is the anvil</li> <li>Thermodynamics vs kinetics - \"is it allowed?\" vs \"how fast?\"</li> <li>H\u2082 on Cu is kinetics - copper has lower activation energy, not that zinc \"can't\" reduce H\u207a</li> <li>Le Chatelier's principle - explains voltage drop over time</li> <li>Energy source is zinc - the lemon just provides acidic environment</li> <li>Electrodes must share electrolyte - or charge buildup stops current</li> </ol>"},{"location":"batteries/voltaic-cell-tutorial/#12-references","title":"12. References","text":""},{"location":"batteries/voltaic-cell-tutorial/#video-resources","title":"Video Resources","text":"<ul> <li>Reaction of Zinc with Dilute Sulfuric acid - YouTube</li> <li>Dilute acid, zinc and copper make an electric cell - YouTube</li> <li>Daniell Cell - YouTube</li> </ul>"},{"location":"batteries/voltaic-cell-tutorial/#interactive-resources","title":"Interactive Resources","text":"<ul> <li>Simple Electrical Cell Animation - National MagLab</li> </ul>"},{"location":"batteries/voltaic-cell-tutorial/#academic-resources","title":"Academic Resources","text":"<ul> <li>Standard Potentials - Chemistry LibreTexts</li> <li>Batteries and Electrolytic Cells - Chemistry LibreTexts</li> <li>Electrochemistry Crash Course for Engineers - SpringerLink</li> </ul>"},{"location":"batteries/voltaic-cell-tutorial/#general-references","title":"General References","text":"<ul> <li>Lemon battery - Wikipedia</li> <li>Understanding galvanic corrosion - Canada.ca</li> </ul>"},{"location":"c-programming/c-memory-model/","title":"The C Memory Model: Objects, Pointers &amp; the Danger Zone","text":"<p>A C variable isn't a name for a value \u2014 it's a name for a place in memory. In assembly you manipulated memory addresses directly with <code>LDR</code> and <code>STR</code>. C does the same thing \u2014 it just hides the addresses behind variable names. Understanding this is the key to understanding C.</p> <p>Try it yourself</p> <p>Paste any code example into Compiler Explorer (godbolt.org) to see the generated assembly, or OnlineGDB to compile and run.</p>"},{"location":"c-programming/c-memory-model/#1-what-is-an-object-in-c","title":"1. What Is an \"Object\" in C?","text":"<p>Not the OOP kind. In C, an object is a region of storage that can hold a value. That's it.</p> <p>Consider this declaration:</p> C<pre><code>const static uint8_t a = 5;\n</code></pre> <p>This single line specifies six properties of the object:</p> Property Value What it means Name <code>a</code> How you refer to it in source code Type <code>uint8_t</code> Size (1 byte) and legal operations Address (assigned by compiler) Where it lives in memory Storage duration <code>static</code> Exists for the entire program Linkage internal Only visible in this file Mutability <code>const</code> Cannot be modified after initialization <p>The mailbox analogy</p> <p>Think of memory as a giant row of mailboxes. Each object gets some number of consecutive mailboxes. The variable name is the label on the first mailbox. The type tells you how many mailboxes this object uses and how to interpret what's inside.</p>"},{"location":"c-programming/c-memory-model/#2-byte-addressing-endianness","title":"2. Byte Addressing &amp; Endianness","text":"<p>Every memory address identifies one byte. A <code>uint32_t</code> occupies 4 consecutive bytes. The \"address\" of a multi-byte object is always its lowest byte address.</p> <p>The question is: which byte goes at the lowest address?</p> Byte order Lowest address holds Used by Little-endian Least significant byte (LSB) ARM, x86 Big-endian Most significant byte (MSB) Network protocols, some PowerPC <p>You can prove which byte order your system uses:</p> C<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdint.h&gt;\n\nint main() {\n    uint32_t a = 0x12345678;\n    uint8_t *p = (uint8_t *)&amp;a;\n    printf(\"First byte: 0x%x\\n\", *p);\n    // Prints 0x78 on little-endian (ARM, x86)\n    // Prints 0x12 on big-endian\n    return 0;\n}\n</code></pre> <p>This works because C lets you reinterpret memory through different pointer types \u2014 casting <code>&amp;a</code> to a <code>uint8_t *</code> lets you read individual bytes of a 4-byte integer. That's powerful and dangerous.</p>"},{"location":"c-programming/c-memory-model/#3-pointers-the-bridge-from-assembly-to-c","title":"3. Pointers: The Bridge from Assembly to C","text":"<p>A pointer is an address. That's literally all it is. If you understood <code>LDR</code> and <code>STR</code> in the ARM tutorials, you already understand pointers.</p> ARM Assembly C Meaning <code>LDR R0, =list</code> <code>int *ptr = list;</code> Get the address of something <code>LDR R1, [R0]</code> <code>x = *ptr;</code> Dereference \u2014 read the value at that address <code>LDR R2, [R0, #4]</code> <code>x = *(ptr + 1);</code> Offset \u2014 read from address + offset <code>STR R1, [R0]</code> <code>*ptr = x;</code> Store \u2014 write a value to that address <p>The <code>*</code> operator in C is just <code>LDR</code>/<code>STR</code> in disguise. The <code>&amp;</code> operator gives you the address \u2014 what <code>LDR R0, =label</code> does.</p> C<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n    int x = 42;\n    int *ptr = &amp;x;      // ptr holds the address of x\n    printf(\"%d\\n\", *ptr); // dereference: read the value at that address \u2192 42\n    *ptr = 99;           // store: write 99 to that address\n    printf(\"%d\\n\", x);   // x is now 99\n    return 0;\n}\n</code></pre>"},{"location":"c-programming/c-memory-model/#4-arrays-arent-what-you-think","title":"4. Arrays Aren't What You Think","text":"<p>Arrays are not first-class in C. You can't assign one array to another, pass an array to a function, or return an array. When you try, C silently converts the array to a pointer to element 0. This is called decay.</p> C<pre><code>#include &lt;stdio.h&gt;\n\nvoid print_size(int arr[]) {\n    // arr has decayed to a pointer \u2014 sizeof gives pointer size, not array size!\n    printf(\"Inside function: %lu\\n\", sizeof(arr));  // 8 (pointer size on 64-bit)\n}\n\nint main() {\n    int data[] = {1, 2, 3, 4, 5};\n    printf(\"In main: %lu\\n\", sizeof(data));  // 20 (5 ints \u00d7 4 bytes)\n    print_size(data);\n    return 0;\n}\n</code></pre> <p>The indexing operator <code>arr[i]</code> is literally defined as <code>*(arr + i)</code> \u2014 pointer arithmetic with syntactic sugar. This even means <code>arr[3]</code> and <code>3[arr]</code> are the same thing (though please don't write it that way).</p> <p>sizeof trap</p> <p><code>sizeof(arr)</code> gives the total array size only when <code>arr</code> is a real array (not a decayed pointer). Inside a function that receives an array parameter, <code>sizeof</code> gives you the pointer size. This is one of the most common C bugs.</p>"},{"location":"c-programming/c-memory-model/#5-structs-the-arrow-operator","title":"5. Structs &amp; the Arrow Operator","text":"<p>Unlike arrays, structs are first-class \u2014 you can assign, pass, and return them.</p> C<pre><code>#include &lt;stdio.h&gt;\n\ntypedef struct {\n    int x;\n    int y;\n} Point;\n\nvoid print_point(Point *p) {\n    // p-&gt;x is shorthand for (*p).x \u2014 dereference, then access member\n    printf(\"(%d, %d)\\n\", p-&gt;x, p-&gt;y);\n}\n\nint main() {\n    Point a = {3, 7};\n    Point b = a;         // struct assignment: copies all fields\n    b.x = 10;\n    print_point(&amp;a);     // (3, 7) \u2014 a is unchanged\n    print_point(&amp;b);     // (10, 7)\n    return 0;\n}\n</code></pre> <p>The arrow operator <code>-&gt;</code> is shorthand for \"dereference the pointer, then access the member\":</p> Syntax Meaning When to use <code>point.x</code> Access member directly When you have the struct itself <code>p-&gt;x</code> Same as <code>(*p).x</code> When you have a pointer to the struct"},{"location":"c-programming/c-memory-model/#6-why-c-is-dangerous","title":"6. Why C Is Dangerous","text":"<p>C operates outside the sandbox. There is no runtime checking, no garbage collector, no bounds checking. You are one abstraction level above the hardware:</p> <ul> <li>You can read/write any memory address \u2014 buffer overflows and dangling pointers are trivially easy</li> <li>You can forget to free heap memory \u2014 memory leaks accumulate silently</li> <li>Behavior depends on compiler and architecture \u2014 endianness, struct padding, and optimization level can all change program behavior</li> <li>The compiler won't catch out-of-bounds access or uninitialized reads \u2014 these are undefined behavior, not compile errors</li> </ul> <p>This isn't a flaw \u2014 it's a design choice</p> <p>C was built to be a \"portable assembler.\" The danger is the point: you're one abstraction level above the hardware. Every guardrail would cost performance. Languages like Rust add safety guarantees, but at the cost of complexity. C trusts you \u2014 for better or worse.</p>"},{"location":"c-programming/c-memory-model/#resources","title":"Resources","text":"<ul> <li>Compiler Explorer (godbolt.org) \u2014 see generated assembly for any C code</li> <li>OnlineGDB C Compiler \u2014 compile and run C online</li> <li>ARM Assembly: Fundamentals \u2014 the <code>LDR</code>/<code>STR</code> instructions behind every pointer operation</li> <li>ARM Assembly: Control Flow &amp; Functions \u2014 stack and function calls that C generates for you</li> </ul>"},{"location":"c-programming/c-program-structure/","title":"Organizing C Programs: Scope, Linkage &amp; Storage Duration","text":"<p>\"The real insight into C is understanding these three attributes: scope, linkage, and storage duration.\" \u2014 Dan Saks</p> <p>When you write assembly, every label and every piece of data is visible to the whole program. C adds rules about who can see what and how long things last. These rules are what <code>static</code>, <code>extern</code>, and header files are really about.</p> <p>Try it yourself</p> <p>Paste any code example into Compiler Explorer (godbolt.org) to see the generated assembly, or OnlineGDB to compile and run.</p> <p> Dan Saks \u2014 Storage Duration and Linkage</p>"},{"location":"c-programming/c-program-structure/#1-the-three-attributes","title":"1. The Three Attributes","text":"<p>Every named entity in C has up to three attributes:</p> Attribute Question it answers Analogy Scope Where in the source code can you use this name? Which rooms in the building can you enter Linkage Can other files refer to this same entity? Whether people in other buildings know your name Storage duration How long does the memory exist? Your employment contract length <p>Scope is visibility to the compiler. Linkage is visibility to the linker. Storage duration is the object's lifetime at runtime.</p>"},{"location":"c-programming/c-program-structure/#2-scope-block-vs-file","title":"2. Scope: Block vs File","text":"<p>C has two levels of scope:</p> Scope Where declared Visible to Block Inside <code>{}</code> Only code within that block File Outside any function Code from the declaration to end of file C<pre><code>int count = 0;          // file scope \u2014 visible from here to end of file\n\nvoid increment() {\n    int step = 1;       // block scope \u2014 only visible inside this function\n    count += step;\n}\n\n// step is NOT accessible here\n// count IS accessible here\n</code></pre> <p>There is no \\\"global scope\\\" in C</p> <p>File scope is the biggest it gets. What people call \"global variables\" are really file-scope variables with external linkage. The mechanism for sharing across files is linkage, not scope.</p>"},{"location":"c-programming/c-program-structure/#3-storage-duration","title":"3. Storage Duration","text":"<p>Storage duration determines when memory is allocated and freed:</p> Duration Lifetime Where it lives How you get it Automatic Function entry \u2192 function exit Stack Local variables (default) Static Program start \u2192 program end Data segment <code>static</code> keyword, or file-scope variables Dynamic <code>malloc()</code> \u2192 <code>free()</code> Heap You manage it manually C<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint global = 100;               // static duration (file scope)\n\nvoid demo() {\n    int local = 1;              // automatic \u2014 destroyed when demo() returns\n    static int persistent = 0;  // static \u2014 survives across calls\n    int *heap = malloc(sizeof(int));  // dynamic \u2014 lives until you free() it\n\n    persistent++;\n    printf(\"call #%d\\n\", persistent);\n\n    free(heap);\n}\n\nint main() {\n    demo();  // prints \"call #1\"\n    demo();  // prints \"call #2\" \u2014 persistent survived!\n    return 0;\n}\n</code></pre> <p>Connection to ARM tutorials</p> <p>Automatic storage is the same stack you saw in ARM Tutorial 10. When the compiler generates <code>PUSH {R4-R6, LR}</code> at the start of a function, it's allocating automatic storage. <code>POP</code> at the end deallocates it.</p>"},{"location":"c-programming/c-program-structure/#4-linkage-the-linkers-perspective","title":"4. Linkage: The Linker's Perspective","text":"<p>Linkage determines whether the linker can match a name across files:</p> Linkage Who can see it Default for ARM assembly equivalent External Any file in the program Functions, file-scope variables <code>.global _start</code> Internal Only this translation unit <code>static</code> file-scope declarations Labels without <code>.global</code> None Not the linker's business Local variables Stack variables <p>You already know this</p> <p>In your ARM assembly, <code>.global _start</code> gave <code>_start</code> external linkage \u2014 the linker could see it. Without <code>.global</code>, labels had internal linkage. Same concept \u2014 C just has keywords for it.</p>"},{"location":"c-programming/c-program-structure/#5-what-extern-and-static-actually-do","title":"5. What <code>extern</code> and <code>static</code> Actually Do","text":"<p>Here's the part most C tutorials get wrong: <code>static</code> and <code>extern</code> do not affect scope. They affect linkage and storage duration. Where you declare something determines scope.</p> Where declared Specifier Scope Linkage Storage Duration Outside function (none) file external static Outside function <code>static</code> file internal static Outside function <code>extern</code> file external static Inside function (none) block none automatic Inside function <code>static</code> block none static Inside function <code>extern</code> block external static <p>Read the table row by row. Notice:</p> <ul> <li><code>static</code> outside a function changes linkage from external to internal (hides it from other files)</li> <li><code>static</code> inside a function changes storage from automatic to static (persists across calls)</li> <li><code>extern</code> inside a function gives a local name external linkage (lets you reference something from another file)</li> </ul> <p>Rule of thumb</p> <p><code>static</code> means \"restrict visibility.\" Outside a function: restrict to this file. Inside a function: restrict the lifetime to... the whole program (counterintuitive, but that's the keyword C chose).</p>"},{"location":"c-programming/c-program-structure/#6-declarations-definitions-header-files","title":"6. Declarations, Definitions &amp; Header Files","text":"Term Meaning Example Declaration \"This exists somewhere\" <code>int sum_array(int arr[], int n);</code> Definition \"This exists right here\" <code>int sum_array(int arr[], int n) { ... }</code> Translation unit One <code>.c</code> file with all its <code>#include</code>s pasted in What the compiler actually sees <p>All definitions are declarations. Not all declarations are definitions.</p> <p>Header files exist to share declarations. They give other files the references (the <code>U</code> symbols from <code>nm</code>) so the linker can match them to definitions (the <code>T</code> symbols).</p> C<pre><code>// --- math_utils.h (declarations only) ---\n#ifndef MATH_UTILS_H    // header guard: prevent double-inclusion\n#define MATH_UTILS_H\n\nint sum_array(int arr[], int n);    // declaration \u2014 no body\nint max_array(int arr[], int n);    // declaration \u2014 no body\n\n#endif\n</code></pre> C<pre><code>// --- math_utils.c (definitions) ---\n#include \"math_utils.h\"\n\nint sum_array(int arr[], int n) {   // definition \u2014 has a body\n    int sum = 0;\n    for (int i = 0; i &lt; n; i++) sum += arr[i];\n    return sum;\n}\n\nint max_array(int arr[], int n) {   // definition \u2014 has a body\n    int max = arr[0];\n    for (int i = 1; i &lt; n; i++) {\n        if (arr[i] &gt; max) max = arr[i];\n    }\n    return max;\n}\n</code></pre> C<pre><code>// --- main.c ---\n#include &lt;stdio.h&gt;\n#include \"math_utils.h\"            // paste declarations so compiler knows the types\n\nint main() {\n    int data[] = {3, 7, 2, 9, 1};\n    printf(\"Sum: %d\\n\", sum_array(data, 5));   // linker matches this to math_utils.o\n    printf(\"Max: %d\\n\", max_array(data, 5));\n    return 0;\n}\n</code></pre> <p>Build with: <code>clang main.c math_utils.c -o program</code></p> <p>The restaurant analogy</p> <p>A header file is like a restaurant menu \u2014 it tells you what's available (declarations) but doesn't contain the food (definitions). The <code>.c</code> file is the kitchen.</p> <p>Header guards (<code>#ifndef</code> / <code>#define</code> / <code>#endif</code> or <code>#pragma once</code>) prevent the same declarations from being pasted in twice when multiple files include the same header.</p>"},{"location":"c-programming/c-program-structure/#7-quick-reference","title":"7. Quick Reference","text":""},{"location":"c-programming/c-program-structure/#the-dan-saks-table","title":"The Dan Saks Table","text":"Where declared Specifier Scope Linkage Storage Duration Outside function (none) file external static Outside function <code>static</code> file internal static Outside function <code>extern</code> file external static Inside function (none) block none automatic Inside function <code>static</code> block none static Inside function <code>extern</code> block external static"},{"location":"c-programming/c-program-structure/#key-definitions","title":"Key Definitions","text":"Term One-line definition Scope Where in source code a name is visible (block or file) Linkage Whether the linker can match a name across files (external, internal, none) Storage duration How long an object's memory exists (automatic, static, dynamic) Declaration Introduces a name \u2014 \"this exists somewhere\" Definition Creates the entity \u2014 \"this exists right here, and here's the memory / code\" Translation unit One <code>.c</code> file after preprocessing (all <code>#include</code>s expanded)"},{"location":"c-programming/c-program-structure/#resources","title":"Resources","text":"<ul> <li> Dan Saks \u2014 Storage Duration and Linkage \u2014 the talk this page is based on</li> <li>Compiler Explorer (godbolt.org) \u2014 see generated assembly for any C code</li> <li>OnlineGDB C Compiler \u2014 compile and run C online</li> <li>ARM Assembly: Fundamentals \u2014 <code>.global</code> is external linkage</li> <li>ARM Assembly: Control Flow &amp; Functions \u2014 the stack is automatic storage</li> </ul>"},{"location":"c-programming/c-toolchain/","title":"The C Toolchain: From Source to Binary","text":"<p>The compiler isn't magic \u2014 it just produces assembly, the same kind you wrote by hand in the ARM tutorials. The difference is that it also generates metadata for the linker to stitch everything together. This page covers the full pipeline from <code>.c</code> to executable, and how to inspect each stage.</p> <p>Try it yourself</p> <p>Paste any code example into Compiler Explorer (godbolt.org) to see the generated assembly side-by-side with your C code. Use OnlineGDB to compile and run.</p>"},{"location":"c-programming/c-toolchain/#1-the-pipeline","title":"1. The Pipeline","text":"<p>Most people think \"the compiler\" turns source code into an executable. It doesn't \u2014 that's four separate tools in a trenchcoat:</p> Text Only<pre><code>hello.c \u2192 [preprocessor] \u2192 hello.i \u2192 [compiler] \u2192 hello.s \u2192 [assembler] \u2192 hello.o \u2192 [linker] \u2192 hello\n</code></pre> Stage Tool Input Output What it does Preprocess <code>cpp</code> <code>.c</code> <code>.i</code> Expands macros, pastes <code>#include</code> files Compile <code>cc</code> <code>.i</code> <code>.s</code> Translates C to assembly Assemble <code>as</code> <code>.s</code> <code>.o</code> Converts assembly to machine code Link <code>ld</code> <code>.o</code> executable Resolves symbols, produces final binary <p>Key insight</p> <p>The compiler's only job is to produce assembly \u2014 the same kind of <code>.s</code> file you could write by hand. The assembler and linker are separate steps. When people say \"compile\" they usually mean all four steps, but understanding the pipeline explains most toolchain errors.</p>"},{"location":"c-programming/c-toolchain/#2-see-it-yourself-one-file-five-outputs","title":"2. See It Yourself: One File, Five Outputs","text":"<p>Here's the running example we'll use throughout \u2014 a function that sums an array and prints the result:</p> C<pre><code>#include &lt;stdio.h&gt;\n\nint sum_array(int arr[], int n) {\n    int sum = 0;\n    for (int i = 0; i &lt; n; i++) {\n        sum += arr[i];\n    }\n    return sum;\n}\n\nint main() {\n    int data[] = {1, 2, 3, 4, 5};\n    int result = sum_array(data, 5);\n    printf(\"Sum: %d\\n\", result);\n    return 0;\n}\n</code></pre> <p>You can stop <code>clang</code> at any stage of the pipeline:</p> Command Output What you get What to look for <code>clang -E hello.c -o hello.i</code> Preprocessed All <code>#include</code> content pasted in, macros expanded <code>hello.i</code> is thousands of lines \u2014 your code is at the bottom <code>clang -S hello.c -o hello.s</code> Assembly Human-readable assembly \u2014 this is what the compiler actually produces Find <code>sum_array:</code> \u2014 compare it to the assembly you'd write by hand <code>clang -c hello.c -o hello.o</code> Object file Machine code + symbol table, but not yet executable Not human-readable \u2014 use <code>nm</code> and <code>objdump</code> to inspect (next section) <code>clang hello.c -o hello</code> Executable Linked binary \u2014 all symbols resolved, ready to run <code>./hello</code> prints <code>Sum: 15</code> <code>clang -S -emit-llvm hello.c</code> LLVM IR Compiler's intermediate representation (bonus) A portable assembly-like language between C and machine code <p>Try it: Run <code>clang -S hello.c</code> and open <code>hello.s</code>. Find the <code>sum_array</code> function \u2014 you'll see the same <code>LDR</code>, <code>ADD</code>, and branch instructions you wrote in the ARM tutorials.</p>"},{"location":"c-programming/c-toolchain/#3-whats-inside-an-object-file","title":"3. What's Inside an Object File?","text":"<p>An object file (<code>.o</code>) contains machine code, but it's not runnable yet because it has unresolved references. Two tools let you peek inside:</p> <p><code>nm</code> \u2014 list symbols:</p> Text Only<pre><code>$ nm hello.o\n0000000000000000 T sum_array\n0000000000000050 T main\n                 U printf\n</code></pre> Symbol Flag Meaning <code>sum_array</code> <code>T</code> Defined here (T = text/code section) <code>main</code> <code>T</code> Defined here <code>printf</code> <code>U</code> Undefined \u2014 referenced but not defined. The linker must find it. <p><code>objdump -d</code> \u2014 disassemble:</p> Text Only<pre><code>$ objdump -d hello.o\n</code></pre> <p>This shows the machine code translated back to assembly \u2014 useful for verifying what the compiler actually generated.</p> <p>The puzzle piece analogy</p> <p>An object file is like a puzzle piece with tabs (definitions \u2014 symbols marked <code>T</code>) and holes (references \u2014 symbols marked <code>U</code>). The linker's job is to snap pieces together by matching each hole to a tab from another object file or library. <code>printf</code>'s hole gets filled by the C standard library (<code>libc</code>).</p>"},{"location":"c-programming/c-toolchain/#4-debug-symbols-how-the-debugger-maps-back-to-your-code","title":"4. Debug Symbols: How the Debugger Maps Back to Your Code","text":"<p>When you run a debugger and set a breakpoint on line 5, how does it know which machine code address that corresponds to? Answer: DWARF debug information, generated by the <code>-g</code> flag.</p> Text Only<pre><code>$ clang -g hello.c -o hello\n</code></pre> <p>The <code>-g</code> flag embeds a mapping from machine code addresses back to your source file. For <code>sum_array</code>, the debug info contains:</p> Field Value What it means Name <code>sum_array</code> Function name File/Line <code>hello.c:3</code> Where in your source it's defined Address range <code>0x0000-0x004c</code> Which machine code bytes belong to this function Return type <code>int</code> The function's return type <p>When to use <code>-g</code></p> <p>Always compile with <code>-g</code> during development. It costs nothing at runtime (debug info is metadata, not executed code). Strip it for release builds with <code>strip</code> or by omitting <code>-g</code>.</p>"},{"location":"c-programming/c-toolchain/#5-arm-toolchains","title":"5. ARM Toolchains","text":"<p>When targeting your desktop, <code>clang</code> (or <code>gcc</code>) handles everything. When targeting an ARM microcontroller like a Cortex-M, you need a cross-compiler \u2014 a compiler that runs on your machine but produces code for a different architecture.</p> Tool Desktop (native) ARM Embedded (cross) Compiler <code>clang</code> / <code>gcc</code> <code>arm-none-eabi-gcc</code> Assembler <code>as</code> <code>arm-none-eabi-as</code> Linker <code>ld</code> <code>arm-none-eabi-ld</code> Debugger <code>lldb</code> / <code>gdb</code> <code>arm-none-eabi-gdb</code> Disassembler <code>objdump</code> <code>arm-none-eabi-objdump</code> <ul> <li><code>objcopy</code> converts between binary formats (e.g., ELF to raw <code>.bin</code> for flashing to a microcontroller).</li> <li>OpenOCD bridges your debugger to the physical hardware via JTAG/SWD.</li> <li>QEMU emulates ARM hardware so you can test without a physical board.</li> </ul>"},{"location":"c-programming/c-toolchain/#6-quick-reference","title":"6. Quick Reference","text":"Command What it does When you'd use it <code>clang -E file.c</code> Preprocess only Debug macro expansion or <code>#include</code> issues <code>clang -S file.c</code> Compile to assembly See what assembly the compiler generates <code>clang -c file.c</code> Compile + assemble to <code>.o</code> Build object files for separate linking <code>clang file.c -o out</code> Full pipeline to executable Normal build <code>clang -g file.c -o out</code> Build with debug symbols Enable debugging with <code>lldb</code>/<code>gdb</code> <code>nm file.o</code> List symbols See what a file defines and references <code>objdump -d file.o</code> Disassemble Inspect generated machine code <code>clang -S -emit-llvm file.c</code> Emit LLVM IR Explore compiler internals (bonus)"},{"location":"c-programming/c-toolchain/#resources","title":"Resources","text":"<ul> <li>Compiler Explorer (godbolt.org) \u2014 see generated assembly for any C code</li> <li>OnlineGDB C Compiler \u2014 compile and run C online</li> <li>ARM Assembly: Fundamentals \u2014 the assembly you'll see in <code>clang -S</code> output</li> </ul>"},{"location":"c-programming/visual-reference/","title":"Visual Reference: C, Assembly &amp; How Computers Work","text":"<p>MCU-first mental models, with RTOS and Linux as extensions of the same ideas. Each diagram teaches one core insight.</p>"},{"location":"c-programming/visual-reference/#the-one-mental-model-start-here","title":"The One Mental Model (start here)","text":"Text Only<pre><code>C PROGRAM\n    \u2502\n    \u25bc\nASSEMBLY  \u2192  REGISTERS  \u2192  STACK / HEAP / DATA\n    \u2502\n    \u25bc\nLINKER  \u2192  FLASH + RAM LAYOUT\n    \u2502\n    \u25bc\nBARE METAL MCU          RTOS MCU              LINUX\n(main loop + ISRs)      (tasks + scheduler)   (processes + virtual memory)\n</code></pre> <p>Insight: Same ideas everywhere. What changes is who controls memory and scheduling.</p>"},{"location":"c-programming/visual-reference/#big-picture-where-computation-happens","title":"Big Picture \u2014 Where Computation Happens","text":"Text Only<pre><code>FLASH (non-volatile)                CPU                          RAM (volatile)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 .text   (code)      \u2502\u2500\u2500fetch\u2500\u25ba\u2502 REGISTERS + ALU  \u2502\u25c4\u2500LDR\u2500\u2500\u25ba\u2502 .data (init globals)\u2502\n\u2502 .rodata (consts)    \u2502        \u2502 R0..R12 SP LR PC \u2502\u2500\u2500STR\u2500\u2500\u25ba\u2502 .bss  (zero globals)\u2502\n\u2502 .data_init (values) \u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502 heap  (malloc) \u2191    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         computation HERE           \u2502 stack (locals) \u2193    \u2502\n                                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                            storage HERE\n</code></pre> <p>Insight: Compute in registers. RAM is storage. Flash holds code + initial values.</p>"},{"location":"c-programming/visual-reference/#program-memory-layout","title":"Program Memory Layout","text":"Text Only<pre><code>High addresses\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       STACK         \u2502  \u2190 locals + return addresses\n\u2502         \u2193           \u2502    grows DOWN\n\u2502                     \u2502\n\u2502      (unused)       \u2502\n\u2502                     \u2502\n\u2502         \u2191           \u2502    grows UP\n\u2502        HEAP         \u2502  \u2190 malloc / free\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        BSS          \u2502  \u2190 zeroed globals\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502       DATA          \u2502  \u2190 initialized globals\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502       TEXT          \u2502  \u2190 code (in FLASH on MCU)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nLow addresses\n</code></pre> <p>Insight: Stack and heap grow toward each other. If they meet \u2192 crash.</p>"},{"location":"c-programming/visual-reference/#how-c-becomes-assembly","title":"How C Becomes Assembly","text":"C<pre><code>int x = 5;\nint y = x + 3;\n</code></pre> GAS<pre><code>mov  r0, #5       ; x lives in a register first\nadd  r1, r0, #3   ; y computed in a register\n\n; RAM only if needed:\nstr  r0, [sp, #0] ; spill x to stack\nstr  r1, [sp, #4] ; spill y to stack\n</code></pre> <p>Insight: Variables live in REGISTERS first, RAM second.</p>"},{"location":"c-programming/visual-reference/#stack-frames-what-a-function-really-is","title":"Stack Frames (what a function really is)","text":"Text Only<pre><code>call foo()\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\npush {r4, lr}      ; save context\nsub  sp, #8        ; room for locals\n\nSP \u2192 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502   local[1]   \u2502\n     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n     \u2502   local[0]   \u2502\n     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n     \u2502   saved r4   \u2502\n     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n     \u2502   saved LR   \u2502  \u2190 return address\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nadd  sp, #8        ; free locals\npop  {r4, pc}      ; restore &amp; return\n</code></pre> <p>Insight: Stack frame = locals + saved registers + return address. That's it.</p>"},{"location":"c-programming/visual-reference/#arrays-memory-offsets","title":"Arrays = Memory Offsets","text":"Text Only<pre><code>int arr[4] = {10, 20, 30, 40};\n\nMemory:\naddr \u2192  +0    +4    +8    +12\n       \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502 10  \u2502 20  \u2502 30  \u2502 40  \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\narr[2] == *(arr + 2) == memory at (addr + 8) == 30\n</code></pre>"},{"location":"c-programming/visual-reference/#pointers-vs-values","title":"Pointers vs Values","text":"Text Only<pre><code>int x = 10;\nint *p = &amp;x;\n\nRAM:\n0x2000 \u2192 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502  10  \u2502  \u2190 x (the value)\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n0x2004 \u2192 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 2000 \u2502  \u2190 p (holds ADDRESS of x)\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n x  = 10        the value itself\n&amp;x  = 0x2000    address of x\n p  = 0x2000    p stores that address\n*p  = 10        follow the address \u2192 get 10\n</code></pre>"},{"location":"c-programming/visual-reference/#endianness","title":"Endianness","text":"Text Only<pre><code>uint32_t x = 0x12345678;\n\nLittle-endian (ARM, x86):         Big-endian (network):\naddr \u2192  +0   +1   +2   +3         addr \u2192  +0   +1   +2   +3\n       \u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2510             \u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2510\n       \u2502 78 \u2502 56 \u2502 34 \u2502 12 \u2502             \u2502 12 \u2502 34 \u2502 56 \u2502 78 \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2518             \u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2518\n        LSB first                         MSB first\n</code></pre>"},{"location":"c-programming/visual-reference/#memory-mapped-io","title":"Memory-Mapped I/O","text":"C<pre><code>#define GPIO_OUT (*(volatile uint32_t*)0x40001000)\n\nGPIO_OUT = 1;   // turn on LED\n</code></pre> Text Only<pre><code>CPU writes to 0x40001000\n          \u2502\n          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 GPIO peripheral \u2502 \u2500\u2500\u25ba LED lights\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Insight: Hardware registers are just special addresses. Pointers = hardware control.</p>"},{"location":"c-programming/visual-reference/#why-volatile-matters","title":"Why <code>volatile</code> Matters","text":"C<pre><code>volatile int flag;\n\nwhile(flag == 0) {\n   // spin\n}\n</code></pre> Text Only<pre><code>Without volatile:              With volatile:\ncompiler reads flag ONCE       compiler MUST re-read\n\u2192 infinite loop                every iteration\n\nUse volatile for:\n\u2022 Hardware registers\n\u2022 ISR-shared variables\n\u2022 Multi-threaded shared data\n</code></pre>"},{"location":"c-programming/visual-reference/#bare-metal-main-loop-vs-isrs","title":"Bare Metal \u2014 Main Loop vs ISRs","text":"Text Only<pre><code>Time \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\n\nMain:  \u2502 work \u2502 work \u2502 work \u2502 work \u2502 work \u2502 work \u2502\n\nISR:          \u25b2           \u25b2               \u25b2\n              \u2502           \u2502               \u2502\n           TIMER       UART RX         BUTTON\n</code></pre> <p>Insight: Main runs most of the time. ISRs preempt briefly, then return.</p>"},{"location":"c-programming/visual-reference/#startup-sequence-what-happens-on-reset","title":"Startup Sequence (what happens on reset)","text":"Text Only<pre><code>RESET\n  \u2502\n  \u25bc\n1) Set SP to top of RAM\n  \u2502\n  \u25bc\n2) Copy .data_init from FLASH \u2192 RAM\n  \u2502\n  \u25bc\n3) Zero out .bss in RAM\n  \u2502\n  \u25bc\n4) Call main()\n</code></pre>"},{"location":"c-programming/visual-reference/#the-build-process","title":"The Build Process","text":"Text Only<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 hello.c  \u2502\u2500\u2500\u2500\u2500\u25ba\u2502 hello.s  \u2502\u2500\u2500\u2500\u2500\u25ba\u2502 hello.o  \u2502\u2500\u2500\u2500\u2500\u25ba\u2502  hello   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n   source         assembly         object          executable\n                 (clang -S)       (clang -c)       (clang -o)\n</code></pre>"},{"location":"c-programming/visual-reference/#linking-multiple-files","title":"Linking Multiple Files","text":"Text Only<pre><code>  main.c              utils.c             math.c\n  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  #include \"h\"        #include \"h\"        #include \"h\"\n  void main();        int add();          int mul();\n      \u2502                   \u2502                   \u2502\n      \u25bc                   \u25bc                   \u25bc\n   main.o              utils.o             math.o\n      \u2502                   \u2502                   \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u25bc\n                       LINKER\n                          \u25bc\n                     program.elf\n</code></pre>"},{"location":"c-programming/visual-reference/#scope-linkage-storage-dan-saks-table","title":"Scope, Linkage &amp; Storage (Dan Saks Table)","text":"Text Only<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Where declared    \u2502 Specifier \u2502 Scope \u2502 Linkage  \u2502 Storage   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Outside function  \u2502 (none)    \u2502 file  \u2502 external \u2502 static    \u2502\n\u2502 Outside function  \u2502 static    \u2502 file  \u2502 INTERNAL \u2502 static    \u2502\n\u2502 Outside function  \u2502 extern    \u2502 file  \u2502 external \u2502 static    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Inside function   \u2502 (none)    \u2502 block \u2502 none     \u2502 AUTOMATIC \u2502\n\u2502 Inside function   \u2502 static    \u2502 block \u2502 none     \u2502 STATIC    \u2502\n\u2502 Inside function   \u2502 extern    \u2502 block \u2502 EXTERNAL \u2502 static    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nstatic outside = hide from other files\nstatic inside  = persist across calls\n</code></pre>"},{"location":"c-programming/visual-reference/#rtos-bare-metal-vs-tasks","title":"RTOS \u2014 Bare Metal vs Tasks","text":"Text Only<pre><code>BARE METAL                          RTOS\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nwhile(1) {                          Task A: sensors   (HIGH)\n   do_sensors();                    Task B: comms     (MEDIUM)\n   do_comm();                       Task C: control   (LOW)\n   do_control();\n}                                   Time \u2192\n                                    A | A | A | A | A | A\nProblem: one slow function               B    B        B\nblocks everything.                            C        C\n</code></pre> <p>Insight: RTOS replaces one big loop with many tasks + a scheduler.</p>"},{"location":"c-programming/visual-reference/#context-switch","title":"Context Switch","text":"Text Only<pre><code>Before switch:\nR0..LR, SP = Task A state\n\nKernel saves:\n[ Task A stack ]  \u2190 all registers stored\n\nKernel loads:\n[ Task B stack ]  \u2190 all registers restored\n\nCPU now runs Task B\n</code></pre> <p>Insight: Context switch = save one stack + load another.</p>"},{"location":"c-programming/visual-reference/#mcu-vs-linux","title":"MCU vs Linux","text":"Text Only<pre><code>MCU (bare metal / RTOS)        LINUX\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500     \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2022 single program               \u2022 many programs (processes)\n\u2022 one address space            \u2022 virtual memory per process\n\u2022 direct hardware access       \u2022 drivers + syscalls\n\u2022 deterministic timing         \u2022 high throughput, less deterministic\n</code></pre>"},{"location":"c-programming/visual-reference/#linux-process-memory","title":"Linux Process Memory","text":"Text Only<pre><code>Linux process memory (simplified)\n\nHigh addresses\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Kernel memory (not yours)  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502       STACK                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502       HEAP                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     .data / .bss           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502       .text                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nLow addresses\n</code></pre> <p>Insight: Same layout as MCU, but isolated per process via virtual memory.</p>"},{"location":"c-programming/visual-reference/#16-vs-32-vs-64-bit","title":"16 vs 32 vs 64 bit","text":"<p>Word size:</p> Text Only<pre><code>16-bit register:  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  up to 65,535\n32-bit register:  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  up to ~4 billion\n64-bit register:  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  ~18 quintillion\n</code></pre> <p>Address space:</p> Text Only<pre><code>16-bit:  64 KB\n32-bit:  4 GB\n64-bit:  terabytes+\n</code></pre> <p>Work per cycle:</p> Text Only<pre><code>Processing 64 bits:\n16-bit CPU:  [====][====][====][====]  4 cycles\n32-bit CPU:  [========][========]      2 cycles\n64-bit CPU:  [================]        1 cycle\n</code></pre> <p>Why embedded uses 32-bit:</p> Text Only<pre><code>Cortex-M4 (32-bit)           Desktop x86-64\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500        \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2713 fast enough                \u2713 huge RAM\n\u2713 low power                  \u2713 heavy compute\n\u2713 small &amp; cheap              \u2717 power hungry\n\n32-bit = embedded sweet spot\n64-bit = desktop sweet spot\n</code></pre>"},{"location":"c-programming/visual-reference/#arm-c-pointer-operations","title":"ARM \u2194 C Pointer Operations","text":"Text Only<pre><code>ARM Assembly              C                    Meaning\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500           \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nLDR R0, =list             int *p = list;      get address\nLDR R1, [R0]              x = *p;             dereference (read)\nSTR R1, [R0]              *p = x;             dereference (write)\nLDR R2, [R0, #4]          x = *(p + 1);       offset access\nLDR R3, [R0], #4          x = *p++;           post-increment\nLDR R4, [R0, #4]!         x = *++p;           pre-increment\n</code></pre>"},{"location":"c-programming/visual-reference/#final-mental-map","title":"Final Mental Map","text":"Text Only<pre><code>C PROGRAM\n    \u2502\n    \u25bc\nASSEMBLY  \u2192  REGISTERS  \u2192  STACK / HEAP / DATA\n    \u2502\n    \u25bc\nLINKER  \u2192  FLASH + RAM LAYOUT\n    \u2502\n    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  BARE METAL     \u2502     RTOS        \u2502     LINUX       \u2502\n\u2502  main + ISRs    \u2502  tasks + sched  \u2502 processes + VM  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"dsp/","title":"DSP Teaching Materials","text":"<p>Lab materials for a Digital Signal Processing (DSP) course. These labs were developed at Oregon Institute of Technology for EE 430.</p>"},{"location":"dsp/#labs","title":"Labs","text":"<ol> <li> <p>Working with Signals in MATLAB \u2014 Creating, visualizing, and analyzing signals; sinusoids, Fourier series, and spectral analysis with the FFT.</p> </li> <li> <p>Sampling and Aliasing \u2014 Sampling theory, the Nyquist criterion, aliasing, and reconstruction of signals.</p> </li> <li> <p>Discrete Time Systems and Convolution \u2014 LTI systems, impulse response, convolution, and digital filtering (moving-average and FIR filters).</p> </li> <li> <p>DTMF Tone and Song Generator \u2014 Dual-Tone Multi-Frequency signaling, tone generation, audio synthesis, and spectrogram analysis.</p> </li> <li> <p>DTMF Tone Sequence Detector \u2014 Designing a DTMF decoder using filter banks, signal detection, and frequency analysis.</p> </li> </ol>"},{"location":"dsp/dsp-lab1-signals/","title":"DSP Lab 1: Working with Signals in MATLAB","text":"<p>Authors: Mateo Aboy, Aaron Scher, and Joel Sprunger Oregon Institute of Technology</p> <p>Lab Materials</p> <p>This lab was developed based on coursework for EE 430. It provides hands-on experience with creating, visualizing, and analyzing signals using MATLAB.</p>"},{"location":"dsp/dsp-lab1-signals/#objectives","title":"Objectives","text":"<p>The objectives of this lab are to learn how to visualize signals in MATLAB. This requires two tasks:</p> <ol> <li>Use the functions in MATLAB to create or import signals</li> <li>Plot the signals as functions of time, with appropriate scales for the x and y axis so that the plot is meaningful and easy to see</li> </ol>"},{"location":"dsp/dsp-lab1-signals/#introduction-why-study-sinusoids","title":"Introduction: Why Study Sinusoids?","text":"<p>Sinusoids are the \"building blocks\" of all (reasonably well-behaved) signals. By adding enough sinusoids - sometimes an infinite number - with the correct amplitudes, frequencies, and relative phase angles, we can create any signal. Sinusoids are described in mathematics using the cosine function (or sin function) or by complex exponentials \\(e^{j\\theta}\\).</p> <p>In engineering, many systems, such as RLC circuits and small signal amplifiers, can be accurately described as linear time-invariant (LTI) systems. It is straightforward to show that if a sinusoidal signal is input to an LTI system, the output will always be a sinusoid with the same frequency (though, in general, the output will have a different amplitude and phase). This is illustrated in Figure 1 for a cosine function.</p> <p> Figure 1: LTI System - Sinusoid in, Sinusoid out</p> <p>This means that a 100 Hz sinusoidal signal input to an LTI system will always result in a 100 Hz sinusoid output (zero output would be regarded as a 100 Hz sinusoid with zero amplitude). If a 100 Hz sinusoid input results in a 300 Hz output then the system is not LTI. If a 100 Hz sinusoid input results in a square wave output then the system is not LTI.</p> <p>Why LTI Systems Matter</p> <p>LTI systems are generally much easier to analyze and characterize than nonlinear and time-varying systems. Thus if we can determine how an LTI system will affect sinusoids, we can determine how the system will affect any other signal by first describing the signal as a sum of sinusoids and then determining the effect of the system on the sinusoids separately. This is the motivation for working with sinusoids.</p>"},{"location":"dsp/dsp-lab1-signals/#spectrum-of-periodic-and-quasi-periodic-signals","title":"Spectrum of Periodic and Quasi-Periodic Signals","text":"<p>The frequency spectrum of a signal is the frequency content of a signal. Spectral analysis is a widely used and powerful method of data analysis, which determines the spectrum of a time-varying signal.</p> <p>The spectrum of periodic signals is usually represented as vertical lines, called line spectra, whose frequencies and amplitudes are indicated on the x and y axes, respectively. For example, suppose a signal is represented by:</p> \\[x(t) = \\cos(2\\pi 10t) + 0.5\\cos(2\\pi 15t)\\]"},{"location":"dsp/dsp-lab1-signals/#single-sided-spectrum","title":"Single-Sided Spectrum","text":"<p>The frequency spectrum can be plotted either as single-sided or double-sided. In the single-sided spectrum, the vertical lines represent the amplitudes of the sinusoids (cosines) that comprise a function.</p> <p> Figure 2: Single-sided spectrum of \\(x(t) = \\cos(2\\pi 10t) + 0.5\\cos(2\\pi 15t)\\)</p>"},{"location":"dsp/dsp-lab1-signals/#double-sided-spectrum","title":"Double-Sided Spectrum","text":"<p>The double-sided spectrum is more general than the single-sided spectrum in that it can represent the spectra of both real and complex signals. In the double-sided spectrum, the vertical lines represent the amplitudes of the complex exponentials (\\(e^{j2\\pi f_0t}\\)) that comprise a function. It results from Euler's Equation, which says that we can decompose a sinusoid into a sum of two complex exponential functions:</p> \\[A\\cos(2\\pi f_0t) = A\\frac{e^{j2\\pi f_0t} + e^{-j2\\pi f_0t}}{2}\\] <p>Looking at the equation above, we see that the amplitude of the double-sided spectrum for \\(A\\cos(2\\pi f_0t)\\) is \\(A/2\\) at frequencies \\(f_0\\) and \\(-f_0\\).</p> <p> Figure 3: Double-sided spectrum of \\(x(t) = \\cos(2\\pi 10t) + 0.5\\cos(2\\pi 15t)\\)</p> <p>Spectrum Comparison</p> <p>The double-sided spectra for a general function \\(x(t)\\) looks just like the single-sided spectra at positive frequencies (\\(f &gt; 0\\)). The only difference is that the amplitudes of the double-sided spectrum are one-half as large (i.e. the vertical lines are one-half as long) compared to the single-sided spectrum. If a DC component exists (i.e. a component at \\(f = 0\\)) then this spectral component is identical for both single-sided and double-sided spectra.</p>"},{"location":"dsp/dsp-lab1-signals/#fundamental-frequency","title":"Fundamental Frequency","text":"<p>The fundamental frequency is the greatest common divisor of all the frequencies present in the signal. The fundamental frequency of the signal given above is 5 Hz, so \\(x(t)\\) will repeat every 1/5 = 0.2 seconds.</p> <p>Quasi-Periodic Signals:</p> <p>Non-periodic signals can have discrete line spectrum as well. These types of signals are called \"quasi-periodic\" (or sometimes \"non-periodic\" or \"almost periodic\"). If in our previous example we change the 15 Hz component to \\(\\sqrt{8}\\) Hz it will never repeat, since \\(\\sqrt{8}\\) and 10 do not share a common divisor. However, this signal is still composed of two sinusoids, which are represented as line spectra.</p> <p>On the other hand, if in our previous example we change the 15 Hz component to 15.21 Hz it will repeat, since 15.21 and 10 share 0.01 as the common divisor. In this case \\(x(t)\\) would repeat every 1/0.01 = 100 seconds.</p>"},{"location":"dsp/dsp-lab1-signals/#sampling","title":"Sampling","text":"<p>In this lab we will be creating (simulating) analog signals on a computer using MATLAB. This takes a bit of careful thinking. True analog signals are continuous signals (continuous in both amplitude and time). However, computers deal with digital signals that are both discrete in time and amplitude.</p> <p>Discrete amplitudes aren't usually a problem, since MATLAB can handle large floating point numbers. However the time instances for a signal are not selected automatically in MATLAB. Meaning that if we chose too few points in time to define our signal, it will not be plotted smoothly.</p> <p>Sampling Rate Selection</p> <p>Soon you will learn that the minimum sampling frequency that you can have for a signal is \\(2f_{max}\\), where \\(f_{max}\\) = the maximum frequency of the signal being sampled. This prevents aliasing. You need not worry about aliasing yet, just remember to select enough time samples for your signal so that the plots appear smooth. For a sinusoid a smooth plot will have a sampling frequency 25 - 100 times the frequency of the signal.</p>"},{"location":"dsp/dsp-lab1-signals/#example-1-plotting-a-sinusoid","title":"Example 1: Plotting a Sinusoid","text":"<p>Consider the following sinusoidal signal:</p> \\[x(t) = A\\cos(2\\pi f_0t + \\phi)\\] <p>We will simulate the signal with the following parameters:</p> <ul> <li>\\(A = 2\\)</li> <li>\\(f_0 = 100\\) Hz</li> <li>\\(\\phi = 0\\)</li> </ul>"},{"location":"dsp/dsp-lab1-signals/#choice-1-choose-the-time-span-of-the-plot","title":"Choice 1: Choose the Time-Span of the Plot","text":"<p>A true sinusoid is everlasting - it started at negative infinity (before the Age of the Dinosaurs) and will continue into the indefinite future forever (long after the Sun consumes the Earth). It is impossible to plot an everlasting signal, so let us choose to plot the sinusoid starting at \\(t = 0\\) and end at 3 periods (\\(3T\\)).</p> <p>This is a reasonable time span for the purposes of visual communication (it is not too \"zoomed in\" nor too \"zoomed out\"). Since \\(f_0 = 100\\) Hz, the period \\(T = 1/f_0 = 0.01\\) seconds. Hence we will plot the signal between 0 seconds and 0.03 seconds.</p>"},{"location":"dsp/dsp-lab1-signals/#choice-2-choose-the-sampling-frequency","title":"Choice 2: Choose the Sampling Frequency","text":"<p>As mentioned above, \\(x(t)\\) is a continuous time-waveform. MATLAB cannot plot continuous-time signals directly. Instead it plots the waveform at isolated (discrete) points in time. By default, MATLAB's <code>plot</code> function then automatically connects those points with straight lines.</p> <p>Here we will choose a sampling rate that is 100 times the frequency of the signal: \\(f_s = (100)(100) = 10000\\) Hz. In other words, we choose to plot the sampled continuous-time signal \\(x(t)\\) at equally spaced time instances \\(T_s = 1/f_s = 0.0001\\) seconds. With this choice, our plot will appear to the eye continuous (more-or-less).</p> Matlab<pre><code>% Example 1: Plotting a sinusoid\nA = 2;           % Amplitude\nf0 = 100;        % Frequency (Hz)\nphi = 0;         % Phase (radians)\nfs = 10000;      % Sampling frequency (Hz)\nT = 1/f0;        % Period\nt = 0:1/fs:3*T;  % Time vector (3 periods)\n\n% Generate sinusoid\nx = A * cos(2*pi*f0*t + phi);\n\n% Plot\nfigure;\nplot(t, x, 'LineWidth', 1.5);\nxlabel('Time (s)');\nylabel('Amplitude');\ntitle('Sinusoidal Signal');\ngrid on;\n</code></pre> <p> Figure 4: Plot of sinusoidal signal</p>"},{"location":"dsp/dsp-lab1-signals/#example-2-eulers-identity","title":"Example 2: Euler's Identity","text":"<p>In this example we will simulate and plot two signals to demonstrate Euler's equation:</p> \\[e^{j\\theta} = \\cos(\\theta) + j\\sin(\\theta)\\] <p>We will simulate the signal with the following parameters:</p> <ul> <li>\\(\\theta = 2\\pi f_0t + \\phi\\)</li> <li>\\(f_0 = 3\\) Hz</li> <li>\\(\\phi = 0\\)</li> </ul> Matlab<pre><code>% Example 2: Euler's Identity\nf0 = 3;              % Frequency (Hz)\nfs = 100;            % Sampling frequency (Hz)\nt = 0:1/fs:2;        % Time vector\n\n% Generate signals\ntheta = 2*pi*f0*t;\ncomplex_exp = exp(1j*theta);\neuler_sum = cos(theta) + 1j*sin(theta);\n\n% Plot\nfigure;\nplot(t, real(complex_exp), 'b-', 'LineWidth', 1.5);\nhold on;\nplot(t, real(euler_sum), 'r.', 'MarkerSize', 8);\nplot(t, imag(complex_exp), 'g-', 'LineWidth', 1.5);\nplot(t, imag(euler_sum), 'm.', 'MarkerSize', 8);\nxlabel('Time (s)');\nylabel('Amplitude');\nlegend('Real(e^{j\\theta})', 'Real(cos+jsin)', ...\n       'Imag(e^{j\\theta})', 'Imag(cos+jsin)');\ntitle('Euler''s Identity Demonstration');\ngrid on;\n</code></pre> <p> Figure 5: This plot shows the complex signal \\(e^{j\\theta} = \\cos(\\theta) + j\\sin(\\theta)\\) with \\(\\theta = 2\\pi ft = \\omega t\\). The complex exponential \\(e^{j\\theta}\\) is plotted as solid lines for real and imaginary parts, and \\(\\cos(\\theta) + j\\sin(\\theta)\\) as dots for real and imaginary parts. The solid lines and dotted lines overlap exactly, demonstrating the validity of Euler's Identity.</p>"},{"location":"dsp/dsp-lab1-signals/#example-3-signals-as-a-sum-of-sinusoids","title":"Example 3: Signals as a Sum of Sinusoids","text":"<p>In this example we demonstrate the linear combination of sinusoids that are present in a square wave. The Fourier Series of a square wave is shown below in Figure 6. Using this Fourier Series, we simulate in MATLAB how adding up various numbers of sinusoids approximates a square wave.</p> <p> Figure 6: Fourier Series of a square wave</p>"},{"location":"dsp/dsp-lab1-signals/#summing-four-harmonics","title":"Summing Four Harmonics","text":"Matlab<pre><code>% Example 3: Square wave approximation\nf0 = 1;           % Fundamental frequency (Hz)\nfs = 1000;        % Sampling frequency (Hz)\nt = 0:1/fs:2;     % Time vector\n\n% Sum first 4 harmonics\nx = zeros(size(t));\nfor n = 1:2:7     % n = 1, 3, 5, 7\n    x = x + (4/(n*pi)) * sin(2*pi*n*f0*t);\nend\n\nfigure;\nplot(t, x, 'LineWidth', 1.5);\nxlabel('Time (s)');\nylabel('Amplitude');\ntitle('Square Wave Approximation (4 harmonics)');\ngrid on;\naxis([0 2 -1.5 1.5]);\n</code></pre> <p> Figure 7: Summing the first four harmonics of a square wave</p>"},{"location":"dsp/dsp-lab1-signals/#summing-1000-harmonics","title":"Summing 1000 Harmonics","text":"Matlab<pre><code>% Sum first 1000 harmonics\nx = zeros(size(t));\nfor n = 1:2:1999  % n = 1, 3, 5, ..., 1999\n    x = x + (4/(n*pi)) * sin(2*pi*n*f0*t);\nend\n\nfigure;\nplot(t, x, 'LineWidth', 1.5);\nxlabel('Time (s)');\nylabel('Amplitude');\ntitle('Square Wave Approximation (1000 harmonics)');\ngrid on;\naxis([0 2 -1.5 1.5]);\n</code></pre> <p> Figure 8: Summing the first 1000 harmonics of a square wave</p> <p>Gibbs Phenomenon</p> <p>Even with 1000 harmonics, you can see small ripples near the discontinuities. This is called the Gibbs phenomenon and occurs when approximating discontinuous functions with Fourier series.</p>"},{"location":"dsp/dsp-lab1-signals/#example-4-using-matlabs-fft-function-to-find-the-spectrum","title":"Example 4: Using MATLAB's FFT Function to Find the Spectrum","text":"<p>Later in this course we will study the Discrete Fourier Transform (DFT) and Fast Fourier Transform (FFT) in detail. For now, let's take an initial look at how to use MATLAB's <code>fft</code> function for spectral analysis.</p> <p>Consider the sinusoidal signal:</p> \\[x(t) = \\cos(2\\pi 10t) + 0.5\\cos(2\\pi 15t)\\] Matlab<pre><code>% Example 4: FFT spectrum\nf1 = 10;          % First frequency (Hz)\nf2 = 15;          % Second frequency (Hz)\nfs = 1000;        % Sampling frequency (Hz)\nT_window = 1.3333; % Time window (s)\nt = 0:1/fs:T_window;\n\n% Generate signal\nx = cos(2*pi*f1*t) + 0.5*cos(2*pi*f2*t);\n\n% Compute FFT\nN = length(x);\nX = fft(x);\nf = (-N/2:N/2-1)*(fs/N);  % Frequency vector\nX_shifted = fftshift(X);   % Shift zero frequency to center\n\n% Plot double-sided magnitude spectrum\nfigure;\nplot(f, abs(X_shifted)/N, 'LineWidth', 1.5);\nxlabel('Frequency (Hz)');\nylabel('Magnitude');\ntitle('Double-Sided Spectrum using FFT');\ngrid on;\nxlim([-30 30]);\n</code></pre> <p> Figure 9: Spectrum of \\(x(t) = \\cos(2\\pi 10t) + 0.5\\cos(2\\pi 15t)\\) computed with MATLAB's FFT function</p> <p>Windowing Effects</p> <p>Compare the double-sided spectrum shown in Figure 9 with that shown in Figure 3. Qualitatively, they are very similar. The ripples and \"signal spread\" exhibited in Figure 9 are due to windowing, which we will study more about later in this course. A true sinusoid is everlasting, but with MATLAB we can only simulate a sinusoid that exists for a finite time. Hence the FFT of a sinusoid gives the spectrum of a truncated sinusoid (not an ideal everlasting sinusoid). If we increase the time window, the spectra will be \"sharper\" (approaching that shown in Figure 3).</p>"},{"location":"dsp/dsp-lab1-signals/#lab-tasks","title":"Lab Tasks","text":""},{"location":"dsp/dsp-lab1-signals/#report-guidelines","title":"Report Guidelines","text":"<p>For the lab report, you will create a PowerPoint presentation (or use a similar presentation program), save it as a PDF, and submit it online according to the instructions given in class. All submitted MATLAB code should be well organized and commented with clear comments for easy readability. All submitted plots should be easy to see and well-labeled.</p> <p>Independent Work</p> <p>You are to work independently. This is not a team assignment. Feel free to help your fellow classmates understand principles and concepts, but please do not share your work.</p> <p>Your presentation will have 23 slides. Please include a slide number in the footer of each slide. To earn full credit your presentation must contain the slides in the order asked for in this lab. If you miss a slide, please leave a blank slide in its place so that you still have exactly 23 slides total.</p> <p>Slide 1: Title slide with your name, student ID number, date, lab name, class number/title.</p>"},{"location":"dsp/dsp-lab1-signals/#task-1-sinusoids-and-eulers-equation","title":"Task 1: Sinusoids and Euler's Equation","text":"<p>Consider the following continuous time signal:</p> \\[x_1(t) = (1 + \\cos(2\\pi 20t))\\cos(2\\pi 100t)\\] <p>a) Use Euler's Identity to re-write the previous signal \\(x_1(t)\\) as a new signal \\(x_2(t)\\) that is a sum of three sinusoids. Show your derivation and final result in Slides 2-3.</p> <p>b) Plot \\(x_1(t)\\) in blue and \\(x_2(t)\\) in red dots on the same plot to confirm that they are the same. In your plot, choose a sampling frequency of 600 Hz over the time interval \\(0 \\leq t \\leq 0.6\\) seconds. Present your plot and accompanying MATLAB script in Slides 4-6.</p> <p>c) Use the <code>stem</code> command in MATLAB to plot the double-sided spectrum of \\(x_2(t)\\). Present your stem plot in Slide 7.</p> <p>d) Use the <code>fft</code> command in MATLAB to plot the double-sided spectrum of \\(x_1(t)\\). Present your plot in Slide 8. Compare your stem plot with your fft plot. They should be similar (in the sense that Figure 9 is similar to Figure 3).</p>"},{"location":"dsp/dsp-lab1-signals/#task-2-composing-signals-using-sinusoids","title":"Task 2: Composing Signals Using Sinusoids","text":"<p>The Fourier Series for a triangle wave is presented in Figure 10 below.</p> <p> Figure 10: Fourier Series of a triangle wave</p> <p>a) Similarly to what we did in creating Figure 7, use the <code>subplot</code> function in MATLAB to show the effect of combining the first, second, third, and fourth harmonics individually. These harmonics correspond to index values n = 1, 2, 3, and 4 in the series (in Figure 10), respectively. For this problem, choose the fundamental frequency \\(f_0 = 100\\) Hz. Plot over three periods (i.e. plot over the time interval \\(0 \\leq t \\leq 3T\\) where \\(T = 1/f_0\\)). Present your plot and accompanying MATLAB script in Slides 9-11.</p> <p>b) Use a for loop to approximate the triangle wave with 1000 harmonics. Present your plot and accompanying MATLAB script in Slides 12-13.</p>"},{"location":"dsp/dsp-lab1-signals/#task-3-periodic-and-quasi-periodic-signals","title":"Task 3: Periodic and Quasi-Periodic Signals","text":"<p>Consider the following continuous time signal:</p> \\[x(t) = \\cos(2\\pi f_1t) + \\cos(2\\pi f_2t)\\] <p>a) If \\(f_1 = 8\\) Hz and \\(f_2 = 10\\) Hz, what is the fundamental frequency \\(f_0\\) of this periodic signal? Present your derivation and answer in Slide 14. Plot this function over three periods to prove your answer. Present your plot in Slide 15.</p> <p>b) Now change the first frequency component to \\(f_1 = 5\\pi/2\\) Hz (however, let's keep \\(f_2 = 10\\) Hz the same as before). This new signal is not periodic since \\(f_1\\) and \\(f_2\\) do not share a common divisor. Plot this new signal over the same time span as in your previous plot. Present your plot in Slide 16.</p>"},{"location":"dsp/dsp-lab1-signals/#task-4-spectral-analysis","title":"Task 4: Spectral Analysis","text":"<p>\ud83d\udce5 Download: mystery_tone.mat</p> <p>a) Download the file <code>mystery_tone.mat</code>. This file contains a \"mystery tone\", which is just a simple sinusoidal tone with an amplitude of 2 and a yet-unknown frequency and time duration. The easiest way to load the file into MATLAB is to download the file to your desktop and then manually drag the file icon into MATLAB's workspace. MATLAB should then load the file. The more \"traditional\" way of loading the file is by using the <code>load</code> command.</p> <p>b) Once the file is loaded into MATLAB, you can see what variables are saved in the .mat file by simply typing <code>whos</code> in the MATLAB Workspace. You should see that there is a single variable <code>y</code> saved in the file. This variable <code>y</code> is an array of numbers - it is the \"mystery tone\"! Make a simple plot of the signal by entering in the command <code>plot(y)</code>. Present your plot in Slide 17. Since <code>plot(y)</code> does not say anything about the x-axis, the x-axis will simply be a sequence of increasing integers starting from 1 and ending at the integer corresponding to the length of the signal itself.</p> <p>c) The reason why I've been referring to this as the \"mystery tone\" is that the frequency is unknown. Right now, I have not given you enough information to determine the frequency of the tone. This signal could be a 1 Hz sinusoid or a 100 GHz sinusoid. We just don't know. As of yet, the signal is just a sequence of numbers stored in a computer that looks like a sinusoid when you plot it. The key piece of information missing is the sample rate. So let me tell you the sample rate now: the sample rate is \\(f_s = 3\\) kHz.</p> <p>d) Given the sample rate \\(f_s = 3\\) kHz, you are to plot the signal over time (using <code>plot(t,y)</code>) and determine the frequency of the tone from this plot (the time between peaks is the period T and the frequency is equal to 1/T). This will involve you creating an array <code>t</code>, which contains the discrete points in time at which the signal has been sampled. Present the frequency and accompanying plot and MATLAB script in Slides 18-20.</p> <p>e) Use MATLAB's <code>fft</code> function to determine the frequency of the tone. How does this answer compare to that determined using the method above? Present your results (including a plot of the FFT and accompanying MATLAB script) in Slides 21-23.</p>"},{"location":"dsp/dsp-lab1-signals/#key-concepts-summary","title":"Key Concepts Summary","text":""},{"location":"dsp/dsp-lab1-signals/#signal-representation","title":"Signal Representation","text":"<ul> <li>Sinusoids are the fundamental building blocks of signals</li> <li>Any signal can be represented as a sum of sinusoids (Fourier series)</li> <li>Complex exponentials (\\(e^{j\\theta}\\)) and sinusoids are related through Euler's identity</li> </ul>"},{"location":"dsp/dsp-lab1-signals/#spectrum-analysis","title":"Spectrum Analysis","text":"Type Description Use Case Single-sided Shows positive frequencies only Real-valued signals, easier visualization Double-sided Shows both positive and negative frequencies Complex signals, more general representation"},{"location":"dsp/dsp-lab1-signals/#matlab-functions","title":"MATLAB Functions","text":"Function Purpose Example <code>plot</code> Time domain plotting <code>plot(t, x)</code> <code>stem</code> Discrete spectrum plotting <code>stem(f, X)</code> <code>fft</code> Fast Fourier Transform <code>X = fft(x)</code> <code>fftshift</code> Shift zero frequency to center <code>fftshift(X)</code> <code>cos</code>, <code>sin</code> Generate sinusoids <code>cos(2*pi*f*t)</code> <code>exp</code> Generate complex exponentials <code>exp(1j*theta)</code>"},{"location":"dsp/dsp-lab1-signals/#references","title":"References","text":"<ol> <li>J. Sprunger and M. Aboy, Computer Explorations in DSP Laboratory 1 - Working with Signals in MATLAB, prepared for EE 430, Oregon Institute of Technology, 2013.</li> <li>Oppenheim &amp; Schafer, Discrete-Time Signal Processing (textbook)</li> <li>MATLAB Signal Processing Toolbox Documentation</li> </ol> <p>Lab created January 2016 | Updated for modern context February 2026</p>"},{"location":"dsp/dsp-lab2-sampling/","title":"DSP Lab 2: Sampling and Aliasing","text":"<p>Authors: Mateo Aboy, Aaron Scher, and Joel Sprunger Oregon Institute of Technology</p> <p>Lab Materials</p> <p>This lab was developed based on coursework for EE 430. It explores the fundamental concepts of sampling and reconstruction of analog signals, with a focus on understanding and preventing aliasing.</p>"},{"location":"dsp/dsp-lab2-sampling/#objectives","title":"Objectives","text":"<p>The objective of this lab is to explore the concepts of sampling and reconstruction of analog signals. Specifically, we will:</p> <ul> <li>Simulate the sampling process of an analog signal using MATLAB</li> <li>Investigate the effect of sampling in the time and frequency domains</li> <li>Introduce and understand the concept of aliasing</li> <li>Apply the Nyquist sampling theorem</li> </ul>"},{"location":"dsp/dsp-lab2-sampling/#introduction-sampling-and-aliasing","title":"Introduction: Sampling and Aliasing","text":"<p>An analog-to-digital converter (ADC) converts an analog signal to a digital form. An ADC produces a stream of binary numbers from analog signals by taking the samples of the analog signal and digitizing the amplitude at these discrete times.</p> <p>Prior to the ADC conversion, an analog filter called the prefilter or antialiasing filter is applied to the analog signal in order to deal with an effect known as aliasing. Aliasing causes multiple continuous time signals to yield the exact same sampled discrete time signal.</p>"},{"location":"dsp/dsp-lab2-sampling/#the-sampling-process","title":"The Sampling Process","text":"<p>During sampling, an analog signal \\(x_a(t)\\) is periodically measured every \\(T_s\\) seconds:</p> \\[t = nT_s, \\quad n = 0, 1, 2, \\ldots\\] <p>where \\(T_s\\) is called the sampling period, and is the fixed time interval between samples (here we assume a uniform sampling rate that does not change with time). The inverse of \\(T_s\\) is called the sampling frequency, that is, the samples per second:</p> \\[f_s = \\frac{1}{T_s}\\] <p>When sampling an analog signal we must sample fast enough (i.e. be sure \\(f_s\\) is sufficiently high), so that the samples are a good representation of the original analog signal. If the sampling frequency \\(f_s\\) is not fast enough then too much information is lost, and it becomes impossible to reconstruct our original analog signal using a digital-to-analog converter (DAC). However, if you do sample fast enough, then, theoretically, it is possible to exactly reconstruct the original signal.</p>"},{"location":"dsp/dsp-lab2-sampling/#sampling-theorem-nyquist-shannon","title":"Sampling Theorem (Nyquist-Shannon)","text":"<p>Sampling Theorem</p> <p>For accurate representation of signal \\(x_a(t)\\) by its time samples \\(x[nT]\\), two conditions must be met:</p> <ol> <li> <p>The signal \\(x_a(t)\\) must be bandlimited, that is, its frequency content (spectrum) must be limited to contain frequencies up to some maximum frequency \\(f_{max}\\) and no frequencies beyond that.</p> </li> <li> <p>The sampling rate \\(f_s\\) must be chosen to be at least twice the maximum frequency \\(f_{max}\\), that is:</p> </li> </ol> \\[f_s \\geq 2f_{max}\\] <p>According to the sampling theorem, before sampling we must make sure the signal is bandlimited (this is the function of the analog prefilter) and that the sampling frequency is at least twice the maximum frequency.</p> <p>Beyond Traditional Nyquist</p> <p>The traditional Nyquist sampling theorem presented above is true for real-valued (i.e. not complex), lowpass (i.e. baseband) signals.</p> <ul> <li>For complex lowpass signals, the sampling theorem states that the ultimate minimum sampling rate to avoid aliasing is actually \\(f_s = B\\), where \\(B\\) is the double-sided bandwidth.</li> <li>For bandpass signals, things get even more interesting. You can subsample (i.e. sample below the Nyquist rate) to achieve frequency translation to lower frequencies and recover the original signal.</li> </ul>"},{"location":"dsp/dsp-lab2-sampling/#example-1-sampling-higher-than-nyquist-rate","title":"Example 1: Sampling Higher Than Nyquist Rate","text":"<p>Consider the following continuous time signal:</p> \\[x_a(t) = 3\\sin(2\\pi 0.5t) + 2\\sin(2\\pi 2.5t)\\] <p>The signal contains two frequency components at \\(f_1 = 0.5\\) Hz and \\(f_2 = 2.5\\) Hz. The maximum frequency is \\(f_{max} = 2.5\\) Hz.</p> <p>We explore the effect of aliasing by sampling \\(x_a(t)\\) at \\(f_s = 2.5f_{max} = 6.25\\) Hz. This sampling frequency meets the sampling theorem requirement.</p>"},{"location":"dsp/dsp-lab2-sampling/#time-domain-plots","title":"Time Domain Plots","text":"Matlab<pre><code>% Example 1: Sampling above Nyquist rate\nf1 = 0.5;         % First frequency (Hz)\nf2 = 2.5;         % Second frequency (Hz)\nfmax = 2.5;       % Maximum frequency (Hz)\nfs = 2.5 * fmax;  % Sampling frequency = 6.25 Hz\n\n% Create continuous time signal (for visualization)\nt_cont = 0:0.001:10;  % Fine time resolution\nxa = 3*sin(2*pi*f1*t_cont) + 2*sin(2*pi*f2*t_cont);\n\n% Sample the signal\nTs = 1/fs;\nn = 0:floor(10/Ts);\nt_samp = n * Ts;\nx_samp = 3*sin(2*pi*f1*t_samp) + 2*sin(2*pi*f2*t_samp);\n\n% Reconstruct signal (ideal reconstruction)\nx_recon = x_samp;  % For ideal reconstruction\nt_recon = t_samp;\n\n% Plot\nfigure;\nsubplot(3,1,1);\nplot(t_cont, xa, 'LineWidth', 1.5);\nxlabel('Time (s)'); ylabel('Amplitude');\ntitle('Original Continuous Signal x_a(t)');\ngrid on;\n\nsubplot(3,1,2);\nstem(t_samp, x_samp, 'filled');\nxlabel('Time (s)'); ylabel('Amplitude');\ntitle(['Sampled Signal x[n], f_s = ', num2str(fs), ' Hz']);\ngrid on;\n\nsubplot(3,1,3);\nplot(t_cont, xa, 'b-', 'LineWidth', 1);\nhold on;\nstem(t_samp, x_samp, 'r', 'filled');\nxlabel('Time (s)'); ylabel('Amplitude');\ntitle('Sampled Signal Superimposed on Reconstructed Signal');\nlegend('Reconstructed', 'Samples');\ngrid on;\n</code></pre> <p> Figure 1: First plot shows the original continuous time signal \\(x_a(t)\\). Second plot shows the sampled discrete time signal \\(x[n]\\) with \\(f_s = 2.5f_{max} = 6.25\\) Hz. Third plot shows the sampled discrete time signal superimposed on top of the reconstructed time-domain signal.</p>"},{"location":"dsp/dsp-lab2-sampling/#frequency-domain-plots","title":"Frequency Domain Plots","text":"Matlab<pre><code>% Compute FFT of signals\nN = 2048;  % FFT length\nfreq = (-N/2:N/2-1)*(fs/N);\n\n% FFT of original signal (using high sampling rate)\nfs_high = 1000;  % High sampling for \"continuous\" approximation\nt_high = 0:1/fs_high:10;\nxa_high = 3*sin(2*pi*f1*t_high) + 2*sin(2*pi*f2*t_high);\nXa = fftshift(fft(xa_high, N));\n\n% FFT of sampled signal\nXs = fftshift(fft(x_samp, N));\n\n% Plot spectra\nfigure;\nsubplot(3,1,1);\nplot(freq, abs(Xa)/max(abs(Xa)), 'LineWidth', 1.5);\nxlabel('Frequency (Hz)'); ylabel('Normalized Magnitude');\ntitle('Spectrum of Original Signal');\ngrid on; xlim([-10 10]);\n\nsubplot(3,1,2);\nplot(freq, abs(Xs)/max(abs(Xs)), 'LineWidth', 1.5);\nxlabel('Frequency (Hz)'); ylabel('Normalized Magnitude');\ntitle(['Spectrum of Sampled Signal, f_s = ', num2str(fs), ' Hz']);\ngrid on; xlim([-10 10]);\n\nsubplot(3,1,3);\nplot(freq, abs(Xa)/max(abs(Xa)), 'LineWidth', 1.5);\nxlabel('Frequency (Hz)'); ylabel('Normalized Magnitude');\ntitle('Spectrum of Reconstructed Signal');\ngrid on; xlim([-10 10]);\n</code></pre> <p> Figure 2: First plot shows the two-sided amplitude spectrum of the original time signal. Second plot shows the two-sided amplitude spectrum of the sampled discrete time signal with \\(f_s = 2.5f_{max} = 6.25\\) Hz. Third plot shows the two-sided amplitude spectrum of the reconstructed time-domain signal.</p> <p>Perfect Reconstruction</p> <p>Note that the reconstructed time-domain signal is equal to the original time-domain signal, because the sampling theorem is met.</p>"},{"location":"dsp/dsp-lab2-sampling/#example-2-sampling-lower-than-nyquist-rate","title":"Example 2: Sampling Lower Than Nyquist Rate","text":"<p>Consider the same continuous time signal as in Example 1:</p> \\[x_a(t) = 3\\sin(2\\pi 0.5t) + 2\\sin(2\\pi 2.5t)\\] <p>The signal contains two frequency components at \\(f_1 = 0.5\\) Hz and \\(f_2 = 2.5\\) Hz. We explore the effect of aliasing by sampling \\(x_a(t)\\) at \\(f_s = 1\\) Hz. This sampling frequency does not meet the sampling theorem requirement (\\(f_s &lt; 2f_{max}\\)).</p> Matlab<pre><code>% Example 2: Sampling below Nyquist rate (undersampling)\nfs_under = 1;  % Sampling frequency = 1 Hz (violates Nyquist!)\n\n% Sample the signal\nTs_under = 1/fs_under;\nn_under = 0:floor(10/Ts_under);\nt_under = n_under * Ts_under;\nx_under = 3*sin(2*pi*f1*t_under) + 2*sin(2*pi*f2*t_under);\n\n% What we actually reconstruct (aliased version)\n% The 2.5 Hz component aliases to 0.5 Hz!\nx_recon_aliased = 3*sin(2*pi*0.5*t_cont) + 2*sin(2*pi*0.5*t_cont);\nx_recon_aliased = 5*sin(2*pi*0.5*t_cont);\n\n% Plot\nfigure;\nsubplot(4,1,1);\nplot(t_cont, xa, 'LineWidth', 1.5);\ntitle('Original Signal x_a(t)');\nxlabel('Time (s)'); ylabel('Amplitude');\ngrid on;\n\nsubplot(4,1,2);\nstem(t_under, x_under, 'filled');\ntitle(['Sampled Signal x[n], f_s = ', num2str(fs_under), ' Hz']);\nxlabel('Time (s)'); ylabel('Amplitude');\ngrid on;\n\nsubplot(4,1,3);\nplot(t_cont, xa, 'b-', 'LineWidth', 1);\nhold on;\nstem(t_under, x_under, 'r', 'filled');\ntitle('Sampled Signal on Original Signal');\nxlabel('Time (s)'); ylabel('Amplitude');\nlegend('Original', 'Samples');\ngrid on;\n\nsubplot(4,1,4);\nplot(t_cont, x_recon_aliased, 'g-', 'LineWidth', 1.5);\nhold on;\nstem(t_under, x_under, 'r', 'filled');\ntitle('Sampled Signal on Reconstructed Signal (Aliased!)');\nxlabel('Time (s)'); ylabel('Amplitude');\nlegend('Reconstructed (Wrong!)', 'Samples');\ngrid on;\n</code></pre> <p> Figure 3: First plot shows the original continuous time signal \\(x_a(t)\\). Second plot shows the sampled discrete time signal \\(x[n]\\) with \\(f_s = f_{max}/2.5 = 1\\) Hz. Third plot shows the sampled discrete time signal superimposed on top of the original time-domain signal. Fourth plot shows the sampled discrete time signal superimposed on top of the reconstructed time-domain signal.</p>"},{"location":"dsp/dsp-lab2-sampling/#aliasing-effect","title":"Aliasing Effect","text":"<p>Note that the reconstructed time-domain signal is not equal to the original time-domain signal, because the sampling theorem is not met. The reconstructed signal is:</p> \\[x_r(t) = 3\\sin(2\\pi 0.5t) + 2\\sin(2\\pi 0.5t) = 5\\sin(2\\pi 0.5t)\\] <p>The above equation results because a 0.5 Hz sinusoid is an alias of a 2.5 Hz sinusoid.</p> <p>Aliasing Explained</p> <p>When sampling at \\(f_s = 1\\) Hz, the 2.5 Hz component appears as a 0.5 Hz component in the sampled signal. This is because:</p> \\[2.5 \\mod 1 = 0.5\\] <p>The high-frequency component \"wraps around\" and masquerades as a low-frequency component. This is aliasing - different continuous-time signals produce identical discrete-time sequences.</p> <p> Figure 4: Frequency domain plots showing aliasing. The spectrum of the sampled signal shows how the 2.5 Hz component has aliased to 0.5 Hz.</p>"},{"location":"dsp/dsp-lab2-sampling/#lab-tasks","title":"Lab Tasks","text":""},{"location":"dsp/dsp-lab2-sampling/#report-guidelines","title":"Report Guidelines","text":"<p>For the lab report, you will create a PowerPoint presentation (or use a similar presentation program), save it as a PDF, and submit it online according to the instructions given in class. All submitted MATLAB code should be well organized and commented with clear comments for easy readability. All submitted plots should be easy to see and well-labeled.</p> <p>Independent Work</p> <p>You are to work independently. This is not a team assignment. Feel free to help your fellow classmates understand principles and concepts, but please do not share your work.</p> <p>Subplot Formatting</p> <p>This lab involves generating many plots using MATLAB's <code>subplot</code> function. This will likely result in figures which are \"squashed\" together. To unsquash the images, please manually stretch (resize) the figure window before placing the figures in your presentation.</p> <p>Your presentation will have 11 slides. Please include a slide number in the footer of each slide. To earn full credit your presentation must contain the slides in the order asked for in this lab. If you miss a slide, please leave a blank slide in its place so that you still have exactly 11 slides total.</p> <p>Slide 1: Title slide with your name, student ID number, date, lab name, class number/title.</p>"},{"location":"dsp/dsp-lab2-sampling/#task-1-sampling-higher-than-nyquist-rate","title":"Task 1: Sampling Higher Than Nyquist Rate","text":"<p>Consider a continuous time domain signal:</p> \\[x_a(t) = 2\\cos(2\\pi 0.5t) + \\sin(2\\pi 1.5t) + \\cos(2\\pi 2.5t) + 1.5\\cos(2\\pi 3.5t)\\] <p>a) Suppose we sample \\(x_a(t)\\) with a sampling frequency \\(f_s = 10f_{max}\\). Create a 3-by-1 plot like Example 1 where you plot the original continuous time signal \\(x_a(t)\\), the sampled discrete time signal \\(x[n]\\), and the sampled discrete time signal superimposed over the reconstructed signal. Present your plot and your MATLAB code in Slides 2-5. Please program MATLAB to display your name as the title of the plot (e.g. The MATLAB command would be something like: <code>title('Jane Doe')</code>).</p>"},{"location":"dsp/dsp-lab2-sampling/#task-2-sampling-lower-than-nyquist-rate","title":"Task 2: Sampling Lower Than Nyquist Rate","text":"<p>a) Consider the continuous time signal \\(x_a(t)\\) expressed in the equation above (Task 1). Suppose we sample \\(x_a(t)\\) with a sampling frequency \\(f_s = f_{max}\\). Create a 4-by-1 plot like Example 2 where you plot the original continuous time signal \\(x_a(t)\\), the sampled discrete time signal \\(x[n]\\), the sampled discrete time signal superimposed over the original continuous time signal, and the sampled discrete time signal superimposed over the reconstructed signal. Present your plot in Slide 6. Please program MATLAB to display your name as the title of the plot.</p>"},{"location":"dsp/dsp-lab2-sampling/#task-3-anti-aliasing-filter","title":"Task 3: Anti-Aliasing Filter","text":"<p>a) Consider the continuous time signal \\(x_a(t)\\) from Task 1. Consider again a sampling frequency \\(f_s = f_{max}\\). Suppose you first process \\(x_a(t)\\) with an ideal anti-aliasing low-pass prefilter and then sample the signal. What is the signal after the anti-aliasing low-pass prefilter? Present your expression in Slide 7.</p> <p>Create a 4-by-1 plot where you plot: 1. The original continuous time signal \\(x_a(t)\\) 2. The continuous time signal after it has been processed by the prefilter 3. The sampled discrete time signal \\(x[n]\\) 4. The sampled discrete time signal superimposed over the reconstructed signal</p> <p>Present your plot in Slide 8. Please program MATLAB to display your name as the title of the plot.</p> <p>Anti-Aliasing Filter Design</p> <p>An ideal anti-aliasing filter is a low-pass filter with cutoff frequency \\(f_c = f_s/2\\). It removes all frequency components above the Nyquist frequency before sampling.</p>"},{"location":"dsp/dsp-lab2-sampling/#task-4-the-atari-wrap-around-effect","title":"Task 4: The Atari (Wrap-Around) Effect","text":""},{"location":"dsp/dsp-lab2-sampling/#part-a-complex-signal-aliasing","title":"Part A: Complex Signal Aliasing","text":"<p>Consider a continuous time domain complex signal:</p> \\[x_a(t) = e^{j2\\pi 10t}\\] <p>Create a 4-by-1 plot where you plot the double-sided spectrum of the sampled signal for the following sampling frequencies: \\(f_s = 25\\) Hz, \\(f_s = 21\\) Hz, \\(f_s = 19\\) Hz, \\(f_s = 15\\) Hz.</p> <p>Present your plot in Slide 9. Please program MATLAB to display your name as the title of the plot. When creating your signal, sample your signal for a total of 10 seconds or more (this large sampling window will result in \"sharp\" spectral peaks). For your plot make sure to set <code>axis tight</code>.</p> Matlab<pre><code>% Complex signal aliasing example\nf0 = 10;          % Signal frequency (Hz)\nT_window = 10;    % Long window for sharp peaks\n\nfs_values = [25, 21, 19, 15];  % Different sampling rates\n\nfigure;\nfor i = 1:4\n    fs = fs_values(i);\n    t = 0:1/fs:T_window;\n    x = exp(1j*2*pi*f0*t);\n\n    % Compute FFT\n    N = length(x);\n    X = fftshift(fft(x));\n    f = (-N/2:N/2-1)*(fs/N);\n\n    subplot(4,1,i);\n    plot(f, abs(X), 'LineWidth', 1.5);\n    xlabel('Frequency (Hz)');\n    ylabel('Magnitude');\n    title(['f_s = ', num2str(fs), ' Hz']);\n    grid on;\n    axis tight;\nend\n</code></pre>"},{"location":"dsp/dsp-lab2-sampling/#part-b-real-signal-aliasing","title":"Part B: Real Signal Aliasing","text":"<p>Consider a continuous time domain signal:</p> \\[x_a(t) = \\cos(2\\pi 10t)\\] <p>Create a 4-by-1 plot where you plot the double-sided spectrum of the sampled signal for the following sampling frequencies: \\(f_s = 25\\) Hz, \\(f_s = 21\\) Hz, \\(f_s = 19\\) Hz, \\(f_s = 15\\) Hz.</p> <p>Present your plot in Slide 10. Please program MATLAB to display your name as the title of the plot. When creating your signal, sample your signal for a total of 10 seconds or more. For your plot make sure to set <code>axis tight</code>.</p>"},{"location":"dsp/dsp-lab2-sampling/#part-c-explanation","title":"Part C: Explanation","text":"<p>In Slide 11 explain what is meant by the Atari (wrap-around) effect in terms of sampling and aliasing.</p> <p>The Atari Effect</p> <p>The \"Atari effect\" or \"wagon wheel effect\" is a visual manifestation of aliasing. When a rotating object is sampled (filmed) at a rate close to its rotation frequency, it can appear to rotate slowly backwards, stand still, or rotate at the wrong speed. This is because the sampling rate is too low relative to the motion, causing aliasing in the temporal domain.</p> <p>In the frequency domain, when the sampling rate is below Nyquist, high-frequency components \"wrap around\" the Nyquist frequency and appear as low-frequency components in the sampled signal.</p>"},{"location":"dsp/dsp-lab2-sampling/#key-concepts-summary","title":"Key Concepts Summary","text":""},{"location":"dsp/dsp-lab2-sampling/#sampling-theorem","title":"Sampling Theorem","text":"Condition Requirement Effect if Violated Bandlimited signal \\(f_{max}\\) must be finite Cannot sample properly Nyquist rate \\(f_s \\geq 2f_{max}\\) Aliasing occurs Reconstruction Ideal low-pass filter Perfect recovery (if Nyquist met)"},{"location":"dsp/dsp-lab2-sampling/#aliasing-formula","title":"Aliasing Formula","text":"<p>For a sinusoid at frequency \\(f_0\\) sampled at rate \\(f_s\\), the aliased frequency is:</p> \\[f_{alias} = |f_0 - k \\cdot f_s|\\] <p>where \\(k\\) is chosen such that \\(0 \\leq f_{alias} &lt; f_s/2\\).</p>"},{"location":"dsp/dsp-lab2-sampling/#matlab-functions-for-sampling","title":"MATLAB Functions for Sampling","text":"Function Purpose Example <code>stem</code> Plot discrete samples <code>stem(n, x)</code> <code>fft</code> Frequency analysis <code>X = fft(x)</code> <code>fftshift</code> Center zero frequency <code>fftshift(X)</code> <code>resample</code> Change sampling rate <code>y = resample(x, p, q)</code> <code>decimate</code> Downsample with filtering <code>y = decimate(x, r)</code>"},{"location":"dsp/dsp-lab2-sampling/#prevention-strategies","title":"Prevention Strategies","text":"<p>Preventing Aliasing</p> <ol> <li>Analog anti-aliasing filter: Apply low-pass filter before ADC</li> <li>Oversample: Sample at \\(f_s \\gg 2f_{max}\\) (typically 5-10\u00d7 Nyquist rate)</li> <li>Bandlimit the signal: Remove high-frequency components before sampling</li> <li>Use appropriate ADC: Select ADC with sufficient sampling rate</li> </ol>"},{"location":"dsp/dsp-lab2-sampling/#downloadable-resources","title":"Downloadable Resources","text":"<ul> <li>\ud83d\udce5 Complex Signals and Radios (PDF)</li> </ul>"},{"location":"dsp/dsp-lab2-sampling/#references","title":"References","text":"<ol> <li>J. Sprunger and M. Aboy, Computer Explorations in DSP Laboratory 2 - Signals and Aliasing, prepared for EE 430, Oregon Institute of Technology, 2013.</li> <li>M. Valkama, Complex valued signals and systems - Basic principles and applications to radio communications and radio signal processing, Tampere University of Technology.</li> <li>Oppenheim &amp; Schafer, Discrete-Time Signal Processing (textbook)</li> <li>MATLAB Signal Processing Toolbox Documentation</li> </ol> <p>Lab created January 2016 | Updated for modern context February 2026</p>"},{"location":"dsp/dsp-lab3-convolution/","title":"DSP Lab 3: Discrete Time Systems and Convolution","text":"<p>Authors: Mateo Aboy, Aaron Scher, and Joel Sprunger Oregon Institute of Technology</p> <p>Lab Materials</p> <p>This lab was developed based on coursework for EE 430. It provides hands-on experience with discrete time systems, impulse response, convolution, and digital filtering using MATLAB.</p>"},{"location":"dsp/dsp-lab3-convolution/#objectives","title":"Objectives","text":"<p>The objective of this lab is to investigate discrete time systems. We will use the concepts of impulse response and convolution to introduce the concept of digital filtering.</p>"},{"location":"dsp/dsp-lab3-convolution/#introduction-development-of-the-convolution-sum","title":"Introduction: Development of the Convolution Sum","text":"<p>A discrete time system can be modelled mathematically as a transform, a function, or an operator \\(T\\), that takes or \"processes\" an input sequence \\(x[n]\\) and produces an output sequence \\(y[n]\\), that is</p> \\[y[n]=T\\left\\{x[n]\\right\\}\\] <p>Systems are divided in two broad classes: linear and nonlinear. Linear systems obey the principle of superposition:</p> \\[T\\left\\{a_1x_1[n]+a_2x_2[n]\\right\\}=a_1T\\left\\{x_1[n]\\right\\}+a_2T\\left\\{x_2[n]\\right\\}\\] <p>Linear systems can be further divided into two classes: time-invariant and time-variant.</p> <p>Time invariance means that if the input signal \\(x[n]\\) produces an output signal \\(y[n]\\) then any time-shifted signal \\(x[n-n_0]\\) results in a time shifted output \\(y[n-n_0]\\). In other words, a time-invariant system's output does not depend explicitly on time.</p> <p>Linear-time-invariant (LTI) systems are very important in practice because there is a well developed mathematical theory that enables us to analyze, design, and study these systems in great detail. From an anthropocentric perspective, LTI systems are (usually) much \"easier\" to solve and characterize compared to nonlinear systems. So we'll stick with LTI for now.</p> <p>Why LTI Systems Are Easy</p> <p>One thing that makes LTI systems \"easy\" is that we can completely characterize an LTI system by its impulse response \\(h[n]\\) (this is proven in class and in the textbook). In other words, if we have an LTI system and we want to know what the system does to a general signal, the only thing we need to do is apply a unit impulse \\(\\delta[n]\\) and record the output (by definition this output is the impulse response \\(h[n]\\)).</p> <p>Once we have \\(h[n]\\), we can use the convolution sum to find the output of the system for an arbitrary input:</p> \\[y[n]=T\\left\\{x[n]\\right\\}= \\sum_{k=-\\infty}^{\\infty} x[k]h[n-k] = \\sum_{k=-\\infty}^{\\infty} x[n-k]h[k]\\] <p>The expression above for \\(y[n]\\) is called the linear-convolution sum and is denoted by</p> \\[y[n] = x[n]*h[n] = h[n]*x[n]\\]"},{"location":"dsp/dsp-lab3-convolution/#example-low-pass-filter","title":"Example: Low Pass Filter","text":"<p>In this example we show a practical application of the concept of convolution. In particular, we will see how a system with a very simple impulse response can be used as a filter to smooth noisy signals.</p> <p>For this example, we will create a sinusoidal signal with a frequency of 10 Hz, sampled at 300 Hz, which is corrupted by zero mean, unit variance Gaussian noise. Next we generate a sequence \\(h[n] = \\frac{1}{8}(u[n]-u[n-8])\\). This is an impulse response which is constant (i.e. it is equal to \\(\\frac{1}{8}\\)) between \\(n=0\\) and \\(n=7\\). At all other times (i.e. at all values of \\(n\\) outside the interval \\(n=0\\) to \\(n=7\\)), the impulse response is equal to zero. Using convolution, we filter the noisy input signal \\(x[n]\\) with the filter characterized by \\(h[n]\\).</p> <p>Figure 1 shows the input \\(x[n]\\) and output \\(y[n] = x[n]*h[n]\\) of the system discussed above.</p> <p>Notice that the use of this filter with a very simple impulse response can help to reduce the Gaussian noise. The output of the system is less noisy than the original sequence. This system is called a moving-average filter, because the output is an average, that is</p> \\[y[n]= \\sum_{k=-\\infty}^{\\infty} x[n-k]h[k] = \\frac{1}{8}\\sum_{k=0}^{7} x[n-k]\\] <p>which can be expanded as:</p> \\[y[n]=\\frac{1}{8}(x[n]+x[n-1]+x[n-2]+x[n-3]+x[n-4]+x[n-5]+x[n-6]+x[n-7])\\] <p>This is the convolution sum of a finite-impulse response filter (FIR) of order 7. The problem in digital filtering design is how we go about choosing the \\(h[n]\\) coefficients so that we can filter-out the undesired noise present in the input signal. We are usually interested in filtering out some particular frequencies while leaving the others unchanged, therefore we need techniques that allow us to find specific coefficients to remove specific frequencies. This design is done in the frequency domain. We will revisit this topic once we have introduced the concepts of DTFT, DFT, and FFT.</p> <p> Figure 1: Example of a discrete system that smooths an input signal and improves the signal-to-noise ratio. The underlying signal is a 10 Hz sinusoid sampled at 300 Hz which is corrupted by Gaussian noise. Notice the output is a smooth delayed version of the input.</p>"},{"location":"dsp/dsp-lab3-convolution/#lab-tasks","title":"Lab Tasks","text":""},{"location":"dsp/dsp-lab3-convolution/#report-guidelines","title":"Report Guidelines","text":"<p>For the lab report, you will create a PowerPoint presentation (or use a similar presentation program), save it as a PDF, and submit it online according to the instructions given in class. All submitted MATLAB code should be well organized and commented with clear comments for easy readability. All submitted plots should be easy to see and well-labeled.</p> <p>Independent Work</p> <p>You are to work independently. This is not a team assignment. Feel free to help your fellow classmates understand principles and concepts, but please do not share your work.</p> <p>Your presentation will have 24 slides. Please include a slide number in the footer of each slide. To earn full credit your presentation must contain the slides in the order asked for in this lab. If you miss a slide, please leave a blank slide in its place so that you still have exactly 24 slides total.</p> <p>Slide 1: Title slide with your name, student ID number, date, lab name, class number/title.</p>"},{"location":"dsp/dsp-lab3-convolution/#task-i-intracranial-pressure-icp-signal-filtering","title":"Task I: Intracranial Pressure (ICP) Signal Filtering","text":"<p>\ud83d\udce5 Download: ICPComposite.mat</p> <p>1. Download the file <code>ICPComposite.mat</code>. Load the file into MATLAB. The signal is saved as a variable called icpcomposite. You can check this by typing <code>whos</code> after loading the file into MATLAB. Plot the signal in the time domain, and present your plot in Slide 2 (\\(f_s = 125\\) Hz).</p> <p>2. Use a second order moving average filter (\\(h[k] = (\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})\\)) to filter using the convolution sum function (<code>conv</code> in MATLAB). Do the same for a tenth order filter, a filter of order 20, and a filter of order 200. Comment on how increasing the order of the filter changes the system. Present the four plots of the filtered waveform (the plots correspond to filter orders 2, 10, 20, and 200) and discussion in Slides 3-7.</p> <p>3. MATLAB has two functions that can be used to implement filters by providing filter coefficients, one of them is called <code>filter</code> and the other <code>filtfilt</code>. Using the MATLAB help, repeat the first experiment (for a filter order of 20 only) using both functions, and present your two plots in Slides 8 and 9. Comment on the differences. Is a filter implemented using <code>filtfilt</code> a causal system? Present your answer to this question and comments in Slide 10.</p> <p>4. Use an FIR filter to eliminate frequency components higher than the fundamental component. Try to filter the ICP signal in such a way that the filtered signal will be as sinusoidal as possible. Present a plot of your filtered signal in Slide 11. For this plot, please program MATLAB to display your name as the title of the plot (e.g. <code>title('Jane Doe')</code>). Present the MATLAB code you used to generate this plot in Slides 12-13. Please be sure to clearly structure and comment your code so it is easy to read and understand.</p> <p>5. Use the MATLAB <code>detrend</code> command (or devise your own method) to eliminate the ICP trend (DC component). Present a plot of the signal with the DC component eliminated in Slide 14.</p>"},{"location":"dsp/dsp-lab3-convolution/#task-ii-audio-signal-fun","title":"Task II: Audio Signal Fun","text":"<p>\ud83d\udce5 Download: test_audio.mat</p> <p>1. Download the file <code>test_audio.mat</code>. This is an audio recording of me saying \"testing, testing, one, two, three\". I recorded this by speaking into my laptop's built in microphone and using MATLAB's <code>audiorecorder</code> function. Once the file is loaded into MATLAB, the audio signal is saved as variable y. You can check this by typing <code>whos</code> after loading the file into MATLAB. The sample rate of this recording is \\(f_s = 44100\\) Hz. Play the audio signal using the command <code>soundsc(y,44100)</code>. This command sends the data to your computer speakers at a sample rate of 44100 Hz, and automatically plays the signal as loudly as possible without clipping.</p> <p>Volume Warning</p> <p>Before playing the signal, turn down the volume of your speakers a bit (especially if you are playing this through headphones).</p> <p>2. Play the recording at a lower sampling rate of 44100/2 = 22050 Hz using the command <code>soundsc(y,22050)</code>. In Slide 15 describe, qualitatively, how lowering the sample rate affects the audio. How does lowering the sampling rate affect the signal in the frequency domain?</p> <p>3. Suppose your DAC (sound card) is designed to work at 44100/2 = 22050 Hz. How do we faithfully reproduce the audio in this case? The answer is that we need to resample the signal. In this case, because our desired sampling frequency is exactly half the original sampling frequency, we can decimate the signal by 2. Read the MATLAB help document on the <code>decimate</code> function. Now decimate the signal by a factor of 2 via the command: <code>g = decimate(y,2)</code> and play the decimated signal with <code>soundsc(g,22050)</code>. Does the decimated signal sound the same as the original? What is the size in bits and bytes of both the original and decimated signals? Hint: Use the <code>whos</code> command. Present your results in Slide 16.</p> <p>4. Try decimating the signal by a larger factor (say, a factor of 5). How \"far can you go\" with decimating and still be able to easily hear/understand the audio message? Present your maximum decimation factor in Slide 17. Decimation is a simple form of data compression, i.e. you can encode the same message using fewer bits. Quantify the memory savings obtained by decimating with your maximum decimation factor in Slide 17.</p> <p>5. Look up the <code>downsample</code> function in MATLAB. Briefly explain the difference between <code>downsample</code> and <code>decimate</code> in Slide 18. When would you use downsample over decimate (and vice versa)? What MATLAB command would you use to decimate the original signal y by the rational factor 3/7 (i.e. not an integer)? Present the answers to these questions in Slide 18.</p> <p>\ud83d\udce5 Download: very_noisy_signal.mat</p> <p>6. Download the file <code>very_noisy_signal.mat</code>. This is an audio recording of me reciting a secret message to you. The message is about 12 seconds long and the sample rate is again 44100 Hz. Unfortunately, the message is severely corrupted by high-frequency noise. To me, the noise sounds like a terrible hiss, so be sure to turn down your speakers before playing the message. The noisy audio signal is saved as variable z. Play the message using the MATLAB command <code>soundsc(z,44100)</code>. Describe what you hear in Slide 19. Can you hear the secret message? My guess is not. To hear the message you are going to need to filter out the high frequency noise.</p> <p>7. Low-pass filter the noisy signal using a moving average filter with different filter orders. If you can hear the secret message using the moving average filter, then write out the secret message in Slide 20, and take note of the filter order that you used. If you still can't hear the message (even with a high filter order), then take note of this on Slide 21.</p> <p>8. The moving average filter is not an optimal low-pass filter. Let's create a better filter in MATLAB using the <code>fir1</code> command. For example, to design a 100 order FIR low pass filter with a cut-off frequency of 0.1, type the command <code>b = fir1(100,.1)</code>. Note that the cut-off frequency you enter (0.1 in this case) is assumed by MATLAB to be normalized by \\(f_s/2\\). Therefore, the actual (unnormalized) cut-off frequency for this filter is \\(0.1 \\times f_s/2 = 0.1 \\times 44100/2 = 2205\\) Hz. This is a pretty good cut-off frequency for this example, since it will pass the audio spectrum below 2205 Hz (which contains the majority of frequencies for typical adult male human speech), while attenuating the high-frequency noise.</p> <p>The result of this command (<code>b = fir1(100,.1)</code>) is an array b that contains the 101 filter coefficients that describe the low pass FIR filter. Next, you can filter the signal z with the <code>conv</code> command in MATLAB: <code>conv(z,b)</code>. With this method, filter the noisy signal to expose the secret message. Write the message out verbatim on Slide 22. Include your MATLAB code for filtering the signal in Slides 23 and 24.</p>"},{"location":"dsp/dsp-lab3-convolution/#references","title":"References","text":"<ol> <li>J. Sprunger and M. Aboy, Computer Explorations in DSP Laboratory 3 - Discrete Time Systems and Convolution, prepared for EE 430, Oregon Institute of Technology, 2013.</li> </ol> <p>Lab created January 2016 | Updated for modern context February 2026</p>"},{"location":"dsp/dsp-lab4-dtmf-generator/","title":"DSP Lab 4: DTMF Tone and Song Generator","text":"<p>Author: Dr. Aaron Scher Oregon Institute of Technology</p> <p>Lab Materials</p> <p>This lab explores Dual-Tone Multi-Frequency (DTMF) signaling and provides hands-on experience with tone generation, audio synthesis, and frequency analysis using MATLAB.</p>"},{"location":"dsp/dsp-lab4-dtmf-generator/#objectives","title":"Objectives","text":"<p>The objective of this lab is to investigate DTMF tone generation by:</p> <ul> <li>Understanding how telephone keypads encode digits as audio tones</li> <li>Creating MATLAB functions to generate DTMF signals</li> <li>Composing musical sequences using DTMF tones</li> <li>Analyzing signals using spectrograms</li> <li>Exploring the effects of different sampling rates</li> </ul>"},{"location":"dsp/dsp-lab4-dtmf-generator/#introduction","title":"Introduction","text":"<p>When you press a number on the key pad of a standard telephone, you generate a pair of audible tones, each with a different frequency. If your phone is connected to a phone line, then these tones let the phone company know what number you are pressing. As long as the telephone company can identify the two tones, it can identify the digits of the desired telephone number.</p> <p></p>"},{"location":"dsp/dsp-lab4-dtmf-generator/#dtmf-frequency-table","title":"DTMF Frequency Table","text":"<p>A table that summarizes the tones associated with each button on a touch tone phone is shown below. This communication signaling is called Dual Tone Multi-Frequency (DTMF) signaling.</p> <p></p> <p>DTMF Encoding</p> <p>Each key on the telephone keypad is represented by two simultaneous sine waves:</p> <ul> <li>One from the row frequency (697, 770, 852, or 941 Hz)</li> <li>One from the column frequency (1209, 1336, 1477, or 1633 Hz)</li> </ul> <p>For example, pressing \"5\" generates a tone consisting of 770 Hz + 1336 Hz.</p> <p>Note that the last column is not used on a typical touch-tone phone (digits A, B, C, and D), but can be used for data transmission.</p>"},{"location":"dsp/dsp-lab4-dtmf-generator/#lab-instructions","title":"Lab Instructions","text":"<p>You are to generate an m-file called <code>song.m</code>. When this file is executed it should play a song by playing a sequence of DTMF tones in order.</p>"},{"location":"dsp/dsp-lab4-dtmf-generator/#example-mary-had-a-little-lamb","title":"Example: \"Mary Had a Little Lamb\"","text":"<p>As an example, the song \"Mary Had a Little Lamb\" is generated by playing the following sequence of tones in order:</p> <p>4, 5, 6, 5, 6, 6, 6, 5, 5, 5, 6, 6, 6, 4, 5, 6, 5, 6, 6, 6, 4, 5, 5, 6, 5, 4</p> <p>In the case of \"Mary Had a Little Lamb\", to sound correctly I've found that each number should have a duration of about 0.1 seconds, and there should be a space (silence) between each number of about 0.05 seconds. Of course, these values can be changed to suit one's musical tastes.</p>"},{"location":"dsp/dsp-lab4-dtmf-generator/#matlab-requirements","title":"MATLAB Requirements","text":""},{"location":"dsp/dsp-lab4-dtmf-generator/#1-song-requirements","title":"1. Song Requirements","text":"<ul> <li>Your song must have at least 20 numbers</li> <li>Your song must not last longer than 1 minute</li> <li>Use a sampling frequency of \\(f_s = 8000\\) Hz</li> </ul>"},{"location":"dsp/dsp-lab4-dtmf-generator/#2-create-generatedtmf-function","title":"2. Create <code>generateDTMF</code> Function","text":"<p>To generate a clean code, you must create and incorporate a function called <code>generateDTMF</code> that will create a DTMF tone. The inputs to the function are the digit, sampling frequency, and duration of the tone:</p> Matlab<pre><code>x = generateDTMF(digit, duration, fs)\n</code></pre> <p>Input parameters:</p> <ul> <li><code>digit</code>: An integer corresponding to the DTMF tone (0-9, or 10='*', 11='#')</li> <li><code>duration</code>: The time duration of the DTMF tone (in seconds)</li> <li><code>fs</code>: The sampling frequency (Hz)</li> </ul> <p>Output:</p> <ul> <li><code>x</code>: The DTMF signal (array of samples)</li> </ul> <p>Example implementation:</p> Matlab<pre><code>function x = generateDTMF(digit, duration, fs)\n    % DTMF frequency table\n    % Rows: 1=697, 2=770, 3=852, 4=941 Hz\n    % Cols: 1=1209, 2=1336, 3=1477, 4=1633 Hz\n\n    row_freqs = [697, 770, 852, 941];\n    col_freqs = [1209, 1336, 1477, 1633];\n\n    % Map digits to row and column\n    keypad = [1, 2, 3, 10;    % 1, 2, 3, A\n              4, 5, 6, 11;    % 4, 5, 6, B\n              7, 8, 9, 12;    % 7, 8, 9, C\n              13, 0, 14, 15]; % *, 0, #, D\n\n    % Find row and column for this digit\n    [row, col] = find(keypad == digit);\n\n    if isempty(row)\n        error('Invalid digit: %d', digit);\n    end\n\n    % Get frequencies\n    f_row = row_freqs(row);\n    f_col = col_freqs(col);\n\n    % Generate time vector\n    t = 0:1/fs:duration;\n\n    % Generate DTMF tone (sum of two sinusoids)\n    x = sin(2*pi*f_row*t) + sin(2*pi*f_col*t);\nend\n</code></pre>"},{"location":"dsp/dsp-lab4-dtmf-generator/#3-create-generatesilence-function","title":"3. Create <code>generateSilence</code> Function","text":"<p>Create a second function called <code>generateSilence</code> similar to <code>generateDTMF</code> for creating the silent parts of the composition:</p> Matlab<pre><code>x = generateSilence(duration, fs)\n</code></pre> <p>Example implementation:</p> Matlab<pre><code>function x = generateSilence(duration, fs)\n    % Generate silence (zeros) for specified duration\n    num_samples = round(duration * fs);\n    x = zeros(1, num_samples);\nend\n</code></pre>"},{"location":"dsp/dsp-lab4-dtmf-generator/#4-code-organization","title":"4. Code Organization","text":"<ul> <li>Label and include clear comments in your code for maximum readability</li> <li>Your entire musical composition should be saved as one long array <code>s</code></li> <li>Use array concatenation to merge signals together</li> </ul> <p>Example of merging arrays:</p> Matlab<pre><code>% Generate individual tones\ntone1 = generateDTMF(4, 0.1, 8000);\nsilence1 = generateSilence(0.05, 8000);\ntone2 = generateDTMF(5, 0.1, 8000);\n\n% Combine into single array\ns = [tone1, silence1, tone2];\n</code></pre>"},{"location":"dsp/dsp-lab4-dtmf-generator/#5-generate-spectrograms","title":"5. Generate Spectrograms","text":"<p>Generate and display a spectrogram of the song using the MATLAB command <code>spectrogram</code> (this will be figure 1):</p> Matlab<pre><code>% Spectrogram parameters\nfs = 8000;           % Sampling frequency\nres_time = 0.01;     % Time resolution (seconds)\n\n% Generate spectrogram\n% The spectrogram function works by taking an FFT of the signal\n% over small \"chunks\" of time (equal to res_time) and then\n% displays all the data on a time versus frequency axis.\n\nfigure;\nspectrogram(s, blackman(fs*res_time), 0, [], fs, 'yaxis');\ntitle('Song Spectrogram (fs = 8000 Hz)');\ncolorbar;\n</code></pre> <p> Figure 1: Example spectrogram showing DTMF tones over time</p> <p>Understanding Spectrograms</p> <p>A spectrogram displays how the frequency content of a signal changes over time. For DTMF signals, you should see:</p> <ul> <li>Horizontal bands at the DTMF frequencies (697-1633 Hz)</li> <li>Each digit appears as two horizontal lines (row + column frequencies)</li> <li>Gaps between tones appear as blank (silent) regions</li> </ul> <p>Great resources: - StackOverflow: MATLAB Spectrogram Parameters - Textbook Chapter 13</p>"},{"location":"dsp/dsp-lab4-dtmf-generator/#6-demonstrate-your-song","title":"6. Demonstrate Your Song","text":"<p>You must demonstrate your song to me in class using the MATLAB command <code>soundsc</code> at the date specified in the Blackboard shell:</p> Matlab<pre><code>% Play the song\nsoundsc(s, fs);\n</code></pre> <p>Volume Warning</p> <p>Before playing, turn down your speaker/headphone volume to a comfortable level!</p>"},{"location":"dsp/dsp-lab4-dtmf-generator/#7-test-different-sampling-rates","title":"7. Test Different Sampling Rates","text":"<p>Once everything works, change the sampling frequency and observe the effects:</p> <p>Test 1: \\(f_s = 4000\\) Hz</p> Matlab<pre><code>% Regenerate song at fs = 4000 Hz\nfs_new = 4000;\n% ... regenerate all tones with new fs ...\n\n% Listen to the song\nsoundsc(s_4000, fs_new);\n\n% Generate spectrogram (Figure 2)\nfigure;\nspectrogram(s_4000, blackman(fs_new*res_time), 0, [], fs_new, 'yaxis');\ntitle('Song Spectrogram (fs = 4000 Hz)');\n</code></pre> <p>Test 2: \\(f_s = 1000\\) Hz</p> Matlab<pre><code>% Regenerate song at fs = 1000 Hz\nfs_new = 1000;\n% ... regenerate all tones with new fs ...\n\n% Listen to the song\nsoundsc(s_1000, fs_new);\n\n% Generate spectrogram (Figure 3)\nfigure;\nspectrogram(s_1000, blackman(fs_new*res_time), 0, [], fs_new, 'yaxis');\ntitle('Song Spectrogram (fs = 1000 Hz)');\n</code></pre> <p>Analysis Questions</p> <p>In your code comments, answer these questions:</p> <ul> <li>Does the song sound different for \\(f_s = 4000\\) Hz and \\(f_s = 1000\\) Hz?</li> <li>Comment on why the song does or does not sound different for the two lower sampling rates</li> <li>Think about the Nyquist frequency: What is the maximum frequency in a DTMF signal? Is it adequately sampled at these rates?</li> </ul> <p>Hint: The maximum DTMF frequency is 1633 Hz. According to Nyquist, you need \\(f_s \\geq 2 \\times 1633 = 3266\\) Hz to avoid aliasing.</p>"},{"location":"dsp/dsp-lab4-dtmf-generator/#what-to-turn-in","title":"What to Turn In","text":""},{"location":"dsp/dsp-lab4-dtmf-generator/#deliverables","title":"Deliverables","text":"<ol> <li>Save all your work in a single m-file called <code>song.m</code> and publish it as a single PDF to upload to Blackboard</li> <li>Make sure the functions <code>generateDTMF</code> and <code>generateSilence</code> are both local to the file <code>song.m</code></li> <li>To do this, you will need to turn your script into a function that takes no argument (see Appendix below)</li> <li>Submit your published PDF before the due date</li> <li>If you write any other functions, include those as well in your main m-file</li> </ol>"},{"location":"dsp/dsp-lab4-dtmf-generator/#demonstration","title":"Demonstration","text":"<p>To earn full credit, you must demonstrate your song at the time and date specified on Blackboard.</p>"},{"location":"dsp/dsp-lab4-dtmf-generator/#example-code","title":"Example Code","text":"<p>The following MATLAB code plays a tone with frequency 1000 Hz for 0.2 seconds, then plays a tone with frequency 2000 Hz for 0.3 seconds, then plays nothing (silence) for 0.5 seconds, then plays a tone with frequency 1000 Hz for 0.2 seconds. The sampling frequency is \\(f_s = 8000\\) Hz. The spectrogram of the signal is then displayed.</p> Matlab<pre><code>clc; clear all; clf;\n\nfs = 8000;  % Sampling frequency\n\n% Generate first tone (1000 Hz, 0.2 seconds)\nt1 = 0:1/fs:0.2;\nx1 = cos(t1*2*pi*1000);\n\n% Generate second tone (2000 Hz, 0.3 seconds)\nt2 = 0:1/fs:0.3;\nx2 = cos(t2*2*pi*2000);\n\n% Generate silence (0.5 seconds)\nt3 = 0:1/fs:0.5;\nx3 = t3*0;\n\n% Generate third tone (1000 Hz, 0.2 seconds)\nt4 = 0:1/fs:0.2;\nx4 = cos(t1*2*pi*1000);\n\n% Combine arrays into one signal\ns = [x1, x2, x3, x4];\n\n% Play the signal through speakers\nsoundsc(s, fs);\n\n% Spectrogram parameters\nres_time = 0.01;  % Time resolution of spectrogram\n\n% Display spectrogram\nspectrogram(s, blackman(fs*res_time), 0, [], fs, 'yaxis');\n</code></pre>"},{"location":"dsp/dsp-lab4-dtmf-generator/#appendix-how-to-include-a-script-and-function-in-the-same-file","title":"Appendix: How to Include a Script and Function in the Same File","text":"<p>First, if you are unsure of the differences between a script and a function in MATLAB, you can read more about it here:</p> <ul> <li>MATLAB: Scripts and Functions</li> <li>StackOverflow: Script vs Function</li> </ul> <p>For this lab, you will be writing a MATLAB script (<code>song.m</code>) as well as MATLAB functions. Normally, each function is saved as a separate m-file that you call from the script. In terms of uploading your assignment to Blackboard, however, it would be very cumbersome to upload all of your separate functions individually.</p> <p>I would like you to turn in only one published m-file. Unfortunately, MATLAB will not allow you to nest functions inside scripts locally. To get around this limitation we will exploit the fact that you can put functions inside other functions.</p> <p>The fix: Turn your script into a function that takes no argument. In order to do this, you add one line of code to the top of your original script. This one line of code is a function call with the name of your file, but no argument, as illustrated below for m-file named <code>my_file.m</code>:</p> Matlab<pre><code>function my_file\n    % Script here...\n    a = 1;\n    b = 2;\n    the_sum = add_em_up(a, b);  % Custom function\nend\n\nfunction out = add_em_up(x1, x2)\n    % Function here...\n    out = x1 + x2;\nend\n</code></pre> <p>Output: Text Only<pre><code>the_sum =\n     3\n</code></pre></p> <p>With this small fix you can now run and publish the above file (<code>my_file.m</code>) as you would a normal script file.</p>"},{"location":"dsp/dsp-lab4-dtmf-generator/#complete-example-structure","title":"Complete Example Structure","text":"Matlab<pre><code>function song\n    % SONG - Generate and play a DTMF song\n    % This function generates a musical composition using DTMF tones\n\n    clc; clear;\n\n    % Parameters\n    fs = 8000;              % Sampling frequency (Hz)\n    tone_duration = 0.1;    % Duration of each tone (seconds)\n    silence_duration = 0.05; % Duration of silence between tones (seconds)\n\n    % Song sequence (example: Mary Had a Little Lamb)\n    notes = [4, 5, 6, 5, 6, 6, 6, 5, 5, 5, 6, 6, 6, ...\n             4, 5, 6, 5, 6, 6, 6, 4, 5, 5, 6, 5, 4];\n\n    % Generate song\n    s = [];\n    for i = 1:length(notes)\n        tone = generateDTMF(notes(i), tone_duration, fs);\n        silence = generateSilence(silence_duration, fs);\n        s = [s, tone, silence];\n    end\n\n    % Play song\n    soundsc(s, fs);\n\n    % Display spectrogram\n    res_time = 0.01;\n    figure;\n    spectrogram(s, blackman(fs*res_time), 0, [], fs, 'yaxis');\n    title('My Song Spectrogram');\n    colorbar;\nend\n\nfunction x = generateDTMF(digit, duration, fs)\n    % GENERATEDTMF - Generate a DTMF tone\n    % Inputs:\n    %   digit    - Digit to generate (0-9)\n    %   duration - Duration in seconds\n    %   fs       - Sampling frequency (Hz)\n    % Output:\n    %   x        - DTMF signal\n\n    % Your implementation here...\nend\n\nfunction x = generateSilence(duration, fs)\n    % GENERATESILENCE - Generate silence\n    % Inputs:\n    %   duration - Duration in seconds\n    %   fs       - Sampling frequency (Hz)\n    % Output:\n    %   x        - Silent signal (zeros)\n\n    % Your implementation here...\nend\n</code></pre>"},{"location":"dsp/dsp-lab4-dtmf-generator/#key-concepts-summary","title":"Key Concepts Summary","text":""},{"location":"dsp/dsp-lab4-dtmf-generator/#dtmf-frequencies","title":"DTMF Frequencies","text":"Row Frequency (Hz) Columns \u2192 1209 Hz 1336 Hz 1477 Hz 1633 Hz 1 697 Digits \u2192 1 2 3 A 2 770 Digits \u2192 4 5 6 B 3 852 Digits \u2192 7 8 9 C 4 941 Digits \u2192 * 0 # D"},{"location":"dsp/dsp-lab4-dtmf-generator/#matlab-functions","title":"MATLAB Functions","text":"Function Purpose Example <code>sin</code> Generate sinusoid <code>sin(2*pi*f*t)</code> <code>soundsc</code> Play audio <code>soundsc(x, fs)</code> <code>spectrogram</code> Time-frequency analysis <code>spectrogram(x, window, ...)</code> <code>blackman</code> Window function <code>blackman(N)</code>"},{"location":"dsp/dsp-lab4-dtmf-generator/#signal-generation-formula","title":"Signal Generation Formula","text":"<p>For digit with row frequency \\(f_r\\) and column frequency \\(f_c\\):</p> \\[x(t) = \\sin(2\\pi f_r t) + \\sin(2\\pi f_c t)\\]"},{"location":"dsp/dsp-lab4-dtmf-generator/#creative-ideas","title":"Creative Ideas","text":"<p>Song Suggestions</p> <p>Here are some simple songs you could try (with note numbers):</p> <ul> <li>Happy Birthday: 1-1-2-1-4-3, 1-1-2-1-5-4...</li> <li>Twinkle Twinkle: 1-1-5-5-6-6-5...</li> <li>Jingle Bells: 3-3-3, 3-3-3, 3-5-1-2-3...</li> <li>Your own composition!</li> </ul>"},{"location":"dsp/dsp-lab4-dtmf-generator/#references","title":"References","text":"<ol> <li>MATLAB Signal Processing Toolbox Documentation</li> <li>DTMF specification: ITU-T Recommendation Q.23</li> <li>Oppenheim &amp; Schafer, Discrete-Time Signal Processing (textbook)</li> </ol> <p>Lab created February 2016 | Updated for modern context February 2026</p>"},{"location":"dsp/dsp-lab5-dtmf-detector/","title":"DSP Lab 5: DTMF Tone Sequence Detector","text":"<p>Author: Dr. Aaron Scher Oregon Institute of Technology</p> <p>Lab Materials</p> <p>This lab is a continuation of DSP Lab 4: DTMF Tone and Song Generator. It provides hands-on experience with designing a DSP system in MATLAB to detect and decode DTMF tone sequences.</p>"},{"location":"dsp/dsp-lab5-dtmf-detector/#objectives","title":"Objectives","text":"<p>The objective of this lab is to design a DSP system in MATLAB that correctly detects the first two tones of a DTMF sequence.</p> <p> </p>"},{"location":"dsp/dsp-lab5-dtmf-detector/#instructions","title":"Instructions","text":"<p>In this lab you will write a MATLAB function called <code>decodeDTMF</code>, which will decode the first two tones of a DTMF sequence. The input to this function is a DTMF signal which may contain one to several tones of different time durations. The signal may be noisy. The signal may have periods of silence before and/or after the tones. The function <code>decodeDTMF</code> you are to create is specified as follows:</p> Matlab<pre><code>first_two_digits = decodeDTMF(file_path)\n</code></pre> <p>Here <code>file_path</code> is the file path to the input signal. The output of this function <code>first_two_digits</code> is an array of two integers containing the first two decoded DTMF digits in the order they appear.</p>"},{"location":"dsp/dsp-lab5-dtmf-detector/#important-considerations-and-assumptions","title":"Important Considerations and Assumptions","text":"<ul> <li>If the input signal contains just one DTMF digit then the function should display a single integer that is the decoded DTMF digit.</li> <li>If the input signal contains no digits (just noise) or the input sample frequency <code>fs</code> is zero or negative, then the function should display an appropriate error message.</li> <li>Assume each individual DTMF digit is between 0.3 to 3 seconds in duration.</li> <li>Assume the silence between digits is between 0.1 to 3 seconds in duration.</li> <li>Assume the signal begins with a period of silence that is at least 0.1 seconds in duration.</li> <li>Assume the signal can contain at most five digits.</li> <li>Assume the signal level is much higher than the noise (high SNR), as exhibited in the example files below.</li> <li>Assume the signal only contains numerical DTMF digits (0-9). It will not contain special characters like * and #.</li> </ul>"},{"location":"dsp/dsp-lab5-dtmf-detector/#decoding-dtmf-tones","title":"Decoding DTMF Tones","text":"<p>To assist you, below is a list of four files containing known DTMF tones. Download these files and use them to test and develop your function.</p> File link Actual DTMF digits Expected output of <code>decodeDTMF</code> \ud83d\udce5 File_Test_1.wav 2, 7 2, 7 \ud83d\udce5 File_Test_2.wav 4, 0, 7, 8 4, 0 \ud83d\udce5 File_Test_3.wav 9 9 \ud83d\udce5 File_Test_4.wav (noise) Error: \"No signal detected!\""},{"location":"dsp/dsp-lab5-dtmf-detector/#what-to-turn-in","title":"What to Turn In","text":"<p>You will be submitting the following deliverables:</p> <ol> <li>A PDF containing the completed table below (see grading files)</li> <li>Your function <code>decodeDTMF</code> published and saved as a PDF. Your MATLAB code should be clear, logical, organized, and well-commented for ease of reading. Be sure all other functions you write for this assignment are local to this function. You should only publish one m-file.</li> </ol>"},{"location":"dsp/dsp-lab5-dtmf-detector/#in-class-demonstration","title":"In-Class Demonstration","text":"<p>To earn full credit, I will randomly choose one of the DTMF tone files below for you to demonstrate that your function correctly decodes the file and returns the proper decoded values. The date for this in-class demonstration is posted to Blackboard.</p>"},{"location":"dsp/dsp-lab5-dtmf-detector/#grading-files","title":"Grading Files","text":"<p>Complete the table below by running your <code>decodeDTMF</code> function on each file:</p> File link Output of <code>decodeDTMF</code> \ud83d\udce5 File01.wav \ud83d\udce5 File02.wav \ud83d\udce5 File03.wav \ud83d\udce5 File04.wav \ud83d\udce5 File05.wav \ud83d\udce5 File06.wav \ud83d\udce5 File07.wav \ud83d\udce5 File08.wav"},{"location":"dsp/dsp-lab5-dtmf-detector/#hints-and-tips","title":"Hints and Tips","text":"<p>1. Filter Bank Approach</p> <p>An effective way for determining a tone is to create a bank of filters as shown below. In this figure, as an example, it shows a \"0\" signal entering the filter bank. Note that the bandpass filters (BPFs) associated with the 941 Hz and 1336 Hz tones provide the largest outputs.</p> <p></p> <p>2. Finding Signal Start</p> <p>The DTMF signal will usually begin and end with a period of silence. In addition, each DTMF digit in the sequence is separated by silences. So use silence to your advantage. For example, you can create a function called <code>FindSignalStart</code> as follows:</p> Matlab<pre><code>[y, index] = FindSignalStart(x)\n</code></pre> <p>Here, <code>x</code> is the input vector samples. The output <code>y</code> is the same as the input, but with the first silent samples (which may contain some background noise) removed. The output <code>index</code> is the index number in the input vector where the sound actually begins. The MATLAB function <code>find</code> may prove useful in determining the value of the index that is returned.</p> <p>3. Normalize the Signal</p> <p>The DTMF signal can have any amplitude, so it is recommended you normalize the signal (perhaps, after filtering out some noise) so the data falls within the range [-1, +1].</p> <p>4. Detrend the Data</p> <p>Sometimes data collected has a DC bias and/or linear trend that cause errors in DSP algorithms. Shown below is an example of some data that contains these trends (red curve exhibits positive bias, blue curve exhibits negative bias). When processing DTMF signals (or audio signals in general), it is generally a good idea to eliminate the mean (DC) value and any linear trend. This process is called \"detrending\", and can be accomplished in MATLAB using the <code>detrend</code> function.</p> <p></p>"},{"location":"dsp/dsp-lab5-dtmf-detector/#references","title":"References","text":"<ol> <li>MATLAB Signal Processing Toolbox Documentation</li> <li>DTMF specification: ITU-T Recommendation Q.23</li> <li>Oppenheim &amp; Schafer, Discrete-Time Signal Processing (textbook)</li> </ol> <p>Lab created February 2016 | Updated for modern context February 2026</p>"},{"location":"microwave-engineering/positive-negative-coupling/","title":"Positive Coupling, Negative Coupling, and All That","text":""},{"location":"microwave-engineering/positive-negative-coupling/#introduction","title":"Introduction","text":"<p>In the world of microwave filter design you hear a lot about coupling: \"coupling matrices\", \"dual-mode coupling\", \"cross-coupling\", \"coupled-lines\", \"coupling screws\", etc.</p> <p>What is all this coupling going on? Generally speaking, the word \"coupling\" refers to the transfer of energy between two or more physical states or systems. This is a broad definition, and, as such, you could technically say your brain-eye is electromagnetically coupled to the distant stars when you look up on a clear starry night; or, for that matter, that you are coupled to the hand of Van Gogh when you enjoy his swirling painting The Starry Night. However, these are examples of one-way coupling. You have no effect on a distant star whatsoever - if it even still exists today - and you certainly have no effect on Van Gogh. Ordinarily, people use the word \"coupling\" to describe systems interacting more intimately with each other, pushing and pulling, exchanging energy back and forth. People generally talk about coupling between oscillators (or resonant systems).</p> Figure 1. Starry Night by Vincent van Gogh. <p>The classic example of coupling between oscillators is two simple mechanical pendula joined by a spring, as illustrated below. If you get one pendulum swinging it will push and pull on the connecting spring, giving the second pendulum a small tug. As a result, the second pendulum begins to swing too. If the two pendulums have the same length and mass, the second pendulum continues to swing with ever greater speed as the first pendulum slows down. Eventually, the first pendulum is brought to rest; it has transferred all of its energy to the second pendulum. But now the original situation is exactly reversed, and the first pendulum is positioned to steal back its energy from the second. The process repeats itself over and over, with energy being passed back and forth like a game of catch, until finally all the energy is dissipated as heat by friction and air resistance. When this happens all mechanical motion ceases.</p> Figure 2. Two pendula joined by a spring."},{"location":"microwave-engineering/positive-negative-coupling/#synchronously-tuned-coupled-resonator-circuits","title":"Synchronously Tuned Coupled-Resonator Circuits","text":"<p>The electronic analog of the two pendula joined by a spring is a circuit composed of two resonators joined by a coupling element. Energy is coupled in the circuit, not by mechanical springs of course, but by electric and magnetic fields, or a mixture thereof.</p> <p>Consider two lossless identical resonators, each with a self-inductance \\(L\\) and self-capacitance \\(C\\), existing independently from one another as illustrated in Figure 3(a). The resonant frequency \\(\\omega_0\\) of each resonator is related to \\(L\\) and \\(C\\) by the well-known relation \\(\\omega_0 = 1/\\sqrt{LC}\\). As it stands, the resonators have nothing to do with each other, and are completely decoupled. Now suppose that the resonators are brought together to \"share\" a little piece of their self-inductance with one another. Suppose this little piece of shared inductance has value \\(L_m\\) (where \\(L_m \\leq L\\)) as illustrated in Figure 3(b). In this new situation, the resonators are said to be shunt inductively coupled. Alternate forms of the equivalent circuit are illustrated in Figure 3(c); these are particularly convenient for design applications.</p> (a) Two separate resonators existing independently from one another. (b) The resonators are brought together to \"share\" a piece of their inductance $L_m$, forming a coupled pair of $LC$ resonators. (c) Alternative forms of the equivalent circuit (all three have equal loop currents and resonant frequencies). <p>Figure 3. The formation of a shunt inductively coupled pair of LC resonators.</p> <p>While each isolated \\(LC\\) resonator supports only one resonant mode individually, the coupled pair as a whole supports two - each with its own resonant frequency. To understand this, consider the form of the coupled pair shown in Figure 4, where we denote the currents in loops 1 and 2 as \\(I_1\\) and \\(I_2\\), respectively. Here we use capital letters for current to remind us that we are dealing with phasor representations in the frequency domain; as usual, lower-case letters are reserved for time domain variables.</p> Figure 4. Schematic of coupled pair showing currents $I_1$ and $I_2$ for deriving the coupling equations. <p>The coupling equations describing this network are:</p> \\[ j\\omega L \\, I_1 - j\\frac{I_1}{\\omega C} + \\underbrace{j\\omega L_m \\, I_2}_{\\text{coupling: } I_2 \\text{ induces voltage in loop 1}} = 0 \\tag{1} \\] \\[ j\\omega L \\, I_2 - j\\frac{I_2}{\\omega C} + \\underbrace{j\\omega L_m \\, I_1}_{\\text{coupling: } I_1 \\text{ induces voltage in loop 2}} = 0 \\tag{2} \\] <p>In matrix form, these equations become:</p> \\[ \\begin{bmatrix} j\\!\\left(\\omega L - \\dfrac{1}{\\omega C}\\right) &amp; j\\omega L_m \\\\[8pt] j\\omega L_m &amp; j\\!\\left(\\omega L - \\dfrac{1}{\\omega C}\\right) \\end{bmatrix} \\begin{bmatrix} I_1 \\\\ I_2 \\end{bmatrix} = 0 \\tag{3} \\] <p>An easy and obvious solution to this matrix equation is simply \\(I_1 = I_2 = 0\\). However, this is not a very interesting solution. In fact, a resonant mode is precisely the situation when the currents are not zero. For this to be the case, the 2-by-2 square matrix on the left hand side of Eq. (3) must be non-invertible (i.e. it must be singular). Recall from basic linear algebra that a square matrix is singular if and only if its determinant is equal to zero. Singular matrices are rare in the sense that if you pick a random square matrix, it will almost surely not be singular. However, at certain frequencies (which we identify as the resonant frequencies) the matrix on the left hand side of Eq. (3) will indeed be singular. To find these resonant frequencies, we take the determinant and set it equal to zero:</p> \\[ \\begin{vmatrix} j\\!\\left(\\omega L - \\dfrac{1}{\\omega C}\\right) &amp; j\\omega L_m \\\\[8pt] j\\omega L_m &amp; j\\!\\left(\\omega L - \\dfrac{1}{\\omega C}\\right) \\end{vmatrix} = \\left(\\omega L - \\frac{1}{\\omega C}\\right)^2 - (\\omega L_m)^2 = 0 \\tag{4} \\] <p>If you crank through the algebra, you will find that this equation has two positive real solutions \\(\\omega_m\\) and \\(\\omega_e\\), where</p> \\[ \\omega_e = \\frac{1}{\\sqrt{(L - L_m)\\,C}} \\tag{5} \\] \\[ \\omega_m = \\frac{1}{\\sqrt{(L + L_m)\\,C}} \\tag{6} \\] <p>The larger the magnetic coupling \\(L_m\\), the greater the resonant frequencies \\(\\omega_e\\) and \\(\\omega_m\\) \"split away\" from that of an uncoupled resonator \\(\\omega_0\\). On the other hand, in the limit that the coupling is zero, the solutions tend to that of an uncoupled single resonator, i.e. \\(\\omega_e = \\omega_m = \\omega_0 = 1/\\sqrt{LC}\\) when \\(L_m = 0\\).</p> <p>What about the currents \\(I_1\\) and \\(I_2\\) in all of this? If you substitute \\(\\omega = \\omega_e\\) into Eq. (1) or (2), you will find that \\(I_1 = -I_2\\). This means the common current through the \"shared\" inductance \\(L_m\\) is \\(I_1 + I_2 = I_1 - I_1 = 0\\). From the point of view of the loop current voltages, this condition is equivalent to replacing the symmetry plane between the coupled pair with an electric wall (i.e. a short). The symmetry plane is illustrated in Fig. 5. The subscript \"\\(e\\)\" in the symbol \\(\\omega_e\\) stands for \"electric wall\". This mode is referred to as the odd mode.</p> <p>On the other hand, if you substitute \\(\\omega = \\omega_m\\) into Eq. (1) or (2), you will find that \\(I_1 = I_2\\). This means the common current through the \"shared\" inductance \\(L_m\\) is \\(I_1 + I_2 = 2I_1 = 2I_2\\). This condition is equivalent to replacing the symmetry plane with a magnetic wall (i.e. an open). This mode is referred to as the even mode. The subscript \"\\(m\\)\" in the symbol \\(\\omega_m\\) stands for \"magnetic wall\".</p> <p>Summary of Even/Odd Mode Properties</p> Mode Condition Symmetry Plane Frequency Odd (electric wall) \\(I_1 = -I_2\\) Short circuit \\(\\omega_e = 1/\\sqrt{(L - L_m)C}\\) Even (magnetic wall) \\(I_1 = +I_2\\) Open circuit \\(\\omega_m = 1/\\sqrt{(L + L_m)C}\\) Figure 5. Symmetry plane between the coupled resonant pair. <p>An important fact to take away from all this is that a coupled pair of resonators forms a composite system with two resonant modes of its own. The resonant frequencies of these modes are different from that of the uncoupled single resonators. You can say that the original mode has \"split\" into two. This is a general fact of coupled oscillators: a system composed of \\(N\\) weakly coupled discrete oscillators will in general have \\(N\\) independent states (\"state\" is a more general term for \"mode\"). If you increase \\(N\\), then the number of states goes up. Recall that a single pendulum swings back and forth with one frequency only - its resonant frequency. Here we have \\(N = 1\\). On the other hand, consider a guitar string fixed on both ends. Such a structure basically consists of an infinite number of coupled oscillators (i.e. the atoms in the string coupled together by electrostatic forces). As you may know, an ideal vibrating string fixed on both ends has an infinite number of sine waves (i.e. modes or harmonics) it can support. Strum an acoustic guitar to hear some of them.</p> Figure 6. The Old Guitarist by Pablo Picasso."},{"location":"microwave-engineering/positive-negative-coupling/#negative-coupling","title":"Negative Coupling","text":"<p>The mutual inductance \\(L_M\\) can be either positive or negative depending on the polarity of the mutual voltage in reference to the direction of the inducing current. Figure 7 shows two magnetically coupled coils with both positive and negative coupling. Note that the mutual voltage in the second coil can be positive or negative based on the orientation of the coils. The figures assume the dot convention: if a current enters the dotted terminal of a coil, the reference polarity of the voltage induced in the other coil is positive at its dotted terminal. If a current leaves the dotted terminal of a coil, the reference polarity of the voltage induced in the other coil is negative at its dotted terminal.</p> Figure 7. Illustration of sign convention for the mutual inductance $L_m$. (a) $L_m &gt; 0$, (b) $L_m &lt; 0$. In these illustrations, the terminals at port 2 are open so no current flows through them. <p>As it turns out, we are not so much interested in the mutual inductance \\(L_m\\) itself as we are in the ratio of mutual inductance to self-inductance: \\(L_m / L\\). This ratio is a measure of the coupling strength, and is known as the coupling coefficient \\(k_M\\) for a pair of synchronously tuned shunt inductive coupled resonators. From energy considerations, it can be shown that the mutual inductance of a pair of coils cannot be greater than the geometric mean of the self-inductances of the coils:</p> \\[ |L_M| \\leq \\sqrt{L_1 L_2} \\] <p>In this manner, the coupling coefficient \\(k_M\\) specifies the extent to which \\(L_M\\) approaches the upper limit, and satisfies the inequality:</p> \\[ -1 \\leq k_M \\leq 1 \\] <p>The general coupling coefficient \\(k\\) between any two resonators is defined based on the ratio of coupled energy to stored energy of the electromagnetic field. The coupling coefficient shows up all the time in filter theory. When people talk about \"coupling matrices\" of microwave bandpass filters, they really mean a matrix (as in a two-dimensional array of numbers) filled with the coupling coefficients between the various resonators of the filter.</p> <p>A nice feature of the coupling coefficient is that it can be found from the even and odd mode resonant frequencies, \\(\\omega_m\\) and \\(\\omega_e\\). From Eqs. (5) and (6) we have:</p> \\[ k_M \\equiv \\frac{L_m}{L} = \\frac{\\omega_e^2 - \\omega_m^2}{\\omega_e^2 + \\omega_m^2} \\tag{7} \\] <p>So, by our chosen sign convention, if \\(\\omega_e &gt; \\omega_m\\) then \\(k_M\\) is positive, and if \\(\\omega_e &lt; \\omega_m\\) then \\(k_M\\) is negative. This sign convention is as arbitrary as assigning a negative electric charge to an electron and a positive electric charge to a proton. But once the sign convention has been made, you must stick with it or else you will lose your way. In order to ensure this sign convention is satisfied every time, we define the general coupling coefficient \\(k\\) to be:</p> \\[ k \\equiv \\frac{\\omega_e^2 - \\omega_m^2}{\\omega_e^2 + \\omega_m^2} \\tag{8} \\] <p>Equation (8) satisfies the strict definition of \\(k\\) (see Hong and Lancaster <sup>1</sup>). This is a powerful equation that can be used to experimentally or numerically determine the coupling coefficient between any two synchronously tuned pair of resonators - no matter what the type of coupling.</p>"},{"location":"microwave-engineering/positive-negative-coupling/#basic-coupling-mechanisms","title":"Basic Coupling Mechanisms","text":"<p>The basic coupling mechanisms between synchronously tuned pairs of resonators are summarized below. For each type of coupling, we show the coupling equations, the even and odd resonant frequencies (denoted \\(\\omega_e\\) and \\(\\omega_m\\), respectively), and the coupling coefficient - all in terms of the lumped element values.</p> Figure 8. Basic coupling mechanisms: shunt inductive (top), series capacitive (middle), and mixed (bottom)."},{"location":"microwave-engineering/positive-negative-coupling/#a-practical-way-to-extract-the-coupling-coefficient","title":"A Practical Way to Extract the Coupling Coefficient","text":"<p>Suppose you had two coupled resonators and you wanted to know the value of the coupling coefficient. We can use Eq. (8) to extract \\(k\\) if we know the two eigenfrequencies \\(\\omega_m\\) and \\(\\omega_e\\). One easy way to determine the eigenfrequencies is to very lightly \"excite\" the system with a voltage source like that shown in Figure 9, and then measure the voltage or current at some other point in the circuit. The eigenfrequencies will be apparent.</p> <p>Figure 9 shows a SPICE schematic with two LC resonators inductively coupled with \\(k_M = 0.1\\). We loosely couple a voltage source to resonator 1 (via a small capacitor) and loosely couple a resistor to resonator 2 (via a second small capacitor). Figure 10 shows the measured voltage across R1. Here we clearly see two resonant peaks at \\(f_1 = 49.364\\) kHz and \\(f_2 = 54.573\\) kHz. The phase of the voltage is positive 90 degrees, which tells us that the coupling is positive, and therefore inductive (more on that in the next section). Substituting \\(f_1\\) and \\(f_2\\) into Equation (8) yields:</p> \\[ k = \\frac{f_2^2 - f_1^2}{f_2^2 + f_1^2} = \\frac{54.573^2 - 49.364^2}{54.573^2 + 49.364^2} = 0.1000 \\] <p>which is equal to the known value of \\(k_M\\). Note that the stronger the coupling, the wider the frequency gap between the resonant peaks. This translates to the language of bandpass filter design as: \"the wider the bandwidth, the tighter the required coupling between resonators.\"</p> Figure 9. LTspice schematic depicting measurement setup for extracting coupling coefficient. Figure 10. SPICE simulation results. Magnitude and phase of voltage across 100 \u03a9 load resistor."},{"location":"microwave-engineering/positive-negative-coupling/#electric-coupling-is-negative-coupling","title":"Electric Coupling is Negative Coupling","text":"<p>Or is it positive? Some people refer to electric (aka \"series capacitive\") coupling as \"negative\" coupling. To get a better understanding of this, refer to Figure 11. Here we see that two LC resonant circuits capacitively coupled are equivalent to two LC resonant circuits inductively coupled with a negative mutual inductance (here we consider the mutual inductance as negative because the \"dotted\" terminals are on opposite sides). We say these two circuits are equivalent in the sense that they have the same eigenfrequencies (resonant frequencies) and almost identical phase response near resonance. Figure 12 shows a SPICE schematic and simulation to illustrate how well this equivalence holds in a typical situation. The sign of coupling is rather relative. It's similar to how we assign an electron as being \"negatively\" charged.</p> <p>Notice how the phase in Figure 13 is \\(-90\u00b0\\) between the resonant peaks. This \"gives away\" the sign of the coupling as negative. If the phase were \\(+90\u00b0\\) then the sign of coupling would be considered \"positive\". Some people reverse what is positive and negative. Luckily, it makes no difference so long as a convention is established and you stick with that convention.</p> Figure 11. Capacitive coupling is equivalent to negative inductive coupling. Note: This equality is not exact! It works best with light loading (high Q) and loose coupling. Figure 12. SPICE schematic to demonstrate the equivalence shown in Figure 11. Figure 13. SPICE simulation results. Magnitude and phase of voltage across 0.5 \u03a9 load resistor."},{"location":"microwave-engineering/positive-negative-coupling/#sum-the-electric-and-magnetic-couplings-to-find-the-total-coupling","title":"Sum the Electric and Magnetic Couplings to Find the Total Coupling","text":"<p>Two coupled resonators with mixed coupling are approximately equivalent to two coupled resonators with purely capacitive or purely inductive coupling. This is illustrated in Figures 14 and 15. This equivalence only holds for very loose coupling (which is assumed in narrowband microwave filter design). To illustrate this, Figure 16 shows SPICE schematics comparing the mixed coupling case to the pure inductive and pure capacitive cases. Figure 17 shows simulated output voltage. Note the very good agreement between the mixed coupling case and that of pure inductive and pure capacitive coupling.</p> Figure 14. Mixed coupling is equal to inductive coupling near resonance (loose coupling assumed). Figure 15. Mixed coupling is equal to capacitive coupling near resonance (loose coupling assumed). Figure 16. SPICE schematics demonstrating the equivalences shown in Figures 14 and 15. Figure 17. SPICE simulation results. <ol> <li> <p>J.-S. Hong and M. J. Lancaster, Microstrip Filters for RF/Microwave Applications, Wiley, 2001.\u00a0\u21a9</p> </li> </ol>"},{"location":"microwave-engineering/resonant-coupling-networks/","title":"Resonant Coupling Networks","text":"<p>Below are a collection of images and figures of circuit transforms, examples, and theorems regarding resonant coupling networks, with a focus on magnetic coupling.</p>"},{"location":"microwave-engineering/resonant-coupling-networks/#references","title":"References","text":"<ol> <li>K. Clarke, et al., Communication Circuits: Analysis and Design, 1971</li> <li>H. Krauss, et al., Solid State Radio Engineering, 1980</li> <li>J. Carr, Secrets of RF Circuit Design, 1991</li> </ol>"},{"location":"microwave-engineering/resonant-coupling-networks/#equivalent-circuits-and-transformers","title":"Equivalent Circuits and Transformers","text":"Figure 1. High Q approximate equivalent circuits for various parallel, resonant \"transformer-like\" networks. Figure 2. Ideal transformer circuit relationships. Figure 3. Coupled inductor equivalent circuits in terms of magnetizing and leakage inductances. Figure 4. Another coupled inductor equivalent circuit in terms of controlled sources. Figure 5. Coupled inductor impedance transformation. Figure 6. Coupled inductor equivalent circuits for transformer and autotransformer in terms of magnetizing and leakage inductances."},{"location":"microwave-engineering/resonant-coupling-networks/#auto-transformers-and-tapped-inductors","title":"Auto-Transformers and Tapped Inductors","text":"Figure 7. Auto-transformer equivalent circuits. Figure 8. Tapped inductor (auto-transformer) impedance. Figure 9. Non-ideal tapped inductor (auto-transformer) equivalent input impedance. Figure 10. Non-ideal tapped inductor (auto-transformer) characteristics. Figure 11. Non-ideal tapped inductor (auto-transformer) characteristics. Figure 12. Non-ideal tapped inductor (auto-transformer) characteristics."},{"location":"microwave-engineering/resonant-coupling-networks/#series-inductors-and-decomposition","title":"Series Inductors and Decomposition","text":"Figure 13. Equivalent circuit of two coupled inductors connected in series. Figure 14. Just for fun: Decompose a single inductor into two coupled inductors in series."},{"location":"microwave-engineering/resonant-coupling-networks/#parallel-series-conversions-and-rlc-properties","title":"Parallel-Series Conversions and RLC Properties","text":"Figure 15. Parallel-series conversion formulas for RC networks. Figure 16. Parallel-series conversion formulas for RL networks. Figure 17. Summary of the properties of RLC resonant circuits."},{"location":"microwave-engineering/resonant-coupling-networks/#tapped-capacitor-and-tapped-inductor-circuits","title":"Tapped Capacitor and Tapped Inductor Circuits","text":"Figure 18. Design formulas for tapped capacitor circuit. Figure 19. Tapped capacitor circuit design example. Figure 20. Design formulas for tapped inductor circuit. Figure 21. Tapped inductor circuit design example."},{"location":"microwave-engineering/resonant-coupling-networks/#single-and-double-tuned-transformers","title":"Single and Double Tuned Transformers","text":"Figure 22. Design formulas for the single tuned transformer. Figure 23. Example: Design of a tuned amplifier. Figure 24. Double tuned transformer equivalent circuit. Figure 25. Double tuned transformer equivalent circuit and insight."},{"location":"microwave-engineering/resonant-coupling-networks/#tank-circuits-and-matching-examples","title":"Tank Circuits and Matching Examples","text":"Figure 26. Popular forms of tuned RF/IF tank circuits. Figure 27. Tapped capacitor matching example. Figure 28. LTspice confirmation of previous example. Figure 29. Tapped capacitor + tapped inductor matching example. Figure 30. LTspice confirmation of previous example."},{"location":"microwave-engineering/resonant-coupling-networks/#applications","title":"Applications","text":"Figure 31. Hi-Fi switch. Figure 32. Tapped capacitor crystal radio set. Sensitivity versus selectivity."},{"location":"microwave-engineering/resonant-coupling-networks/#filter-coupling-and-multi-stage-networks","title":"Filter Coupling and Multi-Stage Networks","text":"Figure 33. Coupling into and out of a filter. Figure 34. Multiple transformers equivalent circuit. Figure 35. Three inductively coupled loops - equivalent circuits."},{"location":"microwave-engineering/resonant-coupling-networks/#time-domain-and-transient-responses","title":"Time Domain and Transient Responses","text":"Figure 36. Time domain response of narrow band filters. Figure 37. Impulse response of parallel RLC circuit. Figure 38. Transient response of parallel RLC circuit to sinusoid input. Figure 39. Transient response of parallel RLC circuit to on-off shift keying signal. Figure 40. Narrow band filter response to periodic signal. Figure 41. Parallel RLC network response to impulse train."},{"location":"quantum-mechanics/","title":"Quantum Mechanics","text":"<p>An intuitive guide to quantum mechanics - from first principles to entanglement.</p>"},{"location":"quantum-mechanics/#contents","title":"Contents","text":"<ul> <li>Foundations - Schr\u00f6dinger equation, harmonic oscillator, zero-point energy</li> <li>Basis Representation - Energy/position/momentum bases, Dirac notation, Hilbert space</li> <li>Operators and Measurement - Momentum operator, eigenstates vs superpositions, expected values, momentum space</li> <li>Spatial Entanglement - One particle in 2D, separable vs entangled wave functions, correlated measurements</li> </ul>"},{"location":"quantum-mechanics/band-theory-of-solids/","title":"Band Theory of Solids","text":""},{"location":"quantum-mechanics/band-theory-of-solids/#the-big-picture-three-types-of-electrons-in-a-metal","title":"The Big Picture: Three Types of Electrons in a Metal","text":"<p>Starting assumptions: Perfect crystal lattice at T = 0 K, ignoring electron-electron interactions.</p> <p>At this idealized starting point, electrons are delocalized Bloch waves filling energy levels according to Fermi statistics. Each electron has a k-vector and is equally likely to be found anywhere in the crystal.</p> <p>But the fun starts when we relax these assumptions! Add temperature \u2192 lattice vibrations (phonons). Add defects \u2192 imperfections break periodicity. Add external E fields \u2192 electrons accelerate. These perturbations cause scattering, decoherence, and localization - transforming perfect Bloch waves into wave packets that move, scatter, and conduct. This is where real metal behavior emerges.</p>"},{"location":"quantum-mechanics/band-theory-of-solids/#three-tiers-of-electrons","title":"Three Tiers of Electrons","text":"<p>1. Core electrons (1s, 2s, etc.) - Stuck to their ions - Tightly bound near the nucleus with very little coupling between neighboring ions - Their energy bands are extremely narrow (little orbital overlap) - With any imperfection or temperature, these narrow bands can't sustain coherent Bloch waves across the crystal - They localize into standard atomic orbitals around individual ions - Don't participate in conduction or bonding</p> <p>2. Deep valence electrons - The glue - Their orbitals overlap significantly between ions (wider energy bands) - Form delocalized Bloch waves well below the Fermi level - Few adjacent empty states \u2192 rarely scatter \u2192 stay Bloch-like - Provide the bonding \"glue\" holding the crystal together, but don't conduct</p> <p>3. Conduction electrons (near Fermi level) - The main players - Have many adjacent empty energy states - At T &gt; 0: Fermi distribution spreads \u2192 some electrons thermally excited to higher energies - These Bloch waves scatter/interact with lattice vibrations (phonons), defects, and other electrons - Scattering causes decoherence: superposition of multiple Bloch waves with incoherent phases - Decoherence \u2192 localization into wave packets (not single-k Bloch waves) - Wave packets move through crystal, scatter, change direction/shape, gradually spread out (become more Bloch-like) until next scattering event - Responsible for electrical and thermal conduction - they can shift to higher energy states and absorb energy</p> <p>What determines conductivity? The time between collisions (scattering time \u03c4).</p> <ul> <li>In an external E field, electrons accelerate and drift in the field direction</li> <li>Scattering \u2192 decoherence \u2192 localized wave packets: Immediately after a collision, electrons are localized (superposition of Bloch states with random phases)</li> <li>Between collisions \u2192 spreading \u2192 Bloch-like: As the wave packet propagates freely, it gradually spreads out and becomes more extended/delocalized, approaching Bloch character again</li> <li>Next collision \u2192 re-localizes: Another scattering event resets the cycle, localizing the electron again and randomizing its direction</li> <li>Shorter \u03c4 (frequent scattering) \u2192 electrons stay localized, randomize direction often \u2192 lower conductivity</li> <li>Longer \u03c4 (rare scattering) \u2192 electrons spend more time in extended Bloch-like state, maintain drift longer \u2192 higher conductivity</li> <li>Without scattering (\u03c4 \u2192 \u221e), conductivity would be infinite - electrons would remain as perfect extended Bloch waves and accelerate indefinitely</li> </ul> <p>Key insight: The electrons that conduct are those near the Fermi level with accessible empty states. Lower energy electrons stay Bloch-like but don't move. Core electrons are atomically localized and irrelevant for transport.</p>"},{"location":"quantum-mechanics/band-theory-of-solids/#energy-bands-from-isolated-atoms-to-solids","title":"Energy Bands: From Isolated Atoms to Solids","text":"<p>When atoms are brought together to form a solid, their discrete electron energy levels transform into continuous energy bands.</p> <p>Two ways to understand band formation:</p> <p>\u2022 Bottom-up (orbital splitting): Start with isolated atomic orbitals. As atoms approach for covalent bonding, orbitals overlap and split into N states for N atoms. These merge into continuous bands.</p> <p>\u2022 Top-down (Bloch theory): Solve the Schr\u00f6dinger equation for electrons in a periodic crystal potential. The solutions are Bloch waves, and allowed energies naturally form bands separated by gaps.</p>"},{"location":"quantum-mechanics/band-theory-of-solids/#energy-level-spreading","title":"Energy Level Spreading","text":"<p>As atoms approach each other, their electron orbitals begin to overlap. This causes each discrete atomic energy level to split into multiple closely-spaced states.</p> <p></p> <p>Figure 1 [1]: As atoms are brought closer together, individual allowed energy states start to spread in energy. The 1s and 2s electron states each split into 12 states (for 12 atoms), forming the beginning of energy bands.</p>"},{"location":"quantum-mechanics/band-theory-of-solids/#band-formation-at-equilibrium-spacing","title":"Band Formation at Equilibrium Spacing","text":"<p>At the equilibrium interatomic spacing in a solid, these closely-spaced states merge into continuous energy bands, separated by band gaps where no electron states exist.</p> <p></p> <p>Figure 2 [1]: At equilibrium interatomic spacing in a solid, the spread states form continuous energy bands. Band gaps appear between bands - energy ranges where no electron states are allowed.</p>"},{"location":"quantum-mechanics/band-theory-of-solids/#bands-in-real-space","title":"Bands in Real Space","text":"<p>In the bulk of a crystal, electrons occupy states within bands. The energy varies with position, creating potential wells at atomic sites.</p> <p></p> <p>Figure 3 [2]: Conduction and valence bands in a crystal showing spatial variation. The periodic potential creates wells at atomic sites. Note how the energy levels vary between rows of atoms and along rows.</p>"},{"location":"quantum-mechanics/band-theory-of-solids/#bloch-theorem-electrons-in-periodic-crystals","title":"Bloch Theorem: Electrons in Periodic Crystals","text":"<p>To understand electron behavior quantitatively, we solve the Schr\u00f6dinger equation for a periodic potential extending throughout an infinite crystal.</p> <p>Bloch's theorem states that electron wavefunctions in a periodic crystal have the form:</p> \\[\\psi(x) = U_K(x) e^{iKx}\\] <p>where:</p> <ul> <li>\\(U_K(x)\\) has the same periodicity as the crystal lattice</li> <li>\\(e^{iKx}\\) is a plane wave with wavevector K</li> <li>The wavefunction is a plane wave modulated by the crystal periodicity</li> </ul> <p>This is the stationary state (time-independent part). A stationary state has a single definite energy E. The complete time-dependent wavefunction is:</p> \\[\\Psi(x,t) = U_K(x) e^{iKx} e^{-iEt/\\hbar}\\] <p>The probability density \\(|\\Psi|^2\\) doesn't change with time - hence \"stationary.\"</p> <p>Key properties:</p> <ol> <li> <p>Symmetry: \\(E(K) = E(-K)\\) - energy is the same for \u00b1K (forward and backward traveling waves)</p> </li> <li> <p>Periodicity: \\(E(K) = E(K + \\frac{2\\pi n}{a})\\) - adding \\(2\\pi/a\\) to K gives the same energy (physically equivalent states)</p> </li> </ol> <p>For 3D crystals:</p> \\[\\psi(\\vec{r}) = U_{\\vec{K}}(\\vec{r}) e^{i\\vec{K}\\cdot\\vec{r}}\\] <p>where \\(|\\vec{K}| = 2\\pi/\\lambda\\) connects the wavevector magnitude to the de Broglie wavelength.</p>"},{"location":"quantum-mechanics/band-theory-of-solids/#brillouin-zones-and-the-reduced-zone-scheme","title":"Brillouin Zones and the Reduced Zone Scheme","text":"<p>The first Brillouin zone spans \\(K \\in [-\\pi/a, \\pi/a]\\) and contains all distinct k-values - a complete set of unique solutions. K-values outside this range simply duplicate solutions inside.</p>"},{"location":"quantum-mechanics/band-theory-of-solids/#solving-for-the-dispersion-relation-e-k-diagrams","title":"Solving for the Dispersion Relation: E-k Diagrams","text":"<p>When you solve the Schr\u00f6dinger equation for a periodic potential using the Bloch wavefunction form, you obtain the dispersion relation E(K) - how energy depends on wavevector K.</p> <p>The result is the E-k diagram, which reveals the full band structure:</p> <p></p> <p>Figure 4 [3]: Reduced-zone representation of allowed E-k states in a one-dimensional crystal. Solving Schr\u00f6dinger's equation with Bloch wavefunctions gives these dispersion curves - four energy bands separated by band gaps. Each band contains a continuum of allowed k-states between -\u03c0/(a+b) and \u03c0/(a+b).</p> <p>Connecting k, momentum, and energy:</p> Property Free Electron Electron in Crystal (Bloch) Wavefunction \\(\\psi = Ae^{i(kx-\\omega t)}\\) \\(\\psi = U_K(x)e^{i(Kx-\\omega t)}\\) Spatial part Plane wave \\(e^{ikx}\\) Bloch wave \\(U_K(x)e^{iKx}\\) Wavenumber \\(k = \\frac{2\\pi}{\\lambda}\\) \\(K = \\frac{2\\pi}{\\lambda}\\) (crystal momentum) Frequency \\(\\omega = \\frac{E}{\\hbar}\\) \\(\\omega = \\frac{E}{\\hbar}\\) Momentum \\(p = \\hbar k\\) \\(\\hbar K\\) (crystal momentum, not actual) Energy \\(E = \\frac{\\hbar^2 k^2}{2m}\\) \\(E = E_0 + \\frac{\\hbar^2 K^2}{2m_0}\\) (near band minimum, where \\(E_0\\) is band edge at K=0) Velocity \\(v = \\frac{\\hbar k}{m}\\) \\(v = \\frac{1}{\\hbar}\\frac{dE}{dK}\\) (group velocity) <p>Why does energy increase with momentum for free electrons?</p> <p>For a free particle: \\(E = \\frac{p^2}{2m} = \\frac{\\hbar^2 k^2}{2m}\\)</p> <ul> <li>Since \\(p = \\hbar k\\), momentum is directly proportional to k</li> <li>As energy increases: E \u2191 \u2192 \u03c9 \u2191 \u2192 p \u2191 \u2192 k \u2191 \u2192 \u03bb \u2193 (shorter wavelength, since \u03bb = 2\u03c0/k)</li> <li>E \u221d k\u00b2 is a parabola: higher k always means higher energy (monotonic)</li> <li>Higher energy \u2192 higher k \u2192 shorter wavelength \u2192 more localized</li> </ul> <p>Mass dependence: For the same energy E, solving \\(k = \\sqrt{2mE/\\hbar^2}\\) shows lighter particles (electrons) have smaller k \u2192 larger wavelength \u03bb = 2\u03c0/k. A light particle doesn't need much momentum to achieve a given kinetic energy, so it has a longer wavelength. Longer wavelengths are harder to confine spatially (try fitting a long wave into a small box!), making light particles naturally more delocalized than heavy ones at the same energy.</p> <p>Does this happen in crystals?</p> <p>Not always! Near band edges, the E-k curve bends backward - higher |K| can mean lower E. At zone boundaries (K = \u00b1\u03c0/a), electrons form standing waves (Bragg reflection) rather than propagating, with dE/dK = 0. This non-monotonic behavior is unique to crystals and creates band gaps.</p>"},{"location":"quantum-mechanics/band-theory-of-solids/#group-velocity-how-fast-do-electrons-move","title":"Group Velocity: How Fast Do Electrons Move?","text":"<p>The electron velocity is given by the group velocity:</p> \\[v = v_g = \\frac{1}{\\hbar}\\frac{dE}{dK}\\] <p>This is the slope of the E-K curve!</p> <ul> <li>Flat bands (small dE/dK) \u2192 slow electrons</li> <li>Steep bands (large dE/dK) \u2192 fast electrons</li> <li>At band edges, dE/dK = 0 \u2192 electrons have zero velocity (standing waves at Brillouin zone boundaries)</li> </ul>"},{"location":"quantum-mechanics/band-theory-of-solids/#references","title":"References","text":"<p>[1] \"Energy bands and band gaps,\" Pennsylvania State University, Materials Science and Engineering. [Online]. Available: https://courses.ems.psu.edu/matse81/node/2227</p> <p>[2] B. L. Anderson and R. Anderson, Fundamentals of Semiconductor Devices, 2nd ed. McGraw-Hill Education.</p> <p>[3] R. F. Pierret, Advanced Semiconductor Fundamentals, 2nd ed., vol. VI, Modular Series on Solid State Devices. Upper Saddle River, NJ: Prentice Hall, 2003.</p>"},{"location":"quantum-mechanics/electrons-in-metals/","title":"Electrons in Metals: From Bloch Waves to Wave Packets","text":""},{"location":"quantum-mechanics/electrons-in-metals/#what-problem-were-really-solving","title":"What Problem We're Really Solving","text":"<p>You've encountered two conflicting stories about electrons in metals:</p> <p>Classical story: Electrons are little charged particles that move, scatter off ions, and drift through a lattice like pinballs.</p> <p>Quantum story: Electrons occupy Bloch states, fill energy bands, and have well-defined crystal momentum \\(\\mathbf{k}\\).</p> <p>Both stories are partly true, but they apply in different limits. Confusion comes from mixing them without stating assumptions.</p> <p>Our goal is to build a single, unified picture that:</p> <ul> <li>Is fully quantum at its core</li> <li>Reduces to classical intuition when appropriate</li> <li>Makes sense of phonons, scattering, and wave packets</li> <li>Sets the stage for conductivity (without doing transport yet)</li> </ul> <p>We're building the kinematic and quantum foundation here.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#part-1-the-ideal-starting-point","title":"Part 1: The Ideal Starting Point","text":""},{"location":"quantum-mechanics/electrons-in-metals/#a-frozen-infinite-crystal","title":"A Frozen, Infinite Crystal","text":"<p>We begin with a deliberately unrealistic but extremely clarifying limit.</p> <p>Assumptions (state them explicitly):</p> <ol> <li>Infinite crystal \u2014 the ionic lattice extends forever in all directions, no surfaces</li> <li>Perfect periodicity \u2014 the ionic potential satisfies \\(V(\\mathbf{r} + \\mathbf{R}) = V(\\mathbf{r})\\) for all lattice vectors \\(\\mathbf{R}\\)</li> <li>Ions are frozen \u2014 no thermal motion, no phonons, time-independent Hamiltonian</li> <li>Single-electron picture \u2014 electron-electron interactions absorbed into an effective periodic potential</li> </ol> <p>Under these assumptions, the single-electron Hamiltonian is:</p> \\[ H = \\frac{p^2}{2m} + V(\\mathbf{r}) \\] <p>This Hamiltonian has exact discrete translational symmetry.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#why-bloch-waves-exist","title":"Why Bloch Waves Exist","text":"<p>Because of that symmetry, the Schr\u00f6dinger equation admits solutions of the form:</p> \\[ \\psi_{n\\mathbf{k}}(\\mathbf{r}) = e^{i\\mathbf{k}\\cdot\\mathbf{r}} \\, u_{n\\mathbf{k}}(\\mathbf{r}) \\] <p>where \\(u_{n\\mathbf{k}}(\\mathbf{r})\\) has the same periodicity as the lattice.</p> <p>What matters conceptually:</p> <ul> <li>Each \\((n, \\mathbf{k})\\) labels an exact energy eigenstate</li> <li>Energy depends on \\(\\mathbf{k}\\), forming bands: \\(E = E_n(\\mathbf{k})\\)</li> <li>These eigenstates are stationary: their probability density doesn't change in time</li> <li>For a fixed \\(\\mathbf{k}\\), the state evolves only by a global phase:</li> </ul> \\[ \\psi_{n\\mathbf{k}}(\\mathbf{r},t) = \\psi_{n\\mathbf{k}}(\\mathbf{r}) \\, e^{-iE_n(\\mathbf{k})t/\\hbar} \\] <p>Key insight: A Bloch state has a perfectly defined crystal momentum \\(\\mathbf{k}\\). Here \\(\\mathbf{k}\\) is a symmetry label, not the electron's mechanical momentum. By Fourier duality:</p> <ul> <li>Perfectly sharp \\(\\mathbf{k}\\) \\(\\rightarrow\\) completely delocalized in space</li> </ul> <p>In this ideal limit, each electron literally extends across the entire crystal \u2014 a direct mathematical consequence of symmetry.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#where-bands-come-from-physically","title":"Where Bands Come From Physically","text":"<p>While Bloch's theorem tells us bands must exist given perfect symmetry, it doesn't explain their origin or width.</p> <p>Physically, bands arise from atomic orbitals splitting when isolated atoms come together to form a crystal:</p> <p>\u2022 Isolated atom: Each electron occupies a discrete atomic orbital (e.g., 1s, 2s, 2p) \u2022 Bring atoms close: Orbitals on neighboring atoms overlap \u2022 Crystal forms: Each atomic level splits into \\(N\\) slightly different energies (where \\(N\\) is the number of atoms), forming a band</p> <p>Key physical insight:</p> <p>\u2022 Strong orbital overlap \u2192 wide bands \u2192 highly delocalized states (e.g., conduction electrons) \u2022 Weak orbital overlap \u2192 narrow bands \u2192 more atom-like behavior (e.g., core electrons, d-electrons in transition metals)</p> <p>This explains why:</p> <p>\u2022 Deep core electrons barely feel the crystal \u2014 their wavefunctions overlap weakly \u2022 Valence electrons form wide bands \u2014 their wavefunctions extend between atoms \u2022 Phonons disrupt some states more than others \u2014 delocalized states couple more strongly to lattice motion</p>"},{"location":"quantum-mechanics/electrons-in-metals/#filling-the-bands-electrons-in-equilibrium","title":"Filling the Bands: Electrons in Equilibrium","text":"<p>At zero temperature:</p> <ul> <li>Electrons fill the lowest available Bloch states</li> <li>Pauli exclusion enforces one electron per \\((n, \\mathbf{k}, \\text{spin})\\)</li> <li>The many-electron ground state is built from pure Bloch waves</li> </ul> <p>Important conceptual point: In this limit:</p> <ul> <li>Electrons are not particles moving through space</li> <li>They are stationary quantum states</li> <li>Talking about trajectories or collisions is meaningless</li> </ul> <p>This is the cleanest possible starting point.</p> <p>Clarification used throughout this tutorial</p> <p>When we say \"an electron occupies a Bloch state,\" we are using shorthand. The many-electron ground state has Bloch symmetry; individual electrons are not independently observable objects with fixed identities. \"An electron in state \\(\\mathbf{k}\\)\" really means: the single-particle excitations are labeled by Bloch quantum numbers.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#why-energy-eigenstates-are-stationary","title":"Why Energy Eigenstates Are Stationary","text":"<p>You might wonder: why do electrons settle into pure energy eigenstates rather than arbitrary superpositions?</p> <p>The key is stationarity, not energetics.</p> <p>Consider a superposition of two energy eigenstates:</p> \\[ \\Psi(x,t) = c_1 \\phi_1(x) e^{-iE_1 t/\\hbar} + c_2 \\phi_2(x) e^{-iE_2 t/\\hbar} \\] <p>The probability density \\(|\\Psi|^2\\) oscillates in time at frequency \\((E_2 - E_1)/\\hbar\\). The state has internal time structure.</p> <p>But in the frozen crystal:</p> <ul> <li>There is nothing external oscillating at that frequency</li> <li>There is nothing for this time structure to couple to</li> <li>The superposition has no way to exchange energy with its environment</li> </ul> <p>The reason Bloch states dominate in the frozen crystal is not that superpositions are energetically forbidden, but that they introduce internal time dependence in a system with nothing to couple to that time structure.</p> <p>Superpositions are allowed and persist forever in a perfectly isolated system; they are simply not stationary because observables oscillate in time. Energy eigenstates are the natural \"resting states\" \u2014 not because other states are unstable, but because they are the only ones that are truly stationary.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#part-2-the-crucial-pivot","title":"Part 2: The Crucial Pivot","text":"<p>This is the most important conceptual turn in the entire tutorial:</p> <p>Bloch waves exist because of symmetry, not because of energetics.</p> <p>Here is the canonical statement to remember:</p> <p>Once translational symmetry is even weakly broken, exact Bloch eigenstates no longer exist; what survives are long-lived, Bloch-like quasiparticles.</p> <p>And in the real world, symmetry is always broken \u2014 by lattice motion, defects, surfaces, and finite size.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#part-3-reality-enters-lattice-motion-and-phonons","title":"Part 3: Reality Enters \u2014 Lattice Motion and Phonons","text":""},{"location":"quantum-mechanics/electrons-in-metals/#what-phonons-are","title":"What Phonons Are","text":"<p>At any finite temperature, ions vibrate. Even at absolute zero, quantum mechanics forces zero-point motion.</p> <p>These vibrations organize into normal modes: collective oscillations where each mode has a wavevector \\(\\mathbf{q}\\) and frequency \\(\\omega_{\\mathbf{q}}\\).</p> <p>Critical distinction (guitar string analogy):</p> Guitar string Lattice Mode = sinusoidal vibration pattern Mode = collective oscillation pattern Energy is continuous Energy is quantized: \\(E_n = \\hbar\\omega(n + \\frac{1}{2})\\) Amplitude is continuous Amplitude \\(\\propto \\sqrt{n}\\) where \\(n\\) is discrete Loud note = large amplitude Many phonons = large amplitude <p>A phonon is one quantum of energy (\\(\\hbar\\omega\\)) in a mode \u2014 not the wave itself. The word \"phonon\" is misleading: it sounds like the wave, but it's the energy unit in the wave.</p> <ul> <li>Mode with \\(n = 0\\): zero-point energy only, no phonons</li> <li>Mode with \\(n = 3\\): three phonons, energy \\(3.5\\hbar\\omega\\)</li> <li>Mode with \\(n = 1000\\): amplitude looks classical (large \\(\\sqrt{n}\\))</li> </ul> <p>When many phonons occupy a mode coherently, you get a classical-looking wave. This is why \"lattice vibrations\" and \"phonons\" seem interchangeable \u2014 at high occupation, they look the same. But fundamentally, the phonon is the quantum, not the wave.</p> <p>Where does \\(E_n = \\hbar\\omega(n + \\frac{1}{2})\\) come from?</p> <p>Each lattice mode is atoms connected by springs \u2014 a harmonic oscillator. When you solve the Schr\u00f6dinger equation for a harmonic oscillator potential \\(V = \\frac{1}{2}m\\omega^2 x^2\\), you get discrete energy levels spaced by \\(\\hbar\\omega\\). This is standard QM, nothing special to solids.</p> <p>Coherent vs incoherent phonons (numerical example):</p> <p>Consider three modes with \\(\\omega_1, \\omega_2, \\omega_3\\) and wavelengths \\(\\lambda_1, \\lambda_2, \\lambda_3\\).</p> <p>Coherent phonons \u2014 well-defined phases, gives a real traveling wave:</p> \\[u(x,t) = A_1\\cos(q_1 x - \\omega_1 t) + A_2\\cos(q_2 x - \\omega_2 t) + A_3\\cos(q_3 x - \\omega_3 t)\\] <p>Example: \\(A_1 = 0.1\\) nm, \\(A_2 = 0.05\\) nm, \\(A_3 = 0.02\\) nm. This is a wave packet \u2014 a localized pulse traveling through the lattice. The amplitudes relate to phonon number: \\(A \\propto \\sqrt{n}\\).</p> <p>Incoherent phonons \u2014 random phases, no net wave:</p> \\[\\langle u(x,t) \\rangle = 0\\] <p>But there's still jiggling! The mean-square displacement is nonzero:</p> \\[\\langle u^2 \\rangle &gt; 0\\] <p>Example: thermal phonons at room temperature. Each mode has random phase, so there's no organized wave \u2014 just fluctuations. The atoms jitter around their equilibrium positions with no coherent pattern.</p> <p>One-line summary:</p> <ul> <li>Coherent: real wave with phase, like plucking a guitar string</li> <li>Incoherent: random jitter, \\(\\langle u \\rangle = 0\\) but \\(\\langle u^2 \\rangle &gt; 0\\), like thermal noise</li> </ul>"},{"location":"quantum-mechanics/electrons-in-metals/#two-flavors-of-phonons","title":"Two Flavors of Phonons","text":"<p>Not all phonons are alike. The two main types behave very differently:</p> Text Only<pre><code>ACOUSTIC: neighbors move together         OPTICAL: neighbors move opposite\n\n  \u25cf  \u25cb  \u25cf  \u25cb  \u25cf  \u25cb   equilibrium           \u25cf  \u25cb  \u25cf  \u25cb  \u25cf  \u25cb   equilibrium\n\n  \u2192 \u2192 \u2192 \u2192 \u2192 \u2192                              \u2192 \u2190 \u2192 \u2190 \u2192 \u2190\n  \u25cf  \u25cb  \u25cf  \u25cb  \u25cf  \u25cb   displaced             \u25cf  \u25cb  \u25cf  \u25cb  \u25cf  \u25cb   displaced\n     \\____/                                   \\/    \\/\n   move together                           push apart\n\n(\u25cf and \u25cb = two atom types; optical modes require 2+ atoms per unit cell)\n</code></pre> <p>Acoustic phonons:</p> <ul> <li>Neighboring atoms move in phase \u2014 the whole lattice compresses and stretches</li> <li>Called \"acoustic\" because long wavelengths behave like sound waves</li> <li>Linear dispersion at small \\(q\\): \\(E \\approx \\hbar v_s q\\)</li> <li>Dominate ordinary scattering at moderate temperatures</li> </ul> <p>Optical phonons:</p> <ul> <li>Neighboring atoms move out of phase \u2014 one sublattice against another</li> <li>Called \"optical\" because in ionic crystals, opposite motion of +/\u2212 ions creates an oscillating dipole that couples to infrared light</li> <li>Have a minimum energy even at \\(q = 0\\) (typically tens of meV)</li> <li>Important for high-field transport: emitting an optical phonon is an efficient \"energy dump\"</li> </ul> <p>Both types scatter electrons, but through different mechanisms and at different energy scales.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#part-4-the-key-toy-model-particle-in-a-box-with-moving-walls","title":"Part 4: The Key Toy Model \u2014 Particle in a Box with Moving Walls","text":"<p>This model captures the essential physics of how lattice motion affects electrons. It's the cleanest bridge between basic Schr\u00f6dinger physics and electron-phonon intuition.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#static-box-baseline","title":"Static Box (Baseline)","text":"<p>Assumptions:</p> <ul> <li>1D, one electron</li> <li>Infinite walls at \\(x = 0\\) and \\(x = L\\)</li> <li>Walls do not move</li> <li>Time-independent Hamiltonian</li> </ul> <p>The Schr\u00f6dinger equation inside the box:</p> \\[ -\\frac{\\hbar^2}{2m}\\frac{d^2\\psi}{dx^2} = E\\psi \\] <p>Solutions:</p> \\[ \\psi_n(x) = \\sqrt{\\frac{2}{L}}\\sin\\left(\\frac{n\\pi x}{L}\\right) \\] \\[ E_n = \\frac{n^2\\pi^2\\hbar^2}{2mL^2} \\] <p>What matters physically:</p> <ul> <li>Each \\(\\psi_n\\) has a single, sharp energy</li> <li>The probability density \\(|\\psi_n|^2\\) does not change in time</li> <li>This is the analog of a Bloch wave in a frozen crystal</li> </ul>"},{"location":"quantum-mechanics/electrons-in-metals/#now-make-the-walls-move","title":"Now Make the Walls Move","text":"<p>Let:</p> \\[ L(t) = L_0 + \\delta L \\cos(\\Omega t), \\quad \\delta L \\ll L_0 \\] <p>This does two critical things:</p> <ol> <li>The boundary conditions now depend on time</li> <li>The Hamiltonian now depends on time</li> </ol> <p>This immediately means:</p> <p>Energy is no longer conserved for the electron alone.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#how-to-think-about-the-solution-no-heavy-math","title":"How to Think About the Solution (No Heavy Math)","text":"<p>At each instant, the electron tries to look like one of the original box states. So we write:</p> \\[ \\Psi(x,t) \\approx c_1(t)\\psi_1(x) + c_2(t)\\psi_2(x) + c_3(t)\\psi_3(x) \\] <p>You can imagine more terms, but three is enough for intuition.</p> <p>What do these coefficients mean?</p> <ul> <li>\\(|c_n(t)|^2\\) = probability of being in energy level \\(n\\)</li> <li>If walls are static: one \\(c_n = 1\\), the rest are zero</li> <li>If walls move: the \\(c_n\\)'s change in time</li> </ul>"},{"location":"quantum-mechanics/electrons-in-metals/#why-moving-walls-cause-energy-level-mixing","title":"Why Moving Walls Cause Energy Level Mixing","text":"<p>The key physical idea:</p> <ul> <li>Moving the walls changes the shape of the allowed wavefunctions</li> <li>The old eigenstates are no longer exact solutions</li> <li>The electron must \"borrow\" pieces of other states to stay consistent</li> </ul> <p>Mathematically (but gently):</p> \\[ \\frac{dc_m}{dt} \\propto \\cos(\\Omega t) \\times (\\text{overlap integrals}) \\] <p>What matters:</p> <ul> <li>The cosine term is the energy pump</li> <li>The overlaps determine which states can mix</li> <li>Mixing is strongest when frequencies match</li> </ul>"},{"location":"quantum-mechanics/electrons-in-metals/#resonance-when-things-actually-happen","title":"Resonance: When Things Actually Happen","text":"<p>Each energy difference defines a natural frequency:</p> \\[ \\omega_{mn} = \\frac{E_m - E_n}{\\hbar} \\] <p>If the wall oscillation frequency satisfies:</p> \\[ \\Omega \\approx \\omega_{mn} \\] <p>then:</p> <ul> <li>Energy is transferred efficiently between levels</li> <li>\\(c_m(t)\\) grows significantly</li> <li>The electron enters a superposition of energies</li> </ul> <p>This is generic behavior of driven quantum systems.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#what-the-wavefunction-now-looks-like","title":"What the Wavefunction Now Looks Like","text":"<p>Once multiple \\(c_n\\)'s are nonzero:</p> \\[ \\Psi(x,t) = c_1(t)\\psi_1(x) + c_2(t)\\psi_2(x) \\] <p>Then:</p> \\[ |\\Psi(x,t)|^2 \\text{ changes in time} \\] <p>This means:</p> <ul> <li>The electron no longer has a stationary probability cloud</li> <li>The electron is now a dynamic wave packet</li> <li>Energy is genuinely uncertain</li> </ul> <p>This is the moment classical intuition starts to sneak back in.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#why-this-maps-directly-to-phonons","title":"Why This Maps Directly to Phonons","text":"<p>Here is the exact conceptual mapping:</p> Moving Box Real Crystal Wall motion Ion motion Oscillation frequency \\(\\Omega\\) Phonon frequency \\(\\omega_{\\mathbf{q}}\\) Energy pumped externally Energy exchanged internally No momentum exchange Phonon carries momentum \\(\\mathbf{q}\\) <p>In a crystal:</p> <ul> <li>The potential minimum itself moves</li> <li>It moves periodically in both space and time</li> <li>That drives electrons into energy and momentum superpositions</li> </ul>"},{"location":"quantum-mechanics/electrons-in-metals/#part-5-electron-phonon-coupling","title":"Part 5: Electron-Phonon Coupling","text":""},{"location":"quantum-mechanics/electrons-in-metals/#what-the-electron-actually-sees","title":"What the Electron Actually Sees","text":"<p>When ions move, the electron experiences:</p> \\[ V(\\mathbf{r}, t) = V_0(\\mathbf{r}) + \\delta V(\\mathbf{r}, t) \\] <p>where \\(V_0\\) is the periodic lattice potential and \\(\\delta V\\) is the time-dependent perturbation from lattice motion.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#phonons-as-traveling-potential-waves","title":"Phonons as Traveling Potential Waves","text":"<p>A phonon creates a traveling deformation in the lattice:</p> \\[ \\delta V(x,t) \\approx A\\cos(qx - \\omega t) \\] <p>Think of it as a moving grating \u2014 a traveling Bragg reflector that diffracts the electron wave.</p> <p>Key point: Phonons are quantized, like photons. In a scattering event, the electron either:</p> <ul> <li>Absorbs the whole phonon: gains \\(+\\hbar q\\) momentum, gains \\(+\\hbar\\omega\\) energy, phonon destroyed</li> <li>Emits a whole phonon: loses \\(\\hbar q\\) momentum, loses \\(\\hbar\\omega\\) energy, phonon created</li> </ul> <p>No partial absorption. The entire phonon's momentum and energy transfer to the electron (or vice versa). Conservation is exact.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#why-temperature-matters","title":"Why Temperature Matters","text":"<p>Impurities: static perturbation \u2192 scattering rate independent of \\(T\\).</p> <p>Phonons: dynamic perturbation \u2192 more phonons at higher \\(T\\) \u2192 more scattering \u2192 resistivity \\(\\propto T\\).</p> <p>At low \\(T\\), impurity scattering dominates (residual resistivity). At high \\(T\\), phonon scattering dominates.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#why-electron-phonon-scattering-increases-with-temperature","title":"Why Electron-Phonon Scattering Increases with Temperature","text":"<p>As temperature rises, more phonon modes are populated and their amplitudes increase. This makes the lattice potential increasingly time-dependent and spatially distorted. Electrons are therefore driven into superpositions of nearby Bloch states more frequently, shortening quasiparticle lifetimes and increasing momentum relaxation.</p> <p>In wavefunction language, scattering increases because the electron's state is being reshaped more often, not because particles collide more violently.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#part-6-from-bloch-waves-to-wave-packets","title":"Part 6: From Bloch Waves to Wave Packets","text":"<p>A pure Bloch state has one sharp \\(\\mathbf{k}\\). By Fourier duality: sharp \\(\\mathbf{k}\\) means delocalized in space \u2014 the electron is spread across the entire crystal. And because it's an energy eigenstate, it's stationary \u2014 the probability density doesn't move.</p> <p>So how does an electron ever get from A to B?</p> <p>Answer: Phonons (and other symmetry-breakers) mix in multiple \\(\\mathbf{k}\\) components. A superposition of different \\(\\mathbf{k}\\)'s is a wave packet \u2014 localized in space, and it moves with group velocity \\(v_g = \\frac{1}{\\hbar}\\frac{dE}{dk}\\).</p> Pure Bloch state Wave packet \\(\\mathbf{k}\\) content Single sharp \\(\\mathbf{k}\\) Spread of \\(\\mathbf{k}\\)'s Position Delocalized (entire crystal) Localized (\\(\\Delta x \\sim 1/\\Delta k\\)) Motion None \u2014 stationary Moves with group velocity <p>This is exactly what the moving-walls toy model showed: oscillating walls \u2192 superposition of energy levels \u2192 probability density that sloshes around.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#what-scattering-really-is-a-concrete-example","title":"What \"Scattering\" Really Is \u2014 A Concrete Example","text":"<p>Forget billiard balls. Here's what actually happens.</p> <p>Notation: \\(|k\\rangle\\) is shorthand for a Bloch state with crystal momentum \\(k\\) \u2014 the full wavefunction \\(\\psi_k(\\mathbf{r}) = e^{ikx} u_k(\\mathbf{r})\\) that extends across the crystal.</p> <p>Before scattering: An electron wave packet is a superposition of Bloch states with different \\(k\\) values:</p> \\[|\\psi_{\\text{before}}\\rangle = 0.8|k_1\\rangle + 0.5|k_2\\rangle + 0.3|k_3\\rangle\\] <p>where \\(k_1 = 5.0\\), \\(k_2 = 5.1\\), \\(k_3 = 4.9\\) nm\\(^{-1}\\). The packet is centered around \\(k \\approx 5\\) nm\\(^{-1}\\), with spread \\(\\Delta k \\approx 0.1\\) nm\\(^{-1}\\), giving localization \\(\\sim 1/\\Delta k \\approx 10\\) nm.</p> <p>A phonon arrives: wavevector \\(q = 0.5\\) nm\\(^{-1}\\), energy \\(\\hbar\\omega = 15\\) meV.</p> <p>After scattering (phonon absorbed): Each \\(k\\) component shifts by \\(+q\\):</p> \\[|\\psi_{\\text{after}}\\rangle = 0.6|k_1 + q\\rangle + 0.7|k_2 + q\\rangle + 0.4|k_3 + q\\rangle + \\text{new components}\\] <p>Now centered around \\(k \\approx 5.5\\) nm\\(^{-1}\\). The amplitudes changed, new \\(k\\) components appeared, and the spread increased.</p> <p>What happened to the phonon? It was destroyed \u2014 the electron absorbed the whole phonon (they're quantized, like photons). The electron gained its momentum (\\(+\\hbar q\\)) and energy (\\(+\\hbar\\omega\\)). Conservation is exact.</p> <p>For emission: reverse everything. The electron loses momentum (\\(-\\hbar q\\)) and energy (\\(-\\hbar\\omega\\)), creating a new phonon.</p> <p>Why is this irreversible (decoherence)? After scattering, the electron's phase information is now entangled with the phonon bath. The old amplitudes \\(\\{0.8, 0.5, 0.3\\}\\) are gone \u2014 you can't interfere the \"before\" and \"after\" states because which-path information leaked into the environment. The electron has no memory of its previous \\(k\\) distribution. This loss of phase coherence is what makes scattering effectively one-way, even though the underlying physics is time-reversible.</p> <p>The point: \"Scattering\" is the amplitudes \\(\\{0.8, 0.5, 0.3, ...\\}\\) being reshuffled into \\(\\{0.6, 0.7, 0.4, ...\\}\\) with shifted \\(k\\) values. No collision, no impact point \u2014 just Fourier components being added and removed, with phase information lost to the environment.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#the-alphabet-vs-words-analogy","title":"The \"Alphabet vs Words\" Analogy","text":"<p>Think of Bloch states as letters in an alphabet:</p> <ul> <li>Individual letters (Bloch states) are building blocks</li> <li>Real electrons are words \u2014 specific combinations</li> <li>Phonons constantly \"edit\" the words by swapping letters in and out</li> </ul> <p>The alphabet remains fixed; only the combinations change.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#part-7-pauli-exclusion-and-deep-electrons","title":"Part 7: Pauli Exclusion and Deep Electrons","text":"<p>Deep electrons are still wave packets \u2014 phonons perturb them into superpositions of \\(\\mathbf{k}\\) states just like any other electron. Pauli exclusion doesn't freeze them into pure Bloch states. But for transport, what matters is whether electrons can change their net momentum distribution. Deep electrons have all nearby states occupied \u2014 nowhere to go. Only electrons near the Fermi surface have empty states to scatter into, so they dominate conduction.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#part-8-what-does-t-0-really-mean","title":"Part 8: What Does T = 0 Really Mean?","text":"<p>Even at \\(T = 0\\), zero-point motion persists \u2014 ions still jiggle. But the average potential remains periodic, so band structure stays accurate. An electron starting near some \\(\\mathbf{k}\\) will eventually scatter to other \\(\\mathbf{k}\\) values; at lower \\(T\\), fewer phonons means it stays near that \\(\\mathbf{k}\\) longer before being reshuffled. But zero-point motion means some scattering always happens \u2014 electrons never sit in a pure Bloch state forever.</p>"},{"location":"quantum-mechanics/electrons-in-metals/#part-9-common-misconceptions","title":"Part 9: Common Misconceptions","text":"Misconception Reality \"Electrons bounce off ions like billiard balls\" Wavefunctions reshape; there's no impact point \"Only electrons near Fermi level are quantum\" All electrons are quantum waves; deep ones just don't change occupancy \"Phonons are fake mathematical particles\" Phonons are as real as photons \u2014 quantized collective excitations \"Bloch waves are just approximations\" They're exact under symmetry; the approximation is assuming that symmetry holds \"At T=0, electrons are perfect Bloch waves forever\" Zero-point motion still breaks exact periodicity"},{"location":"quantum-mechanics/electrons-in-metals/#part-10-the-mental-model-to-keep","title":"Part 10: The Mental Model to Keep","text":"<p>Carry this hierarchy with you:</p> <ol> <li> <p>In a perfect crystal, electrons are stationary Bloch waves extending across the entire lattice</p> </li> <li> <p>Real crystals break symmetry through:</p> </li> <li>Lattice motion (phonons) \u2014 always present</li> <li>Defects and impurities</li> <li> <p>Surfaces and finite size</p> </li> <li> <p>Phonons are organized, quantized lattice motion \u2014 collective oscillations, not random jiggling</p> </li> <li> <p>Phonons force electrons into superpositions of energies and momenta through time-dependent potentials</p> </li> <li> <p>Momentum and energy are conserved \u2014 when an electron gains \\(+\\mathbf{q}\\), a phonon loses \\(-\\mathbf{q}\\) (absorption), or vice versa (emission). Electrons can create and destroy phonons; the interaction is bidirectional, not electrons being pushed around by a fixed background</p> </li> <li> <p>These superpositions form wave packets \u2014 spatially localized, with uncertain momentum</p> </li> <li> <p>\"Scattering\" is wavefunction reshaping, not collisions \u2014 momentum components are added and removed</p> </li> <li> <p>Classical behavior emerges only after phase coherence is lost over many scattering events</p> </li> </ol>"},{"location":"quantum-mechanics/electrons-in-metals/#the-bottom-line","title":"The Bottom Line","text":"<p>Q: Are electrons in metals Bloch waves or localized particles?</p> <p>A: Bloch waves are the alphabet. Real electrons are words.</p> <p>Bloch states \\(\\psi_{n\\mathbf{k}}\\) are exact solutions only for a perfect, frozen, infinite crystal \u2014 a mathematical idealization. They form a complete basis set. Real electrons are superpositions of Bloch states:</p> \\[\\Psi(\\mathbf{r}) = \\int a(\\mathbf{k}) \\, \\psi_{n\\mathbf{k}}(\\mathbf{r}) \\, d\\mathbf{k}\\] <p>What breaks the symmetry: Phonons, defects, surfaces \u2014 anything that makes \\(V(\\mathbf{r})\\) non-periodic. Phonons are always present (even zero-point motion at \\(T=0\\)).</p> <p>What \"scattering\" actually is: Not collisions. The electron wavefunction gains and loses \\(\\mathbf{k}\\) components. A sharp momentum state spreads into a distribution \u2014 that spread is localization.</p> <p>The intuition pump: Remember the particle-in-a-box with moving walls. Static walls \u2192 electron sits in one energy eigenstate forever. Oscillating walls \u2192 the \\(c_n(t)\\) coefficients change \u2192 electron enters superposition of energy levels. That's exactly what phonons do to Bloch states. A phonon is a moving wall \u2014 a traveling potential deformation that pumps the electron into superpositions of \\(\\mathbf{k}\\) and \\(E\\).</p> <p>Why classical intuition eventually works: After many scattering events, phase coherence is lost. You can no longer track the delicate interference pattern \u2014 only the wave packet's average position and momentum. At that point, it behaves like a localized object bouncing around.</p> <p>The one sentence version:</p> <p>Phonons spread electrons across multiple \\(\\mathbf{k}\\) states; by Fourier duality, uncertainty in momentum is localization in space \u2014 that's why wave packets form and can scatter around like particles.</p>"},{"location":"quantum-mechanics/phonons/","title":"Phonons: What They Are and How They Hit Electrons","text":""},{"location":"quantum-mechanics/phonons/#the-two-layer-picture","title":"The Two-Layer Picture","text":"<p>Solid-state physics is built on a sneaky trick: we solve for electrons assuming the ions are frozen in place, then we let the ions move and treat it as a perturbation.</p> <p>Layer 1 \u2014 Frozen lattice (idealization): You solve Schr\u00f6dinger's equation for an electron in a perfectly periodic potential \\(V(\\mathbf{r})\\). The solutions are Bloch states \\(|n,\\mathbf{k}\\rangle\\) \u2014 the \"alphabet\" for describing electrons. These are exact eigenstates, and in this frozen world, electrons never scatter.</p> <p>Notation: \\(|n,\\mathbf{k}\\rangle\\) is Dirac notation for a Bloch wavefunction. Here \\(n\\) is the band index (which energy band) and \\(\\mathbf{k}\\) is the crystal momentum (wavevector in the Brillouin zone). In position space, this state has the form \\(\\psi_{n\\mathbf{k}}(\\mathbf{r}) = e^{i\\mathbf{k}\\cdot\\mathbf{r}} u_{n\\mathbf{k}}(\\mathbf{r})\\), where \\(u\\) has the periodicity of the lattice. (Note: this \\(n\\) is the band index; later we'll use \\(n\\) for phonon occupation number \u2014 context will make clear which is which.)</p> <p>Layer 2 \u2014 Moving lattice (reality): Ions vibrate. The potential becomes time-dependent: \\(V(\\mathbf{r},t) = V_0(\\mathbf{r}) + \\delta V(\\mathbf{r},t)\\). That \\(\\delta V\\) is made of normal modes. Quantize those modes \u2192 phonons. Electrons scatter: \\(|\\mathbf{k}\\rangle \\rightarrow |\\mathbf{k} \\pm \\mathbf{q}\\rangle\\).</p> <p>Notation: When we write \\(|\\mathbf{k}\\rangle\\) (dropping the band index \\(n\\)), we're assuming the electron stays in the same band during scattering. The arrow \\(|\\mathbf{k}\\rangle \\rightarrow |\\mathbf{k} \\pm \\mathbf{q}\\rangle\\) means the electron's crystal momentum changes by \\(\\pm\\mathbf{q}\\) (the phonon's wavevector) \u2014 plus for absorption, minus for emission.</p> Text Only<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Layer 1: Frozen lattice (idealization)                       \u2502\n\u2502   ions fixed \u2192 V(r) periodic \u2192 Bloch states |n,k\u27e9            \u2502\n\u2502   (alphabet for electrons)                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2193 now un-freeze the ions\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Layer 2: Moving lattice (reality)                            \u2502\n\u2502   ions wiggle \u2192 V(r,t) = V\u2080(r) + \u03b4V(r,t)                     \u2502\n\u2502   \u03b4V is made of normal modes \u2192 quantize \u2192 phonons            \u2502\n\u2502   electrons scatter: |k\u27e9 \u2192 |k\u00b1q\u27e9                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Scope note: This tutorial is metal-first. Many simple metals (Cu, Al, Na) are monatomic Bravais lattices with only acoustic phonon branches. Optical phonons are most important in polar semiconductors (GaAs), ionic crystals (NaCl), and high-field transport.</p>"},{"location":"quantum-mechanics/phonons/#mental-model-map","title":"Mental Model Map","text":"Text Only<pre><code>Frozen lattice (idealization)\n  \u2514\u2500\u2192 Schr\u00f6dinger equation \u2192 Bloch states (alphabet for electrons)\n\nAllow ions to move (reality)\n  \u2514\u2500\u2192 Newton's equations \u2192 normal modes (CLASSICAL waves)\n      \u2514\u2500\u2192 Quantization \u2192 phonons (energy quanta \u210f\u03c9)\n\nPhonons present\n  \u2514\u2500\u2192 Time-dependent potential perturbation\n      \u2514\u2500\u2192 Electrons scatter (\u0394k, \u0394E conserved with phonon)\n\nMacroscopic result\n  \u2514\u2500\u2192 Resistivity, velocity limits, thermal conductivity\n</code></pre>"},{"location":"quantum-mechanics/phonons/#part-1-the-1d-monatomic-chain-where-phonons-come-from","title":"Part 1: The 1D Monatomic Chain \u2014 Where Phonons Come From","text":""},{"location":"quantum-mechanics/phonons/#the-toy-model","title":"The Toy Model","text":"<p>Picture a line of identical atoms, each with mass \\(M\\), connected by springs of stiffness \\(K\\), spaced by lattice constant \\(a\\):</p> Text Only<pre><code>    K       K       K       K\n\u25cb\u2500\u2500\u2500\u228f\u2290\u228f\u2290\u2500\u2500\u2500\u25cb\u2500\u2500\u2500\u228f\u2290\u228f\u2290\u2500\u2500\u2500\u25cb\u2500\u2500\u2500\u228f\u2290\u228f\u2290\u2500\u2500\u2500\u25cb\u2500\u2500\u2500\u228f\u2290\u228f\u2290\u2500\u2500\u2500\u25cb\n   M        M        M        M        M\n   |&lt;--a--&gt;|\n</code></pre> <p>Let \\(u_j(t)\\) be the displacement of atom \\(j\\) from its equilibrium position.</p>"},{"location":"quantum-mechanics/phonons/#newtons-law-for-atom-j","title":"Newton's Law for Atom \\(j\\)","text":"<p>Each atom feels spring forces from its two neighbors:</p> \\[ M\\frac{d^2 u_j}{dt^2} = K(u_{j+1} - u_j) - K(u_j - u_{j-1}) = K(u_{j+1} - 2u_j + u_{j-1}) \\] <p>This is Newton's second law (\\(F = M \\times \\text{acceleration}\\)) with Hooke's law springs on both sides. (Note: don't confuse the acceleration with \\(a\\) the lattice constant!)</p>"},{"location":"quantum-mechanics/phonons/#the-traveling-wave-solution","title":"The Traveling Wave Solution","text":"<p>Try a plane wave:</p> \\[ u_j(t) = A \\, e^{i(kx_j - \\omega t)} = A \\, e^{i(kja - \\omega t)} \\] <p>where \\(x_j = ja\\) is the equilibrium position of atom \\(j\\) (the \\(j\\)th atom sits at position \\(j \\times a\\) along the chain). Here \\(j\\) is the atom index \u2014 an integer labeling which atom.</p> <p>Plug into Newton's law, do some algebra (the \\(e^{i(kja-\\omega t)}\\) cancels), and you get the dispersion relation:</p> \\[ \\omega(k) = 2\\sqrt{\\frac{K}{M}} \\left|\\sin\\left(\\frac{ka}{2}\\right)\\right| \\] <p>This tells you how frequency depends on wavevector \u2014 the phonon's \"E vs k\" curve.</p> Text Only<pre><code>\u03c9\n\u2502            \u2322\u2322\u2322\n\u2502         \u2322'     '\u2322         v_g = d\u03c9/dk\n\u2502      \u2322'           '\u2322      (slope of curve)\n\u2502   \u2322'                 '\u2322\n\u251c\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500 k\n  -\u03c0/a       0         +\u03c0/a\n       First Brillouin Zone\n\nNear k\u22480: slope \u2248 speed of sound in the material (linear dispersion)\nNear zone edge: slope \u2192 0  (group velocity vanishes, standing-wave-like)\n</code></pre>"},{"location":"quantum-mechanics/phonons/#numerical-example-copper","title":"Numerical Example: Copper","text":"<p>Let's plug in real numbers:</p> <ul> <li>\\(M = 63.5\\) amu \\(= 1.05 \\times 10^{-25}\\) kg</li> <li>\\(a = 0.36\\) nm (lattice constant)</li> <li>\\(K \\approx 14\\) N/m (effective spring constant from elastic modulus)</li> </ul> <p>Maximum frequency (at zone edge \\(k = \\pi/a\\)):</p> \\[ \\omega_{\\max} = 2\\sqrt{\\frac{K}{M}} = 2\\sqrt{\\frac{14}{1.05 \\times 10^{-25}}} \\approx 2.3 \\times 10^{13} \\text{ rad/s} \\] <p>Converting: - \\(f_{\\max} = \\omega_{\\max}/2\\pi \\approx 3.7\\) THz - \\(\\hbar\\omega_{\\max} \\approx 15\\) meV</p> <p>So the highest-energy acoustic phonons in copper carry about 15 meV \u2014 compare this to room temperature thermal energy \\(k_B T \\approx 26\\) meV.</p>"},{"location":"quantum-mechanics/phonons/#what-the-phase-does","title":"What the Phase Does","text":"<p>A normal mode means every atom oscillates at the same frequency, but with a phase that shifts along the chain:</p> Text Only<pre><code>Equilibrium positions:\n|----a----|----a----|----a----|----a----|\n\u25cb         \u25cb         \u25cb         \u25cb         \u25cb\n\nTraveling wave u_j(t) = A cos(kja - \u03c9t) for wavelength \u03bb = 4a:\n\nt = 0:      \u2191     \u2192     \u2193     \u2190     \u2191     (phase shifts by 90\u00b0 per atom)\nt = T/4:    \u2192     \u2193     \u2190     \u2191     \u2192     (pattern has moved right)\nt = T/2:    \u2193     \u2190     \u2191     \u2192     \u2193\n</code></pre> <p>The pattern travels to the right \u2014 that's what makes it a traveling wave. For longer wavelengths (smaller \\(k\\)), adjacent atoms are more in phase; for shorter wavelengths (larger \\(k\\)), the phase shifts more rapidly.</p>"},{"location":"quantum-mechanics/phonons/#part-2-quantization-from-waves-to-phonons","title":"Part 2: Quantization \u2014 From Waves to Phonons","text":""},{"location":"quantum-mechanics/phonons/#the-critical-clarification","title":"The Critical Clarification","text":"<p>The normal modes and dispersion relation are CLASSICAL. They come from Newton's equations, not Schr\u00f6dinger. Quantization does NOT change the mode shapes or dispersion; it only discretizes the energy in each mode.</p>"},{"location":"quantum-mechanics/phonons/#the-quantum-step","title":"The Quantum Step","text":"<p>Here's how we bridge from Newton to quantum \u2014 no Schr\u00f6dinger equation for atom positions!</p> <p>Step 1: Rewrite in normal mode coordinates. The displacements \\(u_j(t)\\) can be decomposed into independent normal modes. Define a collective coordinate \\(Q_k(t)\\) for each mode:</p> \\[ u_j(t) = \\frac{1}{\\sqrt{N}} \\sum_k Q_k(t) \\, e^{ikja} \\] <p>What is \\(Q_k(t)\\)? It's the time-dependent amplitude of mode \\(k\\). Remember our classical traveling wave solution \\(u_j = A e^{i(kja - \\omega t)}\\)? Here we've separated the spatial part (\\(e^{ikja}\\)) from the time part. The time dependence lives in \\(Q_k(t)\\), which oscillates as: $\\(Q_k(t) = Q_k(0) \\, e^{-i\\omega_k t}\\)$ This comes from solving Newton's equation for mode \\(k\\): \\(\\ddot{Q}_k = -\\omega_k^2 Q_k\\) gives simple harmonic motion at frequency \\(\\omega_k\\).</p> <p>Why a discrete sum, not an integral? With \\(N\\) atoms in your chain, there are exactly \\(N\\) independent ways to wiggle them \u2014 hence \\(N\\) normal modes with \\(N\\) allowed \\(k\\) values. This comes from boundary conditions (periodic: \\(u_{j+N} = u_j\\), which forces \\(e^{ikNa} = 1\\), so \\(k = 2\\pi m / Na\\) for \\(m = 0, 1, ..., N-1\\)). In the thermodynamic limit \\(N \\to \\infty\\), the spacing between \\(k\\) values shrinks to zero and the sum becomes an integral: \\(\\frac{1}{N}\\sum_k \\to \\frac{a}{2\\pi}\\int dk\\).</p> <p>Why complex exponentials? This is shorthand. The physical displacement \\(u_j\\) must be real. To get a real \\(u_j\\) from this sum, we need \\(Q_{-k} = Q_k^*\\) (complex conjugate). Then \\(Q_k e^{ikja} + Q_{-k} e^{-ikja} = 2\\text{Re}[Q_k e^{ikja}]\\), which is real. The complex form is mathematically cleaner than writing cosines with phases.</p> <p>What does a single mode look like? Combining \\(Q_k(t) = Q_k(0) e^{-i\\omega_k t}\\) with \\(e^{ikja}\\): $\\(Q_k(t) \\, e^{ikja} = Q_k(0) \\, e^{i(kja - \\omega_k t)}\\)$ Taking the real part (using Euler: \\(e^{i\\theta} = \\cos\\theta + i\\sin\\theta\\)), a single mode is a sinusoidal pattern: $\\(u_j^{(k)}(t) \\propto \\cos(kja - \\omega_k t + \\phi_k)\\)$ Each atom oscillates at frequency \\(\\omega_k\\), but with a phase shift \\(kja\\) that increases along the chain \u2014 this is what makes it a traveling wave.</p> <p>What's \\(ja\\)? It's the position of atom \\(j\\) along the chain. In a continuous medium you'd write \\(e^{ikx}\\); here we only have atoms at discrete positions \\(x_j = ja\\) (where \\(a\\) is the lattice spacing), so we write \\(e^{ikja}\\).</p> <p>Why is \\(k\\) limited to \\([-\\pi/a, \\pi/a]\\)? This is aliasing from discrete sampling! A wave with wavevector \\(k\\) and a wave with \\(k + 2\\pi/a\\) look identical when sampled only at positions \\(ja\\): $\\(e^{i(k + 2\\pi/a)ja} = e^{ikja} \\cdot e^{i 2\\pi j} = e^{ikja} \\cdot 1 = e^{ikja}\\)$ So there's no point considering \\(k\\) outside the first Brillouin zone \\([-\\pi/a, \\pi/a]\\) \u2014 those waves are just aliases of waves inside. This is the same math as the Nyquist limit in signal processing: with spacing \\(a\\), you can only distinguish wavelengths \\(\\lambda &gt; 2a\\).</p> <p>Each \\(Q_k\\) evolves independently: \\(\\ddot{Q}_k = -\\omega_k^2 Q_k\\). This is just a harmonic oscillator equation for \\(Q_k\\).</p> <p>Step 2: Quantize each oscillator. The energy (Hamiltonian) for mode \\(k\\) is:</p> \\[ H_k = \\frac{P_k^2}{2M_{\\text{eff}}} + \\frac{1}{2}M_{\\text{eff}}\\omega_k^2 Q_k^2 \\] <p>What's the Hamiltonian? It's just total energy = kinetic + potential. This formula exists in classical mechanics too (Hamilton wrote it down in the 1830s, before quantum mechanics). The first term is kinetic energy, the second is potential energy of a spring. \\(M_{\\text{eff}}\\) is an effective mass that comes from the coordinate transformation \u2014 for our simple chain it's just \\(M\\), the atomic mass.</p> <p>Now we do use Schr\u00f6dinger \u2014 but for the collective coordinate \\(Q_k\\), not for individual atom positions. Since \\(H_k\\) has the form of a harmonic oscillator, we know the answer: promote \\(Q_k\\) and \\(P_k\\) to operators, and the energy eigenvalues are \\((n + \\frac{1}{2})\\hbar\\omega\\).</p> <p>Key insight: There is a wavefunction \u2014 it's \\(\\Psi(Q_k)\\), a function of the collective coordinate (like the Gaussian ground state of a harmonic oscillator). But we almost never write it. Instead, we just track the occupation number \\(n_k\\) (how many phonons in mode \\(k\\)).</p> <p>Each normal mode (each \\(k\\)) is mathematically a harmonic oscillator. When you quantize it (standard QM harmonic oscillator), you get discrete energy levels:</p> \\[ E_n = \\left(n + \\frac{1}{2}\\right)\\hbar\\omega \\] <p>where \\(n = 0, 1, 2, 3, \\ldots\\) is the occupation number (number of phonons in that mode). Note: \\(n\\) here is the quantum number, not the atom index \\(j\\) from earlier.</p> <p>A phonon is one quantum of energy \\(\\hbar\\omega\\) in that mode \u2014 not the wave itself.</p> Text Only<pre><code>Harmonic oscillator levels for one mode (wavevector k):\nE\n\u2502        (5)  (n+\u00bd)\u210f\u03c9\n\u2502        (4)\n\u2502        (3)\n\u2502        (2)\n\u2502        (1)\n\u2502        (0)  zero-point = \u00bd\u210f\u03c9\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 n\n      each step = \u210f\u03c9 = one phonon\n</code></pre> <p>Important \u2014 Amplitude is subtle! You might think: classically \\(E = \\frac{1}{2}m\\omega^2 A^2\\), so if energy is discrete, shouldn't amplitude be discrete too?</p> <p>Partly yes, partly no. In an energy eigenstate \\(|n\\rangle\\):</p> <ul> <li>The average displacement \\(\\langle Q \\rangle = 0\\) (always \u2014 the wavefunction is symmetric about the origin)</li> <li>The RMS amplitude \\(\\sqrt{\\langle Q^2 \\rangle} = \\sqrt{n + \\frac{1}{2}} \\times \\sqrt{\\frac{\\hbar}{M_{\\text{eff}}\\omega}}\\) \u2014 this IS quantized!</li> <li>But if you measure \\(Q\\), you can get any real value (continuous outcomes, drawn from \\(|\\Psi(Q)|^2\\))</li> </ul> <p>So more energy does mean larger RMS amplitude, and since energy is quantized, the RMS amplitude only takes discrete values. What's NOT quantized is the measurement outcome \u2014 the oscillator isn't sitting at one specific displacement.</p> <p>The deeper point: You can't have both sharp energy AND sharp amplitude. An energy eigenstate has definite energy but fuzzy position. A coherent state (classical-like oscillation with definite amplitude \\(A\\cos(\\omega t)\\)) has fuzzy energy \u2014 it's a superposition of many \\(|n\\rangle\\) states.</p>"},{"location":"quantum-mechanics/phonons/#the-photon-analogy","title":"The Photon Analogy","text":"<p>In the table below, \\(n\\) is the occupation number \u2014 how many quanta (photons or phonons) are in that mode:</p> Electromagnetic Field Lattice Vibrations EM mode (k, polarization) Vibration mode (q, branch) Photon = 1 quantum of EM energy Phonon = 1 quantum of vibrational energy \\(n\\) photons \u2192 E-field amplitude \\(\\propto \\sqrt{n}\\) \\(n\\) phonons \u2192 displacement amplitude \\(\\propto \\sqrt{n}\\) Large \\(n\\) \u2192 classical EM wave Large \\(n\\) \u2192 classical sound wave <p>When many quanta occupy a mode, it looks like a classical wave. When \\(n\\) is small (or zero), quantum effects dominate.</p>"},{"location":"quantum-mechanics/phonons/#numerical-example-how-many-phonons","title":"Numerical Example: How Many Phonons?","text":"<p>Consider a mode at \\(\\omega = 2\\pi \\times 1\\) THz:</p> <ul> <li>One phonon energy: \\(\\hbar\\omega = 4.1\\) meV</li> <li>Thermal energy at 300K: \\(k_B T = 26\\) meV</li> </ul> <p>The average number of phonons in this mode at thermal equilibrium (Bose-Einstein statistics):</p> \\[ \\langle n \\rangle = \\frac{1}{e^{\\hbar\\omega/k_B T} - 1} = \\frac{1}{e^{4.1/26} - 1} \\approx 5.3 \\text{ phonons} \\] <p>Here \\(\\langle n \\rangle\\) is the thermal average of the occupation number \u2014 how many phonons are typically in this mode at temperature \\(T\\).</p> <p>So at room temperature, this mode has about 5 phonons on average \u2014 firmly in the \"classical-looking\" regime.</p>"},{"location":"quantum-mechanics/phonons/#part-3-longitudinal-vs-transverse-this-is-about-polarization","title":"Part 3: Longitudinal vs Transverse \u2014 This is About Polarization","text":"<p>This classification is about the direction of displacement relative to propagation:</p> Text Only<pre><code>Propagation direction:  \u2500\u2500\u2500\u2500\u2500&gt;  k\n\nLA (longitudinal acoustic): compression / dilation\n\u25cb\u2500\u2500\u2500&gt; \u25cb\u2500\u2500\u2500&gt; \u25cb\u2500\u2500\u2500&gt; \u25cb\u2500\u2500\u2500&gt; \u25cb\u2500\u2500\u2500&gt;\n(atoms move parallel to wave propagation)\n\nTA (transverse acoustic): shear-like wiggle\n\u25cb  ^  \u25cb  v  \u25cb  ^  \u25cb  v  \u25cb  ^\n(atoms move perpendicular to wave propagation)\n</code></pre> <p>In 3D crystals: - 1 longitudinal branch (compression wave) - 2 transverse branches (two perpendicular shear directions) - Total: 3 acoustic branches for a monatomic crystal</p> <p>Key point: Longitudinal/transverse is a separate concept from acoustic/optical. You can have: - Longitudinal acoustic (LA) - Transverse acoustic (TA) - Longitudinal optical (LO) - Transverse optical (TO)</p>"},{"location":"quantum-mechanics/phonons/#part-4-acoustic-vs-optical-the-real-distinction","title":"Part 4: Acoustic vs Optical \u2014 The REAL Distinction","text":"<p>This is where most tutorials fail you. Here's the clean version:</p> <p>Optical phonons exist ONLY if the unit cell has more than one atom.</p>"},{"location":"quantum-mechanics/phonons/#the-rule","title":"The Rule","text":"Crystal Type Example Phonon Branches Monatomic Cu, Al, Na Acoustic only (LA + 2\u00d7TA) Diatomic Si, GaAs, NaCl Acoustic + Optical"},{"location":"quantum-mechanics/phonons/#the-1d-diatomic-chain","title":"The 1D Diatomic Chain","text":"<p>Now put two different atoms in each unit cell:</p> Text Only<pre><code>    K       K       K       K       K\n\u25cf\u2500\u2500\u2500\u228f\u2290\u228f\u2290\u2500\u2500\u2500\u25cb\u2500\u2500\u2500\u228f\u2290\u228f\u2290\u2500\u2500\u2500\u25cf\u2500\u2500\u2500\u228f\u2290\u228f\u2290\u2500\u2500\u2500\u25cb\u2500\u2500\u2500\u228f\u2290\u228f\u2290\u2500\u2500\u2500\u25cf\n  M\u2081       M\u2082       M\u2081       M\u2082       M\u2081\n  |&lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 a \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;|\n       primitive unit cell\n</code></pre> <p>Now you get two solutions \u2014 two branches in the dispersion:</p>"},{"location":"quantum-mechanics/phonons/#acoustic-branch-lower-energy","title":"ACOUSTIC Branch (Lower Energy)","text":"Text Only<pre><code>Equilibrium:\nA   B   A   B   A   B\n\u25cf   \u25cb   \u25cf   \u25cb   \u25cf   \u25cb\n\nACOUSTIC (long wavelength): all atoms move IN PHASE\n\u2192   \u2192   \u2192   \u2192   \u2192   \u2192\n\u25cf   \u25cb   \u25cf   \u25cb   \u25cf   \u25cb\n\nThe whole lattice sloshes together like a sound wave.\nAt k\u21920: \u03c9\u21920 (this is the defining property!)\n</code></pre> <p>\"Acoustic\" means \\(\\omega \\to 0\\) as \\(k \\to 0\\) \u2014 NOT \"always low frequency.\" (Note: physicists use \\(k\\) and \\(q\\) interchangeably for phonon wavevector; \\(q\\) is more common when discussing electron-phonon scattering.) Near the zone edge, acoustic phonons can have very high frequencies!</p> Text Only<pre><code>Acoustic branch: \u03c9\u21920 at k\u21920  (that's the definition)\n\n\u03c9\n\u2502         acoustic can be HIGH here\n\u2502                 ^\n\u2502                 \u2502\n\u2502              \u2322' '\u2322\n\u2502           \u2322'       '\u2322\n\u2502        \u2322'             '\u2322\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500 k\n        0                 \u03c0/a\n</code></pre>"},{"location":"quantum-mechanics/phonons/#optical-branch-higher-energy","title":"OPTICAL Branch (Higher Energy)","text":"Text Only<pre><code>Equilibrium:\nA   B   A   B   A   B\n\u25cf   \u25cb   \u25cf   \u25cb   \u25cf   \u25cb\n\nOPTICAL (any wavelength): basis atoms move OUT OF PHASE\n\u2192   \u2190   \u2192   \u2190   \u2192   \u2190\n\u25cf   \u25cb   \u25cf   \u25cb   \u25cf   \u25cb\n\nThe two atoms in each unit cell push against each other.\nAt k=0: \u03c9 \u2260 0! (finite minimum energy)\n</code></pre> <p>Why \"optical\"? In ionic crystals like NaCl, the + and \u2212 ions moving against each other creates an oscillating electric dipole. This couples directly to electromagnetic radiation (infrared light) \u2014 hence \"optical.\"</p>"},{"location":"quantum-mechanics/phonons/#numerical-example-gaas","title":"Numerical Example: GaAs","text":"<ul> <li>LO phonon at k=0: \\(\\hbar\\omega_{LO} = 36\\) meV</li> <li>TO phonon at k=0: \\(\\hbar\\omega_{TO} = 33\\) meV</li> <li>Thermal energy at 300K: \\(k_B T = 26\\) meV</li> </ul> <p>Optical phonons require significant energy to excite \u2014 they're not easily populated at room temperature.</p>"},{"location":"quantum-mechanics/phonons/#the-full-picture","title":"The Full Picture","text":"Text Only<pre><code>Diatomic chain (two atoms per unit cell):  ... A B A B A B ...\n\nEquilibrium:\nA   B   A   B   A   B\n\u25cf   \u25cb   \u25cf   \u25cb   \u25cf   \u25cb\n\nACOUSTIC (k\u21920): in-phase (whole lattice sloshes together)\n\u2192   \u2192   \u2192   \u2192   \u2192   \u2192\n\u25cf   \u25cb   \u25cf   \u25cb   \u25cf   \u25cb\n\u03c9 \u2192 0 as k \u2192 0\n\nOPTICAL (k\u21920): out-of-phase within the unit cell\n\u2192   \u2190   \u2192   \u2190   \u2192   \u2190\n\u25cf   \u25cb   \u25cf   \u25cb   \u25cf   \u25cb\n\u03c9 \u2260 0 at k = 0\n</code></pre>"},{"location":"quantum-mechanics/phonons/#part-5-phonon-dispersion-the-phonon-band-structure","title":"Part 5: Phonon Dispersion \u2014 The Phonon \"Band Structure\"","text":"<p>Just like electrons have \\(E(\\mathbf{k})\\) curves, phonons have \\(\\omega(\\mathbf{q})\\) curves. For a diatomic crystal:</p> Text Only<pre><code>\u210f\u03c9\n\u2502    ____________________   Optical branches (LO, TO)\n\u2502   /                    \\    \u2190 gap (if M\u2081 \u2260 M\u2082)\n\u2502  \u2502                      \u2502\n\u2502\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\n\u2502   \\                    /   Acoustic branches (LA, TA)\n\u2502    \\__________________/\n\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 k\n       -\u03c0/a          \u03c0/a\n</code></pre> <p>Key features: - Acoustic branches touch zero at \\(k=0\\) - Optical branches have finite frequency at \\(k=0\\) - Gap between branches (in diatomic crystals with \\(M_1 \\neq M_2\\)) - Group velocity \\(v_g = d\\omega/dk\\) \u2192 zero at zone boundaries</p>"},{"location":"quantum-mechanics/phonons/#part-6-the-moving-walls-model-how-phonons-affect-electrons","title":"Part 6: The Moving Walls Model \u2014 How Phonons Affect Electrons","text":"<p>Here's the key intuition for electron-phonon coupling, borrowed from basic quantum mechanics.</p>"},{"location":"quantum-mechanics/phonons/#static-box-frozen-lattice","title":"Static Box = Frozen Lattice","text":"<p>A particle in a box with fixed walls:</p> Text Only<pre><code>Static box: sharp energy levels\n0 \u2502====================\u2502 L\u2080\n    \u03c8_n fits cleanly\n\nElectron sits in eigenstate \u03c8_n forever.\nEnergy sharp. Position delocalized.\n</code></pre>"},{"location":"quantum-mechanics/phonons/#oscillating-walls-phonons","title":"Oscillating Walls = Phonons","text":"<p>Now let the walls move:</p> \\[ L(t) = L_0 + \\delta L \\cos(\\Omega t) \\] Text Only<pre><code>Moving wall: L(t) = L\u2080 + \u03b4L cos(\u03a9t)\n0 \u2502==================\u2502 L(t)\n    boundary shakes \u2192 \u03c8_n mixes with \u03c8_m\n</code></pre> <p>The oscillating boundary forces the electron into a superposition of energy levels. It can no longer sit quietly in a single eigenstate.</p>"},{"location":"quantum-mechanics/phonons/#the-mapping-to-phonons","title":"The Mapping to Phonons","text":"Moving Box Crystal with Phonons Wall oscillation frequency \\(\\Omega\\) Phonon frequency \\(\\omega_q\\) Energy pumped by walls Energy exchanged with phonon (\\(\\hbar\\omega\\)) No momentum exchange (1D box) Phonon carries crystal momentum \\(\\hbar q\\) Resonance at \\(\\Omega = (E_2-E_1)/\\hbar\\) Resonance when energy/momentum conserved"},{"location":"quantum-mechanics/phonons/#part-7-electron-phonon-scattering-the-actual-physics","title":"Part 7: Electron-Phonon Scattering \u2014 The Actual Physics","text":""},{"location":"quantum-mechanics/phonons/#what-the-electron-sees","title":"What the Electron Sees","text":"<p>You might wonder: if a phonon is just atoms moving, and in a traveling wave they all move the same direction, how does that create a potential that electrons scatter off?</p> <p>The key is that atoms don't all move the same amount at the same time. The phase shift along the chain creates regions of compression and rarefaction:</p> Text Only<pre><code>Equilibrium (uniform spacing):\n\u25cb     \u25cb     \u25cb     \u25cb     \u25cb     \u25cb\n|--a--|--a--|--a--|--a--|--a--|\n\nSnapshot of longitudinal wave (atoms displaced by different amounts):\n\u25cb   \u25cb\u25cb    \u25cb      \u25cb\u25cb   \u25cb     \u25cb\n   \u2191              \u2191\n compressed    compressed\n (atoms closer)\n         \u2191              \u2191\n      stretched      stretched\n      (atoms farther)\n</code></pre> <p>Where atoms are bunched together: higher ion density \u2192 deeper potential well for electrons</p> <p>Where atoms are spread apart: lower ion density \u2192 shallower potential</p> <p>This creates a traveling potential ripple. Crucially, it's the strain (local compression/stretching) that matters, not the displacement itself:</p> \\[ \\delta V(x,t) \\propto -\\frac{\\partial u}{\\partial x} = \\text{strain} \\] <p>For a displacement wave \\(u(x,t) = A\\cos(qx - \\omega t)\\): $$ \\frac{\\partial u}{\\partial x} = -qA\\sin(qx - \\omega t) $$</p> <p>So the potential perturbation is also a traveling wave (phase-shifted by 90\u00b0):</p> \\[ \\delta V(x,t) \\propto \\sin(qx - \\omega t) \\] <p>Think of it as a moving diffraction grating \u2014 a traveling Bragg reflector that diffracts the electron wave.</p> Text Only<pre><code>Phonon makes the potential look like a moving ripple:\n\nx \u2192\n   \u2500\u2510  \u250c\u2500    \u2500\u2510  \u250c\u2500    \u2500\u2510  \u250c\u2500\n    \u2514\u2500\u2500\u2518      \u2514\u2500\u2500\u2518      \u2514\u2500\u2500\u2518     (potential wells travel right)\n     \u2191 compressed  \u2191 compressed\n\nElectron sees a moving diffraction grating:\nk  \u2192  k \u00b1 q   (plus possibly +G)\n</code></pre> <p>Wavelength matters: Long wavelength (small \\(q\\)) means gentle, gradual compression \u2014 weak potential gradient, weak scattering. Short wavelength (large \\(q\\)) means rapid compression/stretching \u2014 stronger potential gradient, stronger scattering.</p>"},{"location":"quantum-mechanics/phonons/#two-views-of-the-same-physics","title":"Two Views of the Same Physics","text":"<p>Here's something important: \"scattering off the potential ripple\" and \"absorbing/emitting a phonon\" are the same event viewed through different lenses:</p> Classical View Quantum View Electron scatters off traveling potential \\(\\delta V(x,t)\\) Electron absorbs or emits a phonon Energy/momentum transfer seems continuous Discrete: \\(\\hbar\\omega\\) energy, \\(\\hbar q\\) momentum Wave with amplitude \\(A\\) Mode with occupation number \\(n\\) <p>The scattering strength (matrix element) comes from the classical \\(\\delta V\\). The quantization tells you the energy/momentum exchange comes in discrete chunks \u2014 each chunk is one phonon.</p> <p>This is exactly like light: an electromagnetic wave scattering an electron (classical) is the same as a photon being absorbed/emitted (quantum). Same physics, two descriptions.</p>"},{"location":"quantum-mechanics/phonons/#conservation-laws","title":"Conservation Laws","text":"<p>When an electron absorbs or emits a phonon:</p> Text Only<pre><code>Phonon absorption:           Phonon emission:\nk' = k + q                   k' = k - q\nE' = E + \u210f\u03c9                  E' = E - \u210f\u03c9\n(phonon destroyed)           (phonon created)\n</code></pre> <p>Phonons are quantized \u2014 you absorb or emit the whole phonon, not a piece of it.</p>"},{"location":"quantum-mechanics/phonons/#umklapp-zone-folding","title":"Umklapp (Zone Folding)","text":"<p>Sometimes \\(k + q\\) lands outside the first Brillouin zone. No problem \u2014 fold it back:</p> \\[ k' = k \\pm q + G \\] <p>where \\(G\\) is a reciprocal lattice vector (\\(G = 2\\pi/a\\) in 1D). \"Umklapp\" is German for \"flip over\" \u2014 the electron's momentum flips back into the first zone. This is crucial for thermal resistance: Umklapp processes can transfer momentum to the lattice, while normal processes cannot.</p> Text Only<pre><code>k-space (1D):\n... \u2502\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2502 ...\n    -2\u03c0/a  -\u03c0/a  0  +\u03c0/a +2\u03c0/a\n\n1st Brillouin zone: [-\u03c0/a, +\u03c0/a]\n\nNormal:    k' = k + q      stays in same zone\nUmklapp:   k' = k + q - G  folds back into 1st zone\n\nk+q  (lands outside zone)  \u2500\u2500\u2500&gt;  subtract G  \u2500\u2500\u2500&gt;  folded k'\n</code></pre>"},{"location":"quantum-mechanics/phonons/#numerical-example-scattering-in-copper","title":"Numerical Example: Scattering in Copper","text":"<p>Electron near the Fermi surface: - \\(k_F \\approx 13.6\\) nm\\(^{-1}\\) - \\(E_F \\approx 7\\) eV</p> <p>Acoustic phonon: - \\(q \\approx 1\\) nm\\(^{-1}\\) - \\(\\hbar\\omega \\approx 5\\) meV</p> <p>After absorbing the phonon: - New wavevector: \\(k' \\approx 14.6\\) nm\\(^{-1}\\) - Momentum change: significant (\\(\\Delta k/k \\sim 7\\%\\)) - Energy change: tiny (5 meV vs 7000 meV, or 0.07%)</p> Text Only<pre><code>E\n\u2502     E_F  _______________________\n\u2502         /                       \\\n\u2502        /                         \\\n\u2502_______/___________________________\\_____ k\n          k_F\n\nphonon scattering mostly changes direction on the Fermi surface\n(energy shift is small: meV vs eV)\n\n\u0394E:  \u2502\u00b7\u2502      \u0394k: \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\n     meV           big\n</code></pre> <p>This is why acoustic phonon scattering randomizes direction but barely changes speed.</p>"},{"location":"quantum-mechanics/phonons/#part-8-physical-consequences","title":"Part 8: Physical Consequences","text":""},{"location":"quantum-mechanics/phonons/#acoustic-phonon-scattering-low-fields-moderate-t","title":"Acoustic Phonon Scattering (Low Fields, Moderate T)","text":"<p>Acoustic phonons modulate the lattice spacing \u2192 compression/dilation \u2192 band edges shift locally. Electrons scatter off these potential variations.</p> <ul> <li>Small energy transfer per scattering event</li> <li>Large angle scattering (momentum randomized)</li> <li>Dominates resistivity at moderate temperatures: \\(\\rho \\propto T\\) at high \\(T\\)</li> </ul>"},{"location":"quantum-mechanics/phonons/#optical-phonon-scattering-high-fields","title":"Optical Phonon Scattering (High Fields)","text":"<p>Optical phonons have a minimum energy (\\(\\sim\\)30-60 meV). An electron needs at least this much kinetic energy to emit one.</p> Text Only<pre><code>E_electron\n\u2502\n\u2502     /  \u2190 electron gains energy from field\n\u2502    /\n\u2502\u2500\u2500\u2500/\u2500\u2500\u2500\u2500\u210f\u03c9_opt\u2500\u2500\u2500\u2500 threshold\n\u2502  /     \u2193 emit optical phonon, lose energy\n\u2502 /\n\u2502/__________________ k\n</code></pre> <p>Once an electron reaches the threshold, it can efficiently dump energy by emitting optical phonons. This causes velocity saturation in semiconductors \u2014 no matter how hard you push, the electron can't go faster because it keeps emitting optical phonons.</p>"},{"location":"quantum-mechanics/phonons/#temperature-dependence","title":"Temperature Dependence","text":"<p>Higher \\(T\\) \u2192 more phonons \u2192 more scattering \u2192 lower mobility</p> <p>At high temperature (\\(k_B T \\gg \\hbar\\omega\\)), the Bose-Einstein distribution simplifies: $$ \\langle n \\rangle = \\frac{1}{e^{\\hbar\\omega/k_B T} - 1} \\approx \\frac{k_B T}{\\hbar\\omega} $$</p> <p>So phonon occupation \u2014 and hence scattering rate and resistivity \u2014 grows roughly linearly with \\(T\\).</p>"},{"location":"quantum-mechanics/phonons/#quick-reference-summary","title":"Quick Reference Summary","text":""},{"location":"quantum-mechanics/phonons/#whats-classical-vs-quantum","title":"What's Classical vs Quantum","text":"Aspect Classical or Quantum? Normal modes, shapes, dispersion \\(\\omega(k)\\) Classical (Newton) Energy quantization \\(E = (n+\\frac{1}{2})\\hbar\\omega\\) Quantum RMS amplitude in energy eigenstate Quantized (scales with \\(\\sqrt{n+1/2}\\)) Measurement outcome of displacement Continuous (any value from $ Energy in a mode Discrete (\\(\\hbar\\omega\\) chunks)"},{"location":"quantum-mechanics/phonons/#phonon-basics","title":"Phonon Basics","text":"<ul> <li>Mode = collective oscillation with wavevector \\(k\\) (or \\(q\\)), frequency \\(\\omega(k)\\)</li> <li>Phonon = one quantum of energy \\(\\hbar\\omega\\) in that mode (NOT the wave itself)</li> <li>Large amplitude \u2248 many phonons (classical limit)</li> </ul>"},{"location":"quantum-mechanics/phonons/#acoustic-vs-optical-memorize-this","title":"Acoustic vs Optical (Memorize This)","text":"Property Acoustic Optical Atoms move... In phase (together) Out of phase (against each other) At \\(k \\to 0\\)... \\(\\omega \\to 0\\) \\(\\omega \\neq 0\\) (finite minimum) Exists in... ALL crystals Only if &gt;1 atom per unit cell Common misconception \"Low frequency\" \u2014 wrong! Can be high near zone edge \u2014"},{"location":"quantum-mechanics/phonons/#longitudinal-vs-transverse","title":"Longitudinal vs Transverse","text":"<ul> <li>Longitudinal: displacement parallel to propagation (compression wave)</li> <li>Transverse: displacement perpendicular to propagation (shear wave)</li> <li>SEPARATE concept from acoustic/optical!</li> </ul>"},{"location":"quantum-mechanics/phonons/#electron-phonon-interaction","title":"Electron-Phonon Interaction","text":"<ul> <li>Bloch states = exact solutions for frozen lattice (the alphabet)</li> <li>Phonon = traveling potential perturbation on that frozen-lattice solution</li> <li>Transfers: energy \\(\\hbar\\omega\\) and crystal momentum \\(\\hbar q\\) (mod \\(G\\))</li> <li>Acoustic scattering: randomizes direction, small \\(\\Delta E\\)</li> <li>Optical scattering: large \\(\\Delta E\\), velocity saturation mechanism</li> </ul>"},{"location":"quantum-mechanics/quantum-basis-representation/","title":"Quantum Mechanics: Basis Representation","text":"<p>Prerequisites: This builds on Quantum Foundations where we derived Schr\u00f6dinger's equation and explored the harmonic oscillator. If you haven't read that yet, start there first!</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#the-harmonic-oscillator-one-state-three-representations","title":"The Harmonic Oscillator: One State, Three Representations","text":"<p>In Part 1, we found that the harmonic oscillator has discrete energy levels \\(E_n = \\hbar\\omega(n + \\frac{1}{2})\\) with corresponding energy eigenstates \\(\\psi_n(x)\\) (Gaussians with increasing nodes). Here's the key insight: we can describe the same quantum state in three different \"languages\" depending on what we want to measure.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#the-solutions","title":"The Solutions","text":"Representation Variable Expression Domain Position \\(x\\) \\(\\psi(x,t) = \\sum_n c_n e^{-iE_n t/\\hbar} \\psi_n(x)\\) continuous Momentum \\(p\\) \\(\\psi(p,t) = \\sum_n c_n e^{-iE_n t/\\hbar} \\phi_n(p)\\) continuous Energy (index) \\(n\\) \\(\\psi_E[n,t] = c_n e^{-iE_n t/\\hbar} = \\sum_m c_m e^{-iE_m t/\\hbar} \\delta_{n,m}\\) discrete <p>Think of these as:</p> <ul> <li>\\(\\psi(x,t)\\) \u2014 amplitudes in space</li> <li>\\(\\psi(p,t)\\) \u2014 amplitudes in momentum</li> <li>\\(\\psi_E[n,t]\\) \u2014 amplitudes in energy index (n)</li> </ul> <p>The position and momentum representations are continuous functions (infinitely many values), while the energy representation is a discrete list of coefficients \\(c_0, c_1, c_2, ...\\) (one for each energy level).</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#the-energy-rotation-phasor","title":"The Energy Rotation Phasor","text":"<p>The factor \\(e^{-iE_n t/\\hbar}\\) is the energy rotation phasor. Each energy level rotates in the complex plane at its own rate:</p> <ul> <li>Higher energy \u2192 faster rotation (higher \\(E_n\\) means faster \\(\\omega_n = E_n/\\hbar\\))</li> <li>Lower energy \u2192 slower rotation</li> <li>The phase \\(\\phi_n(t) = -E_n t/\\hbar\\) advances linearly in time</li> </ul> <p>This phase rotation is fundamental to quantum dynamics, but for now, we'll work at \\(t=0\\) for simplicity. The spatial structure doesn't change with time\u2014only these phases rotate. Understanding basis representation at \\(t=0\\) captures all the key concepts.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#normalization","title":"Normalization","text":"<p>The energy eigenstates \\(\\psi_0(x), \\psi_1(x), \\psi_2(x), ...\\) in the position representation are all normalized, meaning:</p> \\[\\int_{-\\infty}^{\\infty} |\\psi_n(x)|^2 dx = 1\\] <p>This ensures the total probability of finding the particle somewhere is 100%. When we write a general state as a sum \\(\\psi(x) = \\sum_n c_n \\psi_n(x)\\), each \\(\\psi_n(x)\\) already satisfies this normalization condition.</p> <p>The coefficients \\(c_n\\) are ALSO normalized. Since \\(|c_n|^2\\) represents the probability of measuring energy \\(E_n\\), all probabilities must sum to 1:</p> \\[\\sum_{n=0}^{\\infty} |c_n|^2 = 1\\] <p>For our running example with \\(c_0 = \\frac{1}{2}\\), \\(c_1 = \\frac{\\sqrt{2}}{2}\\), \\(c_2 = \\frac{1}{2}\\) (all others zero), let's verify:</p> \\[|c_0|^2 + |c_1|^2 + |c_2|^2 = \\left|\\frac{1}{2}\\right|^2 + \\left|\\frac{\\sqrt{2}}{2}\\right|^2 + \\left|\\frac{1}{2}\\right|^2 = \\frac{1}{4} + \\frac{1}{2} + \\frac{1}{4} = 1 \\,\\checkmark\\] <p>This holds for any valid quantum state\u2014the probability of measuring something must always be 100%.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#why-choose-different-representations","title":"Why Choose Different Representations?","text":"<p>Each representation makes certain questions trivial:</p> <p>\"What's the probability of energy \\(E_1\\)?\" \u2192 Energy basis: just \\(|c_1|^2\\). One number!</p> <p>\"Where is the particle most likely to be found?\" \u2192 Position basis: find max of \\(|\\psi(x)|^2\\)</p> <p>\"What's the momentum distribution?\" \u2192 Momentum basis: \\(|\\psi(p)|^2\\) directly tells you the probability density in momentum space. Useful for scattering problems, particle physics, and understanding wave packets.</p> <p>The art of quantum mechanics is choosing the right tool for the job. Let's see how this works!</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#what-is-the-quantum-state","title":"What IS the Quantum State?","text":"<p>Here's the deeper question: What IS the quantum state? Is it the function \\(\\psi(x)\\)? The coefficients \\([c_0, c_1, c_2, ...]\\)? The momentum function \\(\\psi(p)\\)? Or something more fundamental?</p> <p>The answer: there's an abstract quantum state \\(|\\psi\\rangle\\) (called a \"ket\") that exists independent of how we describe it. The different functions we use are just different ways of writing down the same underlying thing:</p> <ul> <li>\\(\\psi(x)\\) = position representation (a function of position)</li> <li>\\(\\psi(p)\\) = momentum representation (a function of momentum)</li> <li>\\([c_0, c_1, c_2, ...]\\) = energy representation (a list of coefficients)</li> </ul> <p>Same state, different descriptions! Like how a 3D vector doesn't change whether you use Cartesian or spherical coordinates. The vector itself is the same; only the numbers you write change.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#the-abstract-state","title":"The Abstract State |\u03c8\u27e9","text":"<p>The ket \\(|\\psi\\rangle\\) is the complete quantum state\u2014the thing itself before you choose how to describe it.</p> <p>The vector analogy: An arrow in 3D space doesn't change if you describe it using Cartesian coordinates \\((x, y, z)\\) or cylindrical coordinates \\((r, \\theta, z)\\). It's the same arrow, same length, same direction. Only the numbers you write down change. The arrow itself is basis-independent.</p> <p>Similarly, \\(|\\psi\\rangle\\) exists independent of whether you ask about position, momentum, or energy. It contains all the information; you just need to choose which representation to use.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#projecting-the-state-into-different-realms","title":"Projecting the State into Different \"Realms\"","text":"<p>Here's how we extract different representations from the abstract state \\(|\\psi\\rangle\\). We project it into the realm of whatever physical property we want to observe:</p> \\[\\langle x|\\psi\\rangle = \\psi(x) \\quad \\text{(position wavefunction)}\\] \\[\\langle p|\\psi\\rangle = \\tilde{\\psi}(p) \\quad \\text{(momentum wavefunction)}\\] \\[\\langle n|\\psi\\rangle = c_n \\quad \\text{(energy coefficient)}\\] <p>Don't worry about the \\(\\langle\\) \\(|\\) notation yet\u2014we'll explain it thoroughly in the next section. For now, the key idea is:</p> <ul> <li>The abstract state \\(|\\psi\\rangle\\) contains everything</li> <li>Projections extract specific aspects we care about</li> <li>Want to know about position? Project onto position basis with \\(\\langle x|\\)</li> <li>Want to know about energy? Project onto energy basis with \\(\\langle n|\\)</li> <li>Want to know about momentum? Project onto momentum basis with \\(\\langle p|\\)</li> </ul> <p>The state itself doesn't change\u2014only our view of it changes. Just like the arrow in 3D space stays the same whether you describe it in Cartesian, cylindrical, or spherical coordinates.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#the-discrete-case-energy-basis-complete-treatment","title":"THE DISCRETE CASE: Energy Basis (Complete Treatment)","text":"<p>Let's master the discrete case completely before moving to continuous variables. We'll build everything from column vectors (which you already know from linear algebra) to the full quantum formalism.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#building-states-start-with-column-vectors","title":"Building States: Start With Column Vectors","text":""},{"location":"quantum-mechanics/quantum-basis-representation/#our-running-example","title":"Our Running Example","text":"<p>Throughout this section, we'll work with one concrete quantum state:</p> <p>A superposition of three energy levels (n=0, n=1, n=2) with coefficients:</p> <ul> <li>\\(c_0 = \\frac{1}{2}\\)</li> <li>\\(c_1 = \\frac{\\sqrt{2}}{2}\\)</li> <li>\\(c_2 = \\frac{1}{2}\\)</li> <li>All others zero</li> </ul> <p>This isn't random\u2014these coefficients are normalized (we'll verify this soon), and this state will help us see all the key ideas concretely.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#column-vectors","title":"Column Vectors","text":"<p>We can represent our three energy levels as basis vectors\u2014these are \"one-hot\" column vectors with a 1 in one position and 0 everywhere else:</p> <p>Energy level 0:</p> \\[\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\end{pmatrix}\\] <p>Energy level 1:</p> \\[\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\end{pmatrix}\\] <p>Energy level 2:</p> \\[\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\end{pmatrix}\\] <p>Each basis vector represents \"definitely in energy level n\". The first entry corresponds to n=0, second to n=1, third to n=2, and so on.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#vector-addition-with-concrete-numbers","title":"Vector Addition With Concrete Numbers","text":"<p>Now here's the key: our quantum state is built by adding scaled basis vectors, just like in 3D where \\(\\vec{v} = 2\\hat{x} + 3\\hat{y} + 1\\hat{z}\\):</p> \\[ \\text{state} = \\frac{1}{2}\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\end{pmatrix} + \\frac{\\sqrt{2}}{2}\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\end{pmatrix} + \\frac{1}{2}\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\end{pmatrix} \\] <p>Let's do the addition explicitly:</p> \\[ \\text{state} = \\begin{pmatrix} \\frac{1}{2} \\cdot 1 + \\frac{\\sqrt{2}}{2} \\cdot 0 + \\frac{1}{2} \\cdot 0 \\\\ \\frac{1}{2} \\cdot 0 + \\frac{\\sqrt{2}}{2} \\cdot 1 + \\frac{1}{2} \\cdot 0 \\\\ \\frac{1}{2} \\cdot 0 + \\frac{\\sqrt{2}}{2} \\cdot 0 + \\frac{1}{2} \\cdot 1 \\\\ 0 \\\\ \\vdots \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{2}}{2} \\\\ \\frac{1}{2} \\\\ 0 \\\\ \\vdots \\end{pmatrix} \\] <p>This is just vector addition! We're scaling each basis vector by its coefficient and adding them together. The result is a column vector whose entries are exactly our coefficients \\(c_0, c_1, c_2, ...\\)</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#introducing-ket-notation-as-shorthand","title":"Introducing Ket Notation as Shorthand","text":"<p>Writing out full column vectors gets tedious, so physicists invented a compact notation called ket notation. The symbol \\(|n\\rangle\\) is shorthand for the column vector we just wrote:</p> <ul> <li>\\(|0\\rangle\\) means \\(\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\end{pmatrix}\\)</li> <li>\\(|1\\rangle\\) means \\(\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\end{pmatrix}\\)</li> <li>\\(|2\\rangle\\) means \\(\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ \\vdots \\end{pmatrix}\\)</li> </ul> <p>The ket \\(|\\psi\\rangle\\) IS the column vector\u2014it's just cleaner notation. Our state becomes:</p> \\[|\\psi\\rangle = \\frac{1}{2}|0\\rangle + \\frac{\\sqrt{2}}{2}|1\\rangle + \\frac{1}{2}|2\\rangle\\] <p>In general, we write this compactly as:</p> \\[|\\psi\\rangle = \\sum_{n=0}^{\\infty} c_n |n\\rangle\\] <p>where the sum \\(\\sum c_n |n\\rangle\\) just means \"add up all the scaled basis vectors\"\u2014exactly what we did above!</p> <p>Key insight: The ket notation \\(|n\\rangle\\) and \\(|\\psi\\rangle\\) is just shorthand for column vectors. Everything you know about vectors from linear algebra applies directly.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#a-note-on-hilbert-space","title":"A Note on \"Hilbert Space\"","text":"<p>You might hear quantum mechanics described as taking place in a \"Hilbert space\" and think it sounds intimidating. It's not! Hilbert space is just the fancy mathematical name for the vector space we're working in.</p> <p>Think of it this way: - In 3D, vectors live in \\(\\mathbb{R}^3\\) (three-dimensional real space) - In quantum mechanics, state vectors live in Hilbert space</p> <p>Hilbert space is just: 1. A vector space (you can add vectors and multiply by scalars) 2. With an inner product (you can compute \\(\\langle\\phi|\\psi\\rangle\\)) 3. That's complete (technical detail: every Cauchy sequence converges - don't worry about this)</p> <p>For our energy basis with states like \\(|\\psi\\rangle = \\sum_n c_n |n\\rangle\\), the Hilbert space is just the space of all possible sequences \\((c_0, c_1, c_2, ...)\\) where \\(\\sum_n |c_n|^2 &lt; \\infty\\). That's it!</p> <p>Bottom line: \"Hilbert space\" = \"vector space with inner product\". Same linear algebra you know, just with a fancier name. Nothing to be scared of!</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#extracting-components-bras-and-inner-products","title":"Extracting Components: Bras and Inner Products","text":"<p>We have our state \\(|\\psi\\rangle = \\frac{1}{2}|0\\rangle + \\frac{\\sqrt{2}}{2}|1\\rangle + \\frac{1}{2}|2\\rangle\\) as a column vector. But suppose someone just hands you the abstract state \\(|\\psi\\rangle\\)\u2014how do you GET the coefficients \\(c_0, c_1, c_2\\) from it?</p> <p>The answer: inner products (quantum dot products).</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#review-the-3d-dot-product","title":"Review: The 3D Dot Product","text":"<p>In 3D, how do we extract a specific component like \\(v_y\\) from a vector \\(\\vec{v}\\)? We use the dot product, which projects the vector onto an axis.</p> <p>Concrete example: Suppose \\(\\vec{v} = 2\\hat{x} + 3\\hat{y} + 1\\hat{z}\\). To extract the y-component:</p> \\[v_y = \\vec{v} \\cdot \\hat{y} = (2\\hat{x} + 3\\hat{y} + 1\\hat{z}) \\cdot \\hat{y}\\] <p>The dot product picks out just the y-component because: - \\(\\hat{x} \\cdot \\hat{y} = 0\\) (perpendicular) - \\(\\hat{y} \\cdot \\hat{y} = 1\\) (parallel) - \\(\\hat{z} \\cdot \\hat{y} = 0\\) (perpendicular)</p> <p>So: \\(v_y = 2(0) + 3(1) + 1(0) = 3\\)</p> <p>The dot product \"projects\" \\(\\vec{v}\\) onto the y-axis and tells you how much of \\(\\vec{v}\\) points in that direction. Quantum mechanics works the same way!</p> <p>We introduce bras \\(\\langle\\psi|\\) (think \"row vectors\") to pair with kets \\(|\\psi\\rangle\\) (think \"column vectors\"):</p> \\[\\langle\\phi|\\psi\\rangle = \\text{\"bra-ket\"} = \\text{\"bracket\"} = \\text{inner product}\\] <p>What's a bra exactly? It's the conjugate transpose of the ket. If:</p> \\[|\\psi\\rangle = \\begin{pmatrix} 1/2 \\\\ \\sqrt{2}/2 \\\\ 1/2 \\end{pmatrix}\\] <p>Then the bra is:</p> \\[\\langle\\psi| = \\begin{pmatrix} 1/2 &amp; \\sqrt{2}/2 &amp; 1/2 \\end{pmatrix}\\] <p>(For real numbers, it's just transpose. For complex numbers, you also take complex conjugates.)</p> <p>Concrete inner product example: Let's compute \\(\\langle 1|\\psi\\rangle\\) to extract the coefficient \\(c_1\\):</p> \\[\\langle 1| = \\begin{pmatrix} 0 &amp; 1 &amp; 0 \\end{pmatrix}, \\quad |\\psi\\rangle = \\begin{pmatrix} 1/2 \\\\ \\sqrt{2}/2 \\\\ 1/2 \\end{pmatrix}\\] \\[\\langle 1|\\psi\\rangle = \\begin{pmatrix} 0 &amp; 1 &amp; 0 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ \\sqrt{2}/2 \\\\ 1/2 \\end{pmatrix} = 0 \\cdot \\frac{1}{2} + 1 \\cdot \\frac{\\sqrt{2}}{2} + 0 \\cdot \\frac{1}{2} = \\frac{\\sqrt{2}}{2}\\] <p>This is just row-times-column matrix multiplication! The inner product picks out the second entry.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#orthonormal-basis-vectors","title":"Orthonormal Basis Vectors","text":"<p>Energy eigenstates are orthonormal:</p> \\[\\langle m|n\\rangle = \\begin{cases} 1 &amp; \\text{if } m = n \\\\ 0 &amp; \\text{if } m \\neq n \\end{cases}\\] <p>Just like \\(\\hat{x} \\cdot \\hat{y} = 0\\) and \\(\\hat{x} \\cdot \\hat{x} = 1\\) in 3D! Let's verify with our basis vectors:</p> <p>Case 1: Different states (m \u2260 n). Compute \\(\\langle 0|1\\rangle\\):</p> \\[\\langle 0|1\\rangle = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; \\cdots \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\end{pmatrix} = 1(0) + 0(1) + 0(0) + \\cdots = 0\\] <p>They're orthogonal (perpendicular)!</p> <p>Case 2: Same state (m = n). Compute \\(\\langle 1|1\\rangle\\):</p> \\[\\langle 1|1\\rangle = \\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; \\cdots \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\end{pmatrix} = 0(0) + 1(1) + 0(0) + \\cdots = 1\\] <p>The state is normalized (length 1)!</p> <p>Now we can use this to extract components. To extract \\(c_1\\) from our state:</p> \\[\\langle 1|\\psi\\rangle = \\langle 1|\\left(\\frac{1}{2}|0\\rangle + \\frac{\\sqrt{2}}{2}|1\\rangle + \\frac{1}{2}|2\\rangle\\right)\\] \\[= \\frac{1}{2}\\langle 1|0\\rangle + \\frac{\\sqrt{2}}{2}\\langle 1|1\\rangle + \\frac{1}{2}\\langle 1|2\\rangle\\] \\[= \\frac{1}{2}(0) + \\frac{\\sqrt{2}}{2}(1) + \\frac{1}{2}(0) = \\frac{\\sqrt{2}}{2}\\] <p>Perfect! The formula is simply:</p> \\[\\boxed{c_n = \\langle n|\\psi\\rangle}\\] <p>The inner product extracts the coefficient c_n, which is a complex number. Don't confuse c_n (the coefficient) with |n\u27e9 (the basis vector)!</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#probabilities-and-normalization","title":"Probabilities and Normalization","text":"<p>Now that we can extract coefficients, let's use them to calculate physical quantities. We'll use our running example throughout.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#from-coefficients-to-probabilities","title":"From Coefficients to Probabilities","text":"<p>The Born rule tells us: the probability of measuring energy \\(E_n\\) is the squared magnitude of coefficient \\(c_n\\):</p> \\[P(E_n) = |c_n|^2\\] <p>For our state with \\(c_0 = \\frac{1}{2}\\), \\(c_1 = \\frac{\\sqrt{2}}{2}\\), \\(c_2 = \\frac{1}{2}\\):</p> \\[P(E_0) = \\left|\\frac{1}{2}\\right|^2 = \\frac{1}{4} = 25\\%\\] \\[P(E_1) = \\left|\\frac{\\sqrt{2}}{2}\\right|^2 = \\frac{1}{2} = 50\\%\\] \\[P(E_2) = \\left|\\frac{1}{2}\\right|^2 = \\frac{1}{4} = 25\\%\\] <p>So if we measure the energy, we have a 25% chance of finding \\(E_0 = \\frac{1}{2}\\hbar\\omega\\), a 50% chance of finding \\(E_1 = \\frac{3}{2}\\hbar\\omega\\), and a 25% chance of finding \\(E_2 = \\frac{5}{2}\\hbar\\omega\\).</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#the-average-energy","title":"The Average Energy","text":"<p>What's the average (expected) energy if we measure many identical systems? It's the weighted average:</p> \\[\\langle E \\rangle = \\sum_n P(E_n) \\cdot E_n\\] <p>For our example:</p> \\[\\langle E \\rangle = \\frac{1}{4}\\left(\\frac{1}{2}\\hbar\\omega\\right) + \\frac{1}{2}\\left(\\frac{3}{2}\\hbar\\omega\\right) + \\frac{1}{4}\\left(\\frac{5}{2}\\hbar\\omega\\right)\\] \\[= \\frac{1}{8}\\hbar\\omega + \\frac{3}{4}\\hbar\\omega + \\frac{5}{8}\\hbar\\omega = \\frac{1 + 6 + 5}{8}\\hbar\\omega = \\frac{12}{8}\\hbar\\omega = \\frac{3}{2}\\hbar\\omega\\] <p>This makes sense\u2014the average energy is between \\(E_1\\) and \\(E_2\\), weighted toward \\(E_1\\) since that's the most probable outcome.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#normalization-probabilities-must-sum-to-100","title":"Normalization: Probabilities Must Sum to 100%","text":"<p>Here's a fundamental requirement: all probabilities must add up to 1 (100%). The particle must be found in some energy state when we measure!</p> <p>In bra-ket notation, this is written as:</p> \\[\\langle\\psi|\\psi\\rangle = 1\\] <p>This is the inner product of the state with itself. In the energy basis, this expands to:</p> \\[\\langle\\psi|\\psi\\rangle = \\sum_n |c_n|^2 = 1\\] <p>Let's verify this for our example:</p> \\[|c_0|^2 + |c_1|^2 + |c_2|^2 = \\frac{1}{4} + \\frac{1}{2} + \\frac{1}{4} = 1 \\quad \\checkmark\\] <p>Perfect! This is what we mean when we say a state is normalized. If someone gives you arbitrary coefficients, you need to rescale them so this condition holds.</p> <p>Physical meaning: Normalization ensures conservation of probability. When you measure, you're guaranteed to get some result\u2014the probabilities of all possible outcomes must sum to certainty.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#time-evolution-and-phase-rotation","title":"Time Evolution and Phase Rotation","text":"<p>Quantum time evolution is beautifully simple in the energy basis. Each coefficient picks up a rotating phase:</p> \\[\\boxed{c_n(t) = c_n(0) \\cdot e^{-iE_n t/\\hbar}}\\] <p>This is the energy rotation phasor we saw in the summary table earlier. Higher energy means faster rotation.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#applying-to-our-running-example","title":"Applying to Our Running Example","text":"<p>For our state, time evolution looks like:</p> \\[|\\psi(t)\\rangle = \\frac{1}{2}|0\\rangle e^{-iE_0 t/\\hbar} + \\frac{\\sqrt{2}}{2}|1\\rangle e^{-iE_1 t/\\hbar} + \\frac{1}{2}|2\\rangle e^{-iE_2 t/\\hbar}\\] <p>Higher energy means faster rotation in the complex plane (\\(E_2\\) rotates fastest, \\(E_0\\) slowest).</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#what-this-means-physically","title":"What This Means Physically","text":"<p>The magnitudes \\(|c_n(t)| = |c_n(0)|\\) never change! Only the phases rotate. This means:</p> <ul> <li>Energy probabilities \\(P(E_n) = |c_n|^2\\) are constant in time</li> <li>The average energy \\(\\langle E \\rangle\\) never changes</li> <li>Only relative phases between different energy levels evolve</li> </ul> <p>This is why energy is conserved in quantum mechanics\u2014the energy basis is special!</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#the-deep-physics-rotation-and-interference","title":"The Deep Physics: Rotation and Interference","text":"<p>Different numbers of energy levels create fundamentally different dynamics:</p> <p>One energy level: \\(|\\psi\\rangle = |n\\rangle\\) - Just picks up overall phase: \\(|n\\rangle \\to |n\\rangle e^{-iE_n t/\\hbar}\\) - Overall phase has no observable effect (it factors out of all measurements) - These are stationary states\u2014nothing changes! - No dynamics at all</p> <p>Two energy levels: Like a qubit - \\(|\\psi\\rangle = c_0|0\\rangle e^{-i\\omega_0 t} + c_1|1\\rangle e^{-i\\omega_1 t}\\) (with \\(\\omega_n = E_n/\\hbar\\)) - Relative phase \\(\\Delta\\phi = (\\omega_1 - \\omega_0)t\\) creates oscillations - This is fundamentally what makes qubits interesting! - Like Bloch sphere dynamics\u2014interference between two levels - Energy probabilities stay constant (\\(|c_0|^2\\) and \\(|c_1|^2\\) never change) - But other observables (position, momentum, etc.) oscillate at beat frequency \\(\\omega_1 - \\omega_0\\)</p> <p>Multiple energy levels: Our example - Many frequencies beating against each other - Multi-dimensional phase space - Complex interference patterns</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#in-position-space-the-state-sloshes","title":"In Position Space: The State \"Sloshes\"","text":"<p>What does this look like in position space? For our state:</p> \\[\\psi(x,t) = \\frac{1}{2}\\psi_0(x)e^{-iE_0 t/\\hbar} + \\frac{\\sqrt{2}}{2}\\psi_1(x)e^{-iE_1 t/\\hbar} + \\frac{1}{2}\\psi_2(x)e^{-iE_2 t/\\hbar}\\] <p>Different energy levels oscillate at different frequencies. When you compute the probability density \\(|\\psi(x,t)|^2\\), these different frequencies interfere, creating oscillating patterns in space.</p> <p>The quantum state \"sloshes\" back and forth in position space, while maintaining constant energy probabilities. Energy eigenstates don't change shape\u2014only their relative phases rotate, creating interference when viewed in other bases.</p> <p>This is the origin of quantum dynamics! Superpositions create time evolution. Pure energy eigenstates are static (\"stationary\"). Only mixing different energies creates observable change.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#the-continuous-case-position-basis","title":"THE CONTINUOUS CASE: Position Basis","text":"<p>Now we've mastered the discrete case. Let's extend these ideas to continuous variables like position. The key insight: position is just like energy, except the labels are continuous real numbers instead of discrete integers.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#from-discrete-to-continuous-the-natural-path","title":"From Discrete to Continuous: The Natural Path","text":""},{"location":"quantum-mechanics/quantum-basis-representation/#start-with-discrete-position","title":"Start With Discrete Position","text":"<p>Imagine a particle that can only be at N specific locations: \\(x_1, x_2, ..., x_N\\) (like beads on a wire with fixed positions).</p> <p>This is just like the energy basis! We have basis vectors:</p> \\[ |x_1\\rangle = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\end{pmatrix} \\quad |x_2\\rangle = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\end{pmatrix} \\quad |x_3\\rangle = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ \\vdots \\end{pmatrix} \\] <p>Each is a one-hot vector: \"particle definitely at position \\(x_i\\)\".</p> <p>A general state: \\(|\\psi\\rangle = \\sum_i c_i |x_i\\rangle\\) where \\(c_i = \\psi(x_i)\\) is the amplitude at position \\(x_i\\).</p> <p>Same idea as energy basis\u2014just different labels!</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#make-the-lattice-finer","title":"Make the Lattice Finer","text":"<p>Now make the spacing \\(\\Delta x\\) smaller and add more positions: - N = 5 positions \u2192 coarse lattice - N = 20 positions \u2192 finer lattice - N = 100 positions \u2192 very fine lattice</p> <p>As N increases: - The column vector gets longer - The coefficients \\(c_i\\) become a function: \\(c_i \\to \\psi(x_i)\\) - Normalization \\(\\sum_i |\\psi(x_i)|^2 = 1\\) starts looking like area under a curve - If we multiply by \\(\\Delta x\\): \\(\\sum_i |\\psi(x_i)|^2 \\Delta x \\approx \\int |\\psi(x)|^2 dx\\)</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#the-continuous-limit","title":"The Continuous Limit","text":"<p>Let \\(\\Delta x \\to 0\\) and \\(N \\to \\infty\\). The discrete list becomes a smooth function:</p> \\[\\psi(x_i) \\longrightarrow \\psi(x)\\] <p>Thinking of states as column vectors:</p> <p>Discrete (N positions):</p> \\[|\\psi\\rangle \\longleftrightarrow \\begin{pmatrix} \\psi(x_1) \\\\ \\psi(x_2) \\\\ \\psi(x_3) \\\\ \\vdots \\\\ \\psi(x_N) \\end{pmatrix}\\] <p>Continuous limit (infinitely many positions):</p> \\[|\\psi\\rangle \\longleftrightarrow (..., \\psi(x-\\Delta x), \\psi(x), \\psi(x+\\Delta x), ...)^T\\] <p>What about basis vectors?</p> <p>Discrete:</p> \\[|x_i\\rangle \\longleftrightarrow (..., 0, 1 \\text{ (at } x_i), 0, ...)^T\\] <p>Continuous limit:</p> \\[|x\\rangle \\longleftrightarrow (..., 0, 1 \\text{ (at } x), 0, ...)^T\\] <p>In the continuous limit, that \"1 at position x\" becomes a delta function spike: \\(|x\\rangle \\leftrightarrow \\delta(x' - x)\\)</p> <p>One-line summary: \\(|x\\rangle\\) is the continuous version of the one-hot basis vectors\u2014it represents \"particle definitely at position x\".</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#position-basis-the-complete-machinery","title":"Position Basis: The Complete Machinery","text":"<p>Now let's see how everything from the discrete case carries over.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#orthonormality","title":"Orthonormality","text":"<p>Discrete: \\(\\langle x_i|x_j\\rangle = \\delta_{ij}\\) (Kronecker delta)</p> <p>Continuous: \\(\\langle x'|x\\rangle = \\delta(x' - x)\\) (Dirac delta)</p> <p>The Kronecker delta becomes the Dirac delta\u2014same idea, continuous version!</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#extracting-the-wavefunction","title":"Extracting the Wavefunction","text":"<p>Discrete: \\(c_i = \\langle x_i|\\psi\\rangle\\) extracts coefficient at position \\(x_i\\)</p> <p>Continuous: \\(\\boxed{\\psi(x) = \\langle x|\\psi\\rangle}\\) extracts amplitude at position \\(x\\)</p> <p>This is THE key equation! The wavefunction \\(\\psi(x)\\) is just the position coefficient function. You input a position, it tells you the amplitude there.</p> <p>(Note: For complex states, \\(\\psi(x) = \\langle x|\\psi\\rangle = \\langle\\psi|x\\rangle^*\\), but when \u03c8 is real-valued they're equal.)</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#building-states","title":"Building States","text":"<p>Discrete: \\(|\\psi\\rangle = \\sum_i c_i |x_i\\rangle\\)</p> <p>Continuous: \\(|\\psi\\rangle = \\int \\psi(x)|x\\rangle dx\\)</p> <p>Same pattern! Sum becomes integral. We're adding up basis vectors weighted by amplitudes.</p> <p>Physical picture: Each \\(|x\\rangle\\) is a delta spike at position x. The integral says \"place amplitude \\(\\psi(x)\\) worth of spike at each position, then add them all up.\" You're building the state from infinitely many delta functions.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#the-delta-function-sifting-property","title":"The Delta Function Sifting Property","text":"<p>Why does \\(\\langle x|\\psi\\rangle = \\psi(x)\\) work? The delta function \"picks out\" values:</p> \\[\\psi(x) = \\langle x|\\psi\\rangle = \\int \\psi(x') \\langle x|x'\\rangle dx' = \\int \\psi(x') \\delta(x - x') dx' = \\psi(x)\\] <p>The delta function \\(\\delta(x - x')\\) is zero everywhere except at \\(x' = x\\), where it \"sifts out\" exactly \\(\\psi(x)\\).</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#normalization_1","title":"Normalization","text":"<p>Discrete: \\(\\sum_i |c_i|^2 = 1\\)</p> <p>Continuous: \\(\\int_{-\\infty}^{\\infty} |\\psi(x)|^2 dx = 1\\)</p> <p>All probabilities must sum to 100%. In the continuous case, \\(|\\psi(x)|^2 dx\\) is the probability of finding the particle in interval \\([x, x+dx]\\).</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#the-complete-parallel-structure","title":"The Complete Parallel Structure","text":"Concept Discrete (Energy) Continuous (Position) Basis labels \\(n = 0,1,2,...\\) \\(x \\in \\mathbb{R}\\) Basis vectors One-hot columns \\(\\vert n\\rangle\\) Delta spikes \\(\\vert x\\rangle\\) Coefficients \\(c_n = \\langle n\\vert\\psi\\rangle\\) \\(\\psi(x) = \\langle x\\vert\\psi\\rangle\\) Orthonormality \\(\\langle m\\vert n\\rangle = \\delta_{mn}\\) \\(\\langle x'\\vert x\\rangle = \\delta(x'-x)\\) Build state \\(\\vert\\psi\\rangle = \\sum_n c_n\\vert n\\rangle\\) \\(\\vert\\psi\\rangle = \\int \\psi(x)\\vert x\\rangle dx\\) Normalization \\(\\sum_n \\vert c_n\\vert^2 = 1\\) \\(\\int \\vert\\psi(x)\\vert^2 dx = 1\\) Probability \\(P(E_n) = \\vert c_n\\vert^2\\) \\(P(x)dx = \\vert\\psi(x)\\vert^2 dx\\) <p>Discrete sums \\(\\sum\\) become continuous integrals \\(\\int\\). Kronecker deltas \\(\\delta_{mn}\\) become Dirac deltas \\(\\delta(x-x')\\). Everything else is identical!</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#connecting-to-our-running-example","title":"Connecting to Our Running Example","text":"<p>Remember our state: \\(|\\psi\\rangle = \\frac{1}{2}|0\\rangle + \\frac{\\sqrt{2}}{2}|1\\rangle + \\frac{1}{2}|2\\rangle\\)</p> <p>In position space, this becomes:</p> \\[\\psi(x) = \\frac{1}{2}\\psi_0(x) + \\frac{\\sqrt{2}}{2}\\psi_1(x) + \\frac{1}{2}\\psi_2(x)\\] <p>where \\(\\psi_0(x), \\psi_1(x), \\psi_2(x)\\) are the Gaussian energy eigenfunctions from Part 1.</p> <p>Key insight: We only use three energy basis vectors (the rest have \\(c_n = 0\\)). But in position basis, we need \\(\\psi(x)\\) defined for every x! Why?</p> <p>Because each Gaussian energy eigenstate spreads over all space. When we add three Gaussians together, the result is non-zero almost everywhere. So in position basis, we need infinitely many values to describe what's happening.</p> <p>It's like describing a diagonal line in 2D: - Rotated basis aligned with the line: Just one coordinate - Standard xy basis: Need both x and y everywhere</p> <p>Different bases make different aspects simple or complicated. Our state is \"simple\" in energy basis (just 3 numbers) but \"complicated\" in position basis (function over all x).</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#naked-vs-ket-now-its-clear","title":"Naked \u03c8 vs Ket |\u03c8\u27e9: Now It's Clear","text":"<p>The distinction is simple:</p> <p>|\u03c8\u27e9 \u2014 The complete quantum state (basis-independent)</p> <ul> <li>Contains all measurement possibilities</li> <li>Like a vector in 3D space</li> </ul> <p>\u03c8(x) \u2014 The position representation</p> <ul> <li>Gives amplitude at each position: \\(\\psi(x) = \\langle x|\\psi\\rangle\\)</li> <li>Like the x-components of a 3D vector</li> </ul> <p>\\(c_0, c_1, c_2, ...\\) \u2014 The energy representation</p> <ul> <li>Coefficients for each energy level: \\(c_n = \\langle n|\\psi\\rangle\\)</li> <li>Like components in a different coordinate system</li> </ul> <p>The pattern is always:</p> <ul> <li>Energy coefficients: \\(c_n = \\langle n|\\psi\\rangle\\)</li> <li>Position wavefunction: \\(\\psi(x) = \\langle x|\\psi\\rangle\\)</li> <li>Momentum wavefunction: \\(\\tilde{\\psi}(p) = \\langle p|\\psi\\rangle\\)</li> </ul> <p>The ket |\u03c8\u27e9 is the state itself. The functions \u03c8(x), \\(\\tilde{\\psi}(p)\\) and coefficients \\(c_n\\) are different ways of describing it.</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#the-punchline","title":"The Punchline","text":"<p>Quantum mechanics seems complicated because we're juggling multiple representations of the same thing. But once you see the pattern, it all becomes linear algebra:</p>"},{"location":"quantum-mechanics/quantum-basis-representation/#the-core-ideas","title":"The Core Ideas","text":"<p>1. The abstract state |\u03c8\u27e9 exists independent of how we describe it - Like a vector in 3D space that doesn't change whether you use Cartesian, cylindrical, or spherical coordinates - The state itself is basis-independent\u2014only our description changes</p> <p>2. Representations are projections - Energy coefficients: \\(c_n = \\langle n|\\psi\\rangle\\) - Position wavefunction: \\(\\psi(x) = \\langle x|\\psi\\rangle\\) - Momentum wavefunction: \\(\\tilde{\\psi}(p) = \\langle p|\\psi\\rangle\\) - Same formula, different basis!</p> <p>3. Discrete and continuous follow identical patterns - Discrete: sums \\(\\sum\\), Kronecker delta \\(\\delta_{mn}\\), column vectors - Continuous: integrals \\(\\int\\), Dirac delta \\(\\delta(x-x')\\), functions - Everything else is the same\u2014just different limiting cases</p> <p>4. Choose your basis to match what you want to measure - Want energy probabilities? Use energy basis: \\(P(E_n) = |c_n|^2\\) - Want position probabilities? Use position basis: \\(P(x)dx = |\\psi(x)|^2 dx\\) - Each basis makes certain questions trivial and others complicated</p> <p>5. Time evolution is rotating phases in the energy basis - \\(c_n(t) = c_n(0)e^{-iE_n t/\\hbar}\\) (from Schr\u00f6dinger's equation) - Magnitudes stay constant \u2192 energy probabilities conserved - Only relative phases change \u2192 interference creates dynamics - Pure energy eigenstates are stationary; only superpositions evolve</p>"},{"location":"quantum-mechanics/quantum-foundations/","title":"Quantum Mechanics: Foundations","text":""},{"location":"quantum-mechanics/quantum-foundations/#hand-wavey-quick-derivation-to-schrodingers-equation","title":"Hand-Wavey Quick Derivation to Schr\u00f6dinger's Equation","text":"<p>Let's start with a free particle (no potential, just moving in space) and work backwards to get Schr\u00f6dinger's equation.</p>"},{"location":"quantum-mechanics/quantum-foundations/#de-broglie-relations-1924","title":"de Broglie Relations (1924)","text":"<p>By the 1920s, light was known to have both wave and particle properties: - Planck &amp; Einstein showed photons have energy \\(E = h\\nu\\) - Compton scattering (1923) confirmed photons have momentum \\(p = h/\\lambda\\)</p> <p>de Broglie's leap: If light waves have particle properties, maybe particles have wave properties with the same relations?</p> <p>For matter waves, we use:</p> \\[ E = \\hbar\\omega, \\quad p = \\hbar k \\] <p>where \\(\\hbar = h/2\\pi\\), \\(k\\) is wave number, and \\(\\omega\\) is angular frequency. Electron diffraction experiments soon confirmed it.</p>"},{"location":"quantum-mechanics/quantum-foundations/#free-particle-wave-function","title":"Free Particle Wave Function","text":"<p>A free particle moving in space can be described as a plane wave. We have two options:</p> <ol> <li>Real wave: \\(A\\cos(kx - \\omega t)\\)</li> <li>Complex wave (phasor): \\(A e^{i(kx - \\omega t)}\\)</li> </ol> <p>Why use the complex form?</p> <p>This is a deep question! Here's the intuition:</p> <ul> <li>We know waves interfere (double slit experiment) - they add in and out of phase</li> <li>The cosine form can be written as a phasor using Euler's equation: \\(e^{i\\theta} = \\cos\\theta + i\\sin\\theta\\)</li> <li>We need to calculate probability (which should always be positive, not oscillating between + and -)</li> <li>Complex form makes this easy: probability = \\(|\\psi|^2 = \\psi^* \\psi\\) (multiply by complex conjugate)</li> </ul> <p>But why does nature REQUIRE complex numbers? A real wavefunction can't capture all of quantum mechanics. Here's why:</p> <ol> <li> <p>Interference requires amplitude AND phase: When waves interfere, both the magnitude and phase matter. A single real number can't encode both - you need two real numbers (magnitude &amp; phase), which is exactly what a complex number gives you: \\(z = re^{i\\theta}\\)</p> </li> <li> <p>Time evolution rotates the phase: The factor \\(e^{-iEt/\\hbar}\\) rotates the wavefunction in the complex plane. This rotating phase is essential - it's what creates interference patterns and determines when waves add constructively vs destructively. Real numbers can't \"rotate\" like this!</p> </li> <li> <p>The Schr\u00f6dinger equation requires it: Notice the factor of \\(i\\) in \\(i\\hbar\\frac{\\partial\\psi}{\\partial t}\\). This means time derivatives change the real part into the imaginary part and vice versa - the wavefunction must be complex for this to work!</p> </li> </ol> <p>Bottom line: Complex numbers aren't just mathematical convenience - they're fundamental to how quantum mechanics works. The phase of \u03c8 contains physical information about interference and time evolution that can't be captured by real numbers alone.</p> <p>So we use:</p> \\[ \\psi(x,t) = A e^{i(kx - \\omega t)} \\]"},{"location":"quantum-mechanics/quantum-foundations/#deriving-schrodingers-equation","title":"Deriving Schr\u00f6dinger's Equation","text":"<p>Now let's take derivatives and use the de Broglie relations to connect them.</p> <p>Time derivative:</p> \\[ \\frac{\\partial \\psi}{\\partial t} = -i\\omega \\psi \\] <p>Using \\(E = \\hbar\\omega\\):</p> \\[ \\frac{\\partial \\psi}{\\partial t} = -i\\frac{E}{\\hbar}\\psi \\] <p>Multiply both sides by \\(i\\hbar\\):</p> \\[ i\\hbar \\frac{\\partial \\psi}{\\partial t} = E\\psi \\] <p>Spatial derivative (twice):</p> \\[ \\frac{\\partial^2 \\psi}{\\partial x^2} = (ik)^2 \\psi = -k^2 \\psi \\] <p>Using \\(p = \\hbar k\\):</p> \\[ \\frac{\\partial^2 \\psi}{\\partial x^2} = -\\frac{p^2}{\\hbar^2}\\psi \\] <p>Multiply both sides by \\(-\\hbar^2/2m\\):</p> \\[ -\\frac{\\hbar^2}{2m}\\frac{\\partial^2 \\psi}{\\partial x^2} = \\frac{p^2}{2m}\\psi \\] <p>Setting them equal:</p> <p>For a free particle, classical mechanics says \\(E = p^2/2m\\). So:</p> \\[ E\\psi = \\frac{p^2}{2m}\\psi \\] <p>Therefore our two derivative expressions must be equal:</p> \\[ i\\hbar \\frac{\\partial \\psi}{\\partial t} = -\\frac{\\hbar^2}{2m}\\frac{\\partial^2 \\psi}{\\partial x^2} \\] <p>That's Schr\u00f6dinger's equation for a free particle!</p> <p>What does this mean? The time derivative extracts energy from the wave, the spatial derivative extracts kinetic energy (from momentum). For a free particle these are the same thing, so we get one equation. It's the quantum version of \\(E = p^2/2m\\).</p>"},{"location":"quantum-mechanics/quantum-foundations/#adding-a-potential","title":"Adding a Potential","text":"<p>If the particle is in a potential \\(V(x)\\), total energy = KE + PE:</p> \\[ i\\hbar \\frac{\\partial \\psi}{\\partial t} = -\\frac{\\hbar^2}{2m}\\frac{\\partial^2 \\psi}{\\partial x^2} + V(x)\\psi \\] <p>Or more compactly:</p> \\[ i\\hbar \\frac{\\partial \\psi}{\\partial t} = \\hat{H}\\psi \\] <p>where \\(\\hat{H} = -\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2} + V(x)\\) is the Hamiltonian operator.</p>"},{"location":"quantum-mechanics/quantum-foundations/#building-intuition-from-wave-packets-to-stationary-states","title":"Building Intuition: From Wave Packets to Stationary States","text":""},{"location":"quantum-mechanics/quantum-foundations/#position-and-momentum-space-fourier-transform","title":"Position and Momentum Space (Fourier Transform)","text":"<p>A single plane wave \\(e^{ikx}\\) extends forever - not realistic for a localized particle.</p> <p>Real particles are wave packets. Consider a state at fixed energy \\(E\\). We've factored out the time part \\(e^{-iEt/\\hbar}\\), so we're looking at just the spatial part \\(\\psi(x)\\). Start simple - add a few plane waves with different momenta:</p> \\[ \\psi(x) = A_1 e^{ik_1 x} + A_2 e^{ik_2 x} + A_3 e^{ik_3 x} + \\cdots \\] <p>Each \\(A_n\\) tells you the amplitude and phase of momentum \\(k_n\\). As a sum:</p> \\[ \\psi(x) = \\sum_n A_n e^{ik_n x} \\] <p>Now take the limit: spacing between \\(k\\) values \u2192 0, sum \u2192 integral. The discrete amplitudes \\(A_n\\) become a continuous function \\(\\tilde{\\psi}(k)\\):</p> \\[ \\psi(x) = \\int_{-\\infty}^{\\infty} \\tilde{\\psi}(k) e^{ikx} dk \\] <p>That's the Fourier transform. \\(\\tilde{\\psi}(k)\\) tells you \"how much of momentum \\(k\\)\" is in your wave packet.</p> <p>Key insight: You can't have both narrow at once. - Narrow in position (localized) \u2192 wide spread in momentum - Narrow in momentum (definite p) \u2192 spread out in space - This is the uncertainty principle!</p> <p>Quick derivation: From Fourier theory (like time-bandwidth product in signal processing), a wave packet localized to width \\(\\Delta x\\) needs a spread in \\(k\\) of roughly \\(\\Delta k \\sim 1/\\Delta x\\). Since \\(p = \\hbar k\\), we have \\(\\Delta p = \\hbar \\Delta k\\), giving:</p> \\[ \\Delta x \\cdot \\Delta p \\sim \\hbar \\] <p>The rigorous proof (using the commutator \\([\\hat{x},\\hat{p}] = i\\hbar\\)) gives \\(\\Delta x \\cdot \\Delta p \\geq \\hbar/2\\).</p>"},{"location":"quantum-mechanics/quantum-foundations/#time-independent-schrodinger-equation","title":"Time-Independent Schr\u00f6dinger Equation","text":"<p>Now let's think about the time derivative in Schr\u00f6dinger's equation.</p> <p>From \\(i\\hbar \\frac{\\partial \\psi}{\\partial t} = E\\psi\\), we see: higher energy \u2192 faster time oscillation.</p> <p>For states with definite energy (energy eigenstates), we can separate variables:</p> \\[ \\psi(x,t) = \\psi(x) e^{-iEt/\\hbar} \\] <p>The time part is just a rotating phase \\(e^{-i\\omega t}\\) where \\(\\omega = E/\\hbar\\). All the physics (probability, where the particle is) lives in \\(\\psi(x)\\).</p> <p>Plug this back into Schr\u00f6dinger's equation:</p> \\[ i\\hbar \\frac{\\partial}{\\partial t}\\left[\\psi(x) e^{-iEt/\\hbar}\\right] = \\hat{H}\\left[\\psi(x) e^{-iEt/\\hbar}\\right] \\] <p>The time derivative just brings down \\(-iE/\\hbar\\), which cancels the \\(i\\hbar\\), leaving:</p> \\[ E\\psi(x) = \\hat{H}\\psi(x) \\] <p>Or written out fully:</p> \\[ -\\frac{\\hbar^2}{2m}\\frac{d^2\\psi}{dx^2} + V(x)\\psi = E\\psi \\] <p>That's the time-independent Schr\u00f6dinger equation. It's an eigenvalue equation: find the functions \\(\\psi(x)\\) and energies \\(E\\) that satisfy it.</p>"},{"location":"quantum-mechanics/quantum-foundations/#intuition-wiggliness-vs-potential","title":"Intuition: Wiggliness vs Potential","text":"<p>Here's the key physical insight from the time-independent equation.</p> <p>Start with the Schr\u00f6dinger equation and divide both sides by \u03c8:</p> \\[ -\\frac{\\hbar^2}{2m}\\frac{\\psi''}{\\psi} = E - V(x) \\] <p>Now the physics is crystal clear:</p> <p>Left side: The second derivative \u03c8'' measures how much the wavefunction curves. Dividing by \u03c8 itself gives the local kinetic energy.</p> <p>Right side: Since total energy E = KE + PE, we have KE = E - V(x).</p> <p>Why divide by \u03c8? When you localize (squeeze) a wavefunction, two things happen: it gets taller (higher amplitude at the peak) and sharper (higher curvature). Dividing by \u03c8 separates these effects \u2014 \u03c8''/\u03c8 captures just the sharpness of the curve, independent of the overall height. This is what determines the kinetic energy.</p> <p>What this means:</p> <ul> <li>Low potential (V small) \u2192 right side is large \u2192 \u03c8''/\u03c8 is large \u2192 \u03c8 is very wiggly</li> <li>High potential (V large) \u2192 right side is small \u2192 \u03c8''/\u03c8 is small \u2192 \u03c8 is less wiggly</li> </ul> <p>The equation literally says: the curvature of \u03c8 (relative to its value) equals the local kinetic energy!</p> <p>When you solve for \u03c8(x), it automatically adjusts its wiggliness to match the kinetic energy at each point. In regions where potential is low, the wavefunction oscillates rapidly. Where potential is high, it oscillates slowly (or even decays exponentially if V &gt; E).</p>"},{"location":"quantum-mechanics/quantum-foundations/#why-cant-a-particle-just-sit-at-the-bottom-of-a-well","title":"Why Can't a Particle Just Sit at the Bottom of a Well?","text":""},{"location":"quantum-mechanics/quantum-foundations/#classical-vs-quantum-ground-state","title":"Classical vs Quantum Ground State","text":"<p>Classical intuition: If you put a ball in a bowl (harmonic oscillator potential), the lowest energy state is the ball sitting still at the bottom. Total energy = 0.</p> <p>Quantum reality: This doesn't work! Here's why.</p> <p>If we try to localize the particle tightly at the bottom of the well (making \u03c8 very narrow), the wavefunction has high curvature. From the Schr\u00f6dinger equation divided by \u03c8:</p> \\[ -\\frac{\\hbar^2}{2m}\\frac{\\psi''}{\\psi} = E - V(x) \\] <p>Squeezing \u03c8 narrow increases its curvature (\\(\\psi''\\)), making the left side large. This means the kinetic energy \\(E - V(x)\\) shoots up at the center, so \\(E(x) = \\text{KE}(x) + V(x)\\) is not constant \u2014 meaning it's not a stationary state.</p>"},{"location":"quantum-mechanics/quantum-foundations/#finding-the-just-right-width","title":"Finding the \"Just Right\" Width","text":"<p>Let's test three different Gaussian trial wavefunctions with different widths:</p> <p></p> <ul> <li>\u03c8\u2081 (narrow, red): Sharp peak \u2192 high curvature at center \u2192 high KE</li> <li>\u03c8\u2082 (medium, green): \"Just right\" width \u2014 balances spread vs curvature</li> <li>\u03c8\u2083 (wide, blue): Flat peak \u2192 low curvature \u2192 low KE</li> </ul> <p>Now let's check which one gives constant total energy \\(E(x) = \\text{KE}(x) + V(x)\\):</p> <p></p> <ul> <li>E\u2081 (red): Too much KE \u2192 \\(E(x)\\) bulges at center (not constant!)</li> <li>E\u2082 (green): Nearly flat \\(E(x)\\) \u2192 this is a stationary state!</li> <li>E\u2083 (blue): Not enough KE to balance the rising potential \\(V(x)\\) \u2192 \\(E(x)\\) varies</li> </ul>"},{"location":"quantum-mechanics/quantum-foundations/#the-zero-point-energy","title":"The Zero-Point Energy","text":"<p>Only \u03c8\u2082 (the \"just right\" width) gives approximately constant \\(E(x)\\) \u2014 this is the ground state of the harmonic oscillator. Notice:</p> <ol> <li>The wavefunction spreads out (can't be localized to a point)</li> <li>The particle has kinetic energy even in the ground state</li> <li>For this specific potential (\\(V = \\frac{1}{2}m\\omega^2 x^2\\)), the minimum energy is \\(E_0 = \\frac{1}{2}\\hbar\\omega\\), not zero</li> </ol> <p>This minimum energy is the zero-point energy \u2014 a purely quantum effect arising from the uncertainty principle. If you try to confine the particle too tightly (small \u0394x), its momentum uncertainty \u0394p increases, giving it kinetic energy. The ground state is the perfect balance between being localized enough to stay near the potential minimum, but spread out enough to avoid excessive kinetic energy.</p> <p>Key insight: The time-independent Schr\u00f6dinger equation is essentially saying \"find the wavefunction shape where curvature (KE) and potential energy add up to the same constant everywhere.\" Only specific shapes (eigenstates) and energies (eigenvalues) work!</p>"},{"location":"quantum-mechanics/quantum-foundations/#the-harmonic-oscillator-explicit-solutions","title":"The Harmonic Oscillator: Explicit Solutions","text":"<p>Now that we understand why the ground state can't be at the bottom, let's see the actual solutions for the harmonic oscillator potential \\(V(x) = \\frac{1}{2}m\\omega^2 x^2\\).</p>"},{"location":"quantum-mechanics/quantum-foundations/#energy-levels","title":"Energy Levels","text":"<p>The allowed energies are:</p> \\[ E_n = \\hbar\\omega\\left(n + \\frac{1}{2}\\right), \\quad n = 0, 1, 2, ... \\] <p>Notice: - Evenly spaced by \u210f\u03c9 (like a ladder!) - Ground state (n=0): \\(E_0 = \\frac{1}{2}\\hbar\\omega\\) (the zero-point energy we saw) - First excited (n=1): \\(E_1 = \\frac{3}{2}\\hbar\\omega\\) - Second excited (n=2): \\(E_2 = \\frac{5}{2}\\hbar\\omega\\)</p>"},{"location":"quantum-mechanics/quantum-foundations/#the-wavefunctions-in-position-and-momentum-space","title":"The Wavefunctions in Position and Momentum Space","text":"<p>For convenience, define the natural length and momentum scales: \\(x_0 = \\sqrt{\\hbar/(m\\omega)}\\) and \\(p_0 = \\sqrt{m\\hbar\\omega}\\)</p> <p>n=0 (Ground state): E\u2080 = \u00bd\u210f\u03c9</p> <p>Position space:</p> \\[ \\psi_0(x) = \\left(\\frac{1}{\\pi x_0^2}\\right)^{1/4} e^{-x^2/(2x_0^2)} \\] <p>Momentum space:</p> \\[ \\tilde{\\psi}_0(p) = \\left(\\frac{1}{\\pi p_0^2}\\right)^{1/4} e^{-p^2/(2p_0^2)} \\] <p>Both Gaussians! Fourier transform of a Gaussian is a Gaussian. This is the \"just right\" width we found.</p> <p>Notice something special: the ground state has width ~ x\u2080 in position space and width ~ p\u2080 in momentum space. If you calculate \u0394x\u00b7\u0394p, you get exactly \u210f/2 \u2014 the minimum allowed by the uncertainty principle! This is why it's the lowest energy state: it's spread out just enough to minimize both position and momentum uncertainty.</p> <p>n=1 (First excited): E\u2081 = 3/2\u210f\u03c9</p> <p>Position space:</p> \\[ \\psi_1(x) = \\left(\\frac{1}{4\\pi x_0^2}\\right)^{1/4} \\frac{2x}{x_0} e^{-x^2/(2x_0^2)} \\] <p>Momentum space:</p> \\[ \\tilde{\\psi}_1(p) = \\left(\\frac{1}{4\\pi p_0^2}\\right)^{1/4} \\frac{2p}{p_0} e^{-p^2/(2p_0^2)} \\] <p>One node at origin (antisymmetric). Same functional form in both spaces!</p> <p>n=2 (Second excited): E\u2082 = 5/2\u210f\u03c9</p> <p>Position space:</p> \\[ \\psi_2(x) = \\left(\\frac{1}{16\\pi x_0^2}\\right)^{1/4} \\left(\\frac{4x^2}{x_0^2} - 2\\right) e^{-x^2/(2x_0^2)} \\] <p>Momentum space:</p> \\[ \\tilde{\\psi}_2(p) = \\left(\\frac{1}{16\\pi p_0^2}\\right)^{1/4} \\left(\\frac{4p^2}{p_0^2} - 2\\right) e^{-p^2/(2p_0^2)} \\] <p>Two nodes (symmetric). Again, same functional form!</p> <p>Pattern: Higher n \u2192 more nodes \u2192 more wiggly \u2192 higher energy. The harmonic oscillator has beautiful symmetry between position and momentum.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/","title":"Why Massive Particles Stay More Localized","text":""},{"location":"quantum-mechanics/quantum-mass-localization/#a-complete-tutorial-from-first-principles","title":"A Complete Tutorial from First Principles","text":"<p>This tutorial explains why electrons spread out and \"blur\" much more than protons, even though both are quantum particles. We attack this question from every angle: the Schr\u00f6dinger equation, potential wells, free particle spreading, the Heisenberg uncertainty principle, and Fourier analysis. Each approach gives the same answer through different reasoning\u2014that's how you know you truly understand something.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#prerequisites","title":"Prerequisites","text":"<p>This tutorial assumes only: 1. Basic calculus (derivatives, integrals) 2. Familiarity with classical mechanics concepts like kinetic energy KE = \u00bdmv\u00b2 and potential energy 3. The willingness to follow careful step-by-step arguments</p> <p>Every concept and symbol will be defined before use, and critically, we'll specify exactly when each equation applies (free space vs potentials, plane waves vs general states, etc.).</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#part-1-the-core-insight","title":"Part 1: The Core Insight","text":""},{"location":"quantum-mechanics/quantum-mass-localization/#mass-as-quantum-mechanics-resistance-to-delocalization","title":"Mass as Quantum Mechanics' \"Resistance to Delocalization\"","text":"<p>Before diving into equations, let's state the central insight that everything else will support:</p> <p>In classical mechanics, mass resists changes in velocity (this is inertia).</p> <p>In quantum mechanics, mass resists spatial spreading (delocalization).</p> <p>A heavy particle \"wants\" to stay put; a light particle \"wants\" to spread out. This isn't a metaphor\u2014it's a precise mathematical consequence of the Schr\u00f6dinger equation.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#why-does-this-happen","title":"Why Does This Happen?","text":"<p>Quantum mechanics assigns wave-like properties to all particles. The key relationship is the de Broglie wavelength:</p> \\[\\lambda = \\frac{h}{p} = \\frac{h}{mv}\\] <p>IMPORTANT - Domain of Validity: - This formula is exact for plane waves (waves with definite momentum p) - For wave packets and general states: The particle doesn't have a single wavelength, but rather a spectrum of wavelengths corresponding to the momentum components present. The formula still applies to each momentum component individually - Works in any potential (free space or otherwise), but remember: in a general potential, the momentum isn't constant in space\u2014it varies depending on the potential energy at each location - The formula p = mv is the classical momentum, which corresponds to the expectation value \u27e8p\u27e9 for quantum states</p> <p>For particles with the same velocity:</p> Particle Mass Wavelength (for same v) Electron \\(m_e\\) \\(\\lambda_e = h/(m_e v)\\) Proton \\(1836 m_e\\) \\(\\lambda_p = h/(1836 m_e v) = \\lambda_e/1836\\) <p>The proton's wavelength is 1836 times shorter than the electron's! Shorter wavelengths mean the wave \"fits\" into smaller spaces. Longer wavelengths mean the wave is inherently \"fuzzier\" and more spread out in space.</p> <p>This is the intuitive answer. Now let's see it emerge rigorously from the Schr\u00f6dinger equation.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#part-2-foundationthe-schrodinger-equation","title":"Part 2: Foundation\u2014The Schr\u00f6dinger Equation","text":""},{"location":"quantum-mechanics/quantum-mass-localization/#what-is-a-wavefunction","title":"What Is a Wavefunction?","text":"<p>Definition: The wavefunction \\(\\psi(x,t)\\) is a mathematical function that encodes everything knowable about a quantum particle's state. It is a complex-valued function (meaning it can have real and imaginary parts) that varies over space (x) and time (t).</p> <p>The physical meaning: \\(|\\psi(x)|^2\\) (the squared magnitude of the wavefunction) gives the probability density\u2014the probability per unit length of finding the particle near position x.</p> <ul> <li>A particle is localized when \\(|\\psi(x)|^2\\) is tall and narrow (high probability in a small region)</li> <li>A particle is delocalized when \\(|\\psi(x)|^2\\) is short and wide (probability spread over a large region)</li> </ul>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-time-independent-schrodinger-equation","title":"The Time-Independent Schr\u00f6dinger Equation","text":"<p>Domain of Validity: This equation applies to stationary states\u2014states with definite total energy E. These are the eigenstates of the Hamiltonian. The full time-dependent wavefunction is \\(\\Psi(x,t) = \\psi(x)e^{-iEt/\\hbar}\\), where \\(\\psi(x)\\) satisfies:</p> \\[-\\frac{\\hbar^2}{2m} \\frac{d^2\\psi}{dx^2} + V(x)\\psi = E\\psi\\] <p>Definition of each symbol:</p> Symbol Name Meaning Units \\(\\hbar\\) Reduced Planck constant Nature's quantum scale (\\(1.055 \\times 10^{-34}\\) J\u00b7s) J\u00b7s \\(m\\) Particle mass How much \"stuff\" the particle has kg \\(\\psi\\) Wavefunction Probability amplitude at position x \\(m^{-1/2}\\) \\(d^2\\psi/dx^2\\) Second derivative How much \\(\\psi\\) curves (bends) in space \\(m^{-5/2}\\) \\(V(x)\\) Potential energy Energy from external forces at position x J \\(E\\) Total energy Kinetic + potential energy (constant for stationary states) J"},{"location":"quantum-mechanics/quantum-mass-localization/#understanding-the-energy-terms","title":"Understanding the Energy Terms","text":"<p>The total energy E is the sum of kinetic energy (energy of motion) and potential energy (energy from position in a force field):</p> \\[E = KE + PE = KE + V(x)\\] <p>Validity note: For stationary states, E is constant in time. At each position x, we can write:</p> \\[KE(x) = E - V(x)\\] <p>This is crucial: The kinetic energy at each position depends on how deep the particle is in the potential. In a potential well (where V is negative), \\(E - V(x)\\) is larger than E alone, so the kinetic energy is high.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#part-3-the-curvature-argument-most-fundamental","title":"Part 3: The Curvature Argument (Most Fundamental)","text":"<p>This section contains the deepest insight. Read it carefully\u2014everything else follows from here.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#step-1-rearranging-for-curvature","title":"Step 1: Rearranging for Curvature","text":"<p>Start with the Schr\u00f6dinger equation and solve for the second derivative (the curvature term):</p> <p>Starting equation (valid for stationary states in any potential): $\\(-\\frac{\\hbar^2}{2m} \\frac{d^2\\psi}{dx^2} + V(x)\\psi = E\\psi\\)$</p> <p>Move \\(V(x)\\psi\\) to the right side: $\\(-\\frac{\\hbar^2}{2m} \\frac{d^2\\psi}{dx^2} = E\\psi - V(x)\\psi = [E - V(x)]\\psi\\)$</p> <p>Multiply both sides by \\(-2m/\\hbar^2\\): $\\(\\frac{d^2\\psi}{dx^2} = -\\frac{2m}{\\hbar^2} [E - V(x)] \\psi\\)$</p> <p>Recognize that \\([E - V(x)] = KE\\) (kinetic energy at position x): $\\(\\frac{d^2\\psi}{dx^2} = -\\frac{2m}{\\hbar^2} \\cdot KE \\cdot \\psi\\)$</p> <p>Validity: This relationship holds at every point in space for any stationary state in any potential \\(V(x)\\).</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#step-2-defining-normalized-curvature","title":"Step 2: Defining Normalized Curvature","text":"<p>The second derivative \\(d^2\\psi/dx^2\\) tells us how much the wavefunction bends. But to compare curvature meaningfully, we should look at the curvature relative to the wavefunction's value at that point.</p> <p>Definition: The normalized curvature (or relative curvature) is the second derivative divided by the wavefunction itself:</p> \\[\\frac{d^2\\psi/dx^2}{\\psi}\\] <p>Dividing both sides of our equation by \\(\\psi\\):</p> \\[\\boxed{\\frac{d^2\\psi/dx^2}{\\psi} = -\\frac{2m}{\\hbar^2} \\cdot KE}\\] <p>This is the master equation for understanding localization. Applies to any stationary state, any potential.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#step-3-what-does-curvature-mean-geometrically","title":"Step 3: What Does Curvature Mean Geometrically?","text":"<p>The second derivative tells you how much a function \"bends\":</p> Curvature Value What the Function Does Visual Large negative Bends sharply downward (concave down) Like \u2229 with tight curve Small negative Bends gently downward Like \u2229 with wide curve Zero Straight line (no bending) Like \u2014 Small positive Bends gently upward Like \u222a with wide curve Large positive Bends sharply upward (concave up) Like \u222a with tight curve <p>Key geometric fact: A function that curves sharply bends back toward zero quickly. A function that curves gently extends far before turning around.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#step-4-the-mass-dependence","title":"Step 4: The Mass Dependence","text":"<p>Now examine our master equation:</p> \\[\\frac{d^2\\psi/dx^2}{\\psi} = -\\frac{2m}{\\hbar^2} \\cdot KE\\] <p>The normalized curvature is proportional to the mass m. This has two important implications:</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#implication-1-same-kinetic-energy-different-masses","title":"Implication 1: Same kinetic energy, different masses","text":"<p>If an electron and a proton have the same kinetic energy (at some point in space):</p> Particle Mass Normalized Curvature Electron \\(m_e\\) Curvature = C Proton \\(1836 m_e\\) Curvature = \\(1836 \\times C\\) <p>For the same kinetic energy, the proton's wavefunction curves 1836 times more sharply! It bends back toward zero much faster, so it stays in a smaller region.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#implication-2-achieving-a-certain-kinetic-energy","title":"Implication 2: Achieving a certain kinetic energy","text":"<p>Rearranging the equation to solve for KE:</p> \\[KE = -\\frac{\\hbar^2}{2m} \\cdot \\frac{d^2\\psi/dx^2}{\\psi}\\] <p>This says: kinetic energy equals \\((\\hbar^2/2m)\\) times the curvature magnitude. So:</p> Particle To achieve a given KE... Consequence Heavy (large m) Needs large curvature Wavefunction must bend sharply \u2192 stays localized Light (small m) Even small curvature gives large KE Wavefunction can bend gently \u2192 extends far <p>Think of it this way: For a light particle, even a gently curving wavefunction represents a lot of kinetic energy. For a heavy particle, you need sharp curvature to get the same kinetic energy.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#step-5-connecting-curvature-to-spatial-extent","title":"Step 5: Connecting Curvature to Spatial Extent","text":"<p>Imagine drawing a wavefunction that starts at zero (at a boundary), rises up, and must eventually return to zero (at another boundary or at infinity). How far does it extend before returning?</p> <p>Light particle (small m): Gentle curvature \u2192 The wavefunction rises slowly, bends back gradually, and takes a long distance to return to zero. Result: Wide, spread-out wavefunction.</p> <p>Heavy particle (large m): Sharp curvature \u2192 The wavefunction bends back quickly and returns to zero in a short distance. Result: Narrow, localized wavefunction.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#analogy-bending-rods","title":"Analogy: Bending Rods","text":"<p>Think of trying to bend flexible versus stiff rods into an arch: - A flexible rod (like a light particle's wavefunction) makes wide, gentle arcs - A stiff rod (like a heavy particle's wavefunction) makes tight, compact loops</p> <p>Mass acts like \"stiffness\" for the wavefunction's spatial behavior. Heavier particles have \"stiffer\" wavefunctions that resist spreading out.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#part-4-particle-in-a-box-a-concrete-example","title":"Part 4: Particle in a Box (A Concrete Example)","text":"<p>Let's see the curvature argument work out concretely in the simplest confined system.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-physical-setup","title":"The Physical Setup","text":"<p>A particle is trapped between two impenetrable walls at x = 0 and x = L. The potential is:</p> \\[V(x) = \\begin{cases} 0 &amp; \\text{inside the box } (0 &lt; x &lt; L) \\\\ \\infty &amp; \\text{outside the box} \\end{cases}\\] <p>Domain note: Inside the box, we're solving the free-particle Schr\u00f6dinger equation (V=0), but with boundary conditions that \\(\\psi(0) = \\psi(L) = 0\\). The infinite walls mean the wavefunction must be zero at the walls (the particle has zero probability of being at the walls).</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-ground-state-solution","title":"The Ground State Solution","text":"<p>The lowest-energy (ground state) wavefunction is a half sine wave:</p> \\[\\psi_1(x) = \\sqrt{\\frac{2}{L}} \\sin\\left(\\frac{\\pi x}{L}\\right)\\] <p>This wavefunction starts at zero (left wall), rises to a maximum in the middle, and returns to zero (right wall). It makes exactly one \"hump.\"</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-ground-state-energy","title":"The Ground State Energy","text":"<p>Substituting into the Schr\u00f6dinger equation gives the ground state energy:</p> \\[\\boxed{E_1 = \\frac{\\pi^2\\hbar^2}{2mL^2}}\\] <p>This is a crucial result. Notice that \\(E_1\\) depends on both the mass m and the box size L.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#comparison-1-same-box-size-different-masses","title":"Comparison 1: Same Box Size, Different Masses","text":"<p>Physical situation: We have a box of fixed size L and put different particles in it. What is the minimum energy each particle can have?</p> <p>Why this comparison matters: In real systems (atoms, quantum dots, nanostructures), the confining region often has a fixed size. We want to know how different particles behave in that same confinement.</p> Particle Mass Ground State Energy Electron \\(m_e\\) \\(E_1 = \\pi^2\\hbar^2/(2m_e L^2)\\) Proton \\(1836 m_e\\) \\(E_1 = \\pi^2\\hbar^2/(2 \\cdot 1836 \\cdot m_e L^2) = E_e/1836\\) <p>The proton has 1836 times less energy than the electron in the same box!</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#what-does-less-energy-mean-physically","title":"What Does \"Less Energy\" Mean Physically?","text":"<p>This is the key question. Let's understand it through the curvature argument:</p> <p>Both wavefunctions have the same shape (a half sine wave) because they must fit in the same box. Same shape means same curvature.</p> <p>From our master equation: \\(KE = (\\hbar^2/2m) \\times |\\text{curvature}|\\)</p> <p>For the same curvature: - Light particle: small m \u2192 large KE (high energy for this curvature) - Heavy particle: large m \u2192 small KE (low energy for this curvature)</p> <p>The heavy particle \"pays less\" in kinetic energy for the same amount of spatial bending. The light particle has much higher kinetic energy just to achieve the same gentle curve.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#comparison-2-same-energy-different-box-sizes","title":"Comparison 2: Same Energy, Different Box Sizes","text":"<p>Physical situation: We give each particle the same total energy E. How much space does each particle need to \"fit\"?</p> <p>Why this comparison matters: In thermal equilibrium at temperature T, particles have average energy proportional to T. This comparison tells us how localized different particles are at the same temperature.</p> <p>Rearranging \\(E_1 = \\pi^2\\hbar^2/(2mL^2)\\) to solve for L:</p> \\[L = \\frac{\\pi\\hbar}{\\sqrt{2mE}}\\] Particle Mass Box Size Needed for Energy E Electron \\(m_e\\) \\(L_e = \\pi\\hbar/\\sqrt{2m_e E}\\) Proton \\(1836 m_e\\) \\(L_p = \\pi\\hbar/\\sqrt{2 \\cdot 1836 \\cdot m_e E} = L_e/\\sqrt{1836} \\approx L_e/43\\) <p>For the same energy budget, the proton fits in a box 43 times smaller than the electron!</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#why-this-works-the-curvature-story","title":"Why This Works: The Curvature Story","text":"<p>To fit in a smaller box, the wavefunction must curve more sharply (it has less room to turn around). From \\(KE = (\\hbar^2/2m) \\times |\\text{curvature}|\\):</p> <ul> <li>Light particle: Even gentle curvature gives high KE. Sharp curvature (small box) would require enormous energy. So light particles need big boxes.</li> <li>Heavy particle: Curvature is \"cheap\" in energy terms. The particle can curve sharply without requiring much kinetic energy. So heavy particles fit in small boxes.</li> </ul>"},{"location":"quantum-mechanics/quantum-mass-localization/#everyday-analogy","title":"Everyday Analogy","text":"<p>Think of energy as money and spatial confinement as rent: - A light particle pays high rent to stay in a small space. With a fixed budget, it can only afford a large, cheap apartment. - A heavy particle pays low rent. The same budget lets it afford a small, \"expensive\" (tightly confined) space.</p> <p>This is why mass leads to localization: heavy particles can \"afford\" tight confinement on the same energy budget that forces light particles to spread out.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#part-5-the-harmonic-oscillator-smooth-potential-wells","title":"Part 5: The Harmonic Oscillator (Smooth Potential Wells)","text":"<p>The infinite square well has hard walls. Real confining potentials (atoms, molecules, traps) are usually smoother. The harmonic oscillator is the prototype of a \"bowl-shaped\" potential.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-physical-setup_1","title":"The Physical Setup","text":"<p>The potential energy is parabolic (like a ball in a bowl):</p> \\[V(x) = \\frac{1}{2}m\\omega^2 x^2\\] <p>where \\(\\omega\\) (omega) is the angular frequency of oscillation. The potential is lowest at x = 0 and increases as the particle moves away from center.</p> <p>Domain note: This is the potential for a quantum harmonic oscillator. We're solving the time-independent Schr\u00f6dinger equation with this specific \\(V(x)\\) to find stationary states.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-ground-state-wavefunction","title":"The Ground State Wavefunction","text":"<p>The ground state is a Gaussian (bell curve):</p> \\[\\psi_0(x) = \\left(\\frac{m\\omega}{\\pi\\hbar}\\right)^{1/4} \\exp\\left(-\\frac{m\\omega x^2}{2\\hbar}\\right)\\] <p>The width of this Gaussian (specifically, its standard deviation \\(\\sigma\\)) tells us how spread out the particle is:</p> \\[\\sigma = \\sqrt{\\frac{\\hbar}{2m\\omega}}\\]"},{"location":"quantum-mechanics/quantum-mass-localization/#the-mass-dependence-of-width","title":"The Mass Dependence of Width","text":"<p>The width \\(\\sigma\\) is proportional to \\(1/\\sqrt{m}\\). For the same oscillator frequency \\(\\omega\\):</p> Particle Mass Ground State Width Electron \\(m_e\\) \\(\\sigma_e = \\sqrt{\\hbar/(2m_e\\omega)}\\) Proton \\(1836 m_e\\) \\(\\sigma_p = \\sqrt{\\hbar/(2 \\cdot 1836 \\cdot m_e\\omega)} = \\sigma_e/\\sqrt{1836} \\approx \\sigma_e/43\\) <p>The proton's ground state is 43 times narrower than the electron's in the same harmonic trap!</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#physical-interpretation","title":"Physical Interpretation","text":"<p>In a harmonic oscillator, the particle oscillates back and forth around the center. The width \\(\\sigma\\) represents how far from center the particle typically ventures.</p> <ul> <li>A heavier particle has more inertia\u2014it settles into a tighter region near the bottom of the potential well. The same restoring force confines it more effectively.</li> <li>A lighter particle experiences larger quantum \"jitters.\" Its inherent wave-like fuzziness carries it farther from center. The same restoring force cannot hold it as tightly.</li> </ul>"},{"location":"quantum-mechanics/quantum-mass-localization/#part-6-free-particle-wave-packet-spreading","title":"Part 6: Free Particle Wave Packet Spreading","text":"<p>Now we leave potential wells behind. What happens to a localized particle in free space (\\(V = 0\\) everywhere)? It spreads out\u2014but the rate depends critically on mass.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-setup-a-gaussian-wave-packet","title":"The Setup: A Gaussian Wave Packet","text":"<p>Domain note: We're now discussing time-dependent wavefunctions, not stationary states. The wavefunction evolves according to the time-dependent Schr\u00f6dinger equation.</p> <p>At time t = 0, we prepare a particle as a localized \"blob\" of probability\u2014a Gaussian wave packet:</p> \\[\\psi(x,0) = \\left[\\frac{1}{2\\pi\\sigma_0^2}\\right]^{1/4} \\exp\\left(-\\frac{x^2}{4\\sigma_0^2}\\right) \\exp\\left(\\frac{ip_0 x}{\\hbar}\\right)\\] <p>What each part means:</p> Part Mathematical Form Physical Meaning Normalization \\([1/(2\\pi\\sigma_0^2)]^{1/4}\\) Ensures total probability = 1 Gaussian envelope \\(\\exp(-x^2/4\\sigma_0^2)\\) Particle localized near x=0 with width \\(\\sigma_0\\) Momentum factor \\(\\exp(ip_0 x/\\hbar)\\) Particle has average momentum \\(p_0\\) (it's moving) <p>Definition: \\(\\sigma_0\\) is the initial width\u2014the standard deviation of the position distribution at t = 0. It quantifies how localized the particle is initially.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#why-does-a-free-particle-spread","title":"Why Does a Free Particle Spread?","text":"<p>This is the key to understanding. A localized wave packet is actually a superposition (sum) of many plane waves with different momenta. Think of it as a \"swarm\" of waves:</p> \\[\\psi(x,t) = \\int A(k) \\exp[i(kx - \\omega(k)t)] dk\\] <p>Domain clarification: Each component of this integral is a plane wave with definite wave number k (where momentum \\(p = \\hbar k\\)). The de Broglie relation \\(\\lambda = h/p = 2\\pi/k\\) applies exactly to each of these plane wave components.</p> <p>Each wave component has a different wave number k (related to momentum by \\(p = \\hbar k\\)), and each travels at its own velocity. For a free particle, the velocity of each component is:</p> \\[v = \\frac{p}{m} = \\frac{\\hbar k}{m}\\] <p>Components with different k travel at different speeds. Over time, they get out of sync, and the packet spreads out. This is called dispersion.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-width-as-a-function-of-time","title":"The Width as a Function of Time","text":"<p>Solving the time-dependent Schr\u00f6dinger equation for the time evolution of a Gaussian packet gives:</p> \\[\\sigma(t) = \\sigma_0 \\sqrt{1 + \\left(\\frac{\\hbar t}{2m\\sigma_0^2}\\right)^2}\\] <p>This formula says the width grows from its initial value \\(\\sigma_0\\). The rate of growth depends on the combination \\(\\hbar/(m\\sigma_0^2)\\).</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-spreading-rate","title":"The Spreading Rate","text":"<p>For long times (\\(t \\gg 2m\\sigma_0^2/\\hbar\\)), the formula simplifies to:</p> \\[\\sigma(t) \\approx \\frac{\\hbar t}{2m\\sigma_0}\\] <p>The spreading rate \\(d\\sigma/dt\\) is approximately:</p> \\[\\text{Spreading rate} \\approx \\frac{\\hbar}{2m\\sigma_0} \\propto \\frac{1}{m}\\] <p>The spreading rate is inversely proportional to mass!</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#concrete-comparison-electron-vs-proton","title":"Concrete Comparison: Electron vs. Proton","text":"<p>Physical situation: Both particles start with the same initial width \\(\\sigma_0 = 10\\) nm (\\(10^{-8}\\) m). How quickly does each spread?</p> Particle Mass (kg) Time to Double Width Electron \\(9.1 \\times 10^{-31}\\) ~0.2 picoseconds (\\(10^{-12}\\) s) Proton \\(1.67 \\times 10^{-27}\\) ~0.3 nanoseconds (\\(10^{-9}\\) s) <p>The proton takes ~1836 times longer to spread by the same amount!</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#why-does-mass-slow-spreading-rigorous-treatment","title":"Why Does Mass Slow Spreading? (Rigorous Treatment)","text":"<p>This requires understanding group velocity and carefully defining what we mean by \"velocity spread.\" Let's be meticulous.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#what-velocity-are-we-talking-about","title":"What Velocity Are We Talking About?","text":"<p>Critical distinction: For wave packets, there are two types of velocities: 1. Phase velocity \\(v_{\\text{phase}} = \\omega/k\\) - how fast the wiggles of the wave move 2. Group velocity \\(v_{\\text{group}} = d\\omega/dk\\) - how fast the envelope (the probability blob) moves</p> <p>But there's a deeper subtlety you need to understand first!</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-two-meanings-of-group-velocity","title":"The Two Meanings of \"Group Velocity\"","text":"<p>This is where many students get confused, so read carefully:</p> <p>Meaning 1: The velocity of the packet as a whole</p> <p>You're right to think: \"A wave packet has ONE group velocity\u2014the speed at which the peak moves.\" This is correct! If the packet is centered at momentum \\(p_0\\), the peak of the packet travels at:</p> \\[v_{\\text{peak}} = \\left.\\frac{d\\omega}{dk}\\right|_{k_0} = \\frac{p_0}{m}\\] <p>This is \"the\" group velocity you learn about first\u2014the speed of the envelope maximum.</p> <p>Meaning 2: The group velocities of the individual components</p> <p>But here's the key: the packet is built from MANY plane wave components, each with its own momentum p. Each component propagates at its own velocity:</p> \\[v(p) = \\frac{p}{m}\\] <p>If the packet contains momenta from \\((p_0 - \\Delta p)\\) to \\((p_0 + \\Delta p)\\), then these components travel at velocities from: - Slowest: \\(v_{\\text{slow}} = (p_0 - \\Delta p)/m\\) - Fastest: \\(v_{\\text{fast}} = (p_0 + \\Delta p)/m\\)</p> <p>The spread in these component velocities is: \\(\\Delta v = v_{\\text{fast}} - v_{\\text{slow}} = 2\\Delta p/m\\)</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#why-this-matters-dispersive-vs-non-dispersive","title":"Why This Matters: Dispersive vs Non-Dispersive","text":"<p>The crucial question: Do all the components travel at the same speed?</p> <p>Non-dispersive medium (like sound waves in air at low frequencies): - All frequencies have the same group velocity - All Fourier components travel together - The packet moves but doesn't spread - You can send a clean pulse that stays clean</p> <p>Dispersive medium (like quantum particles, or light in glass): - Different frequencies have different group velocities - For quantum free particles: \\(v = p/m\\), so higher-momentum components travel faster - The Fourier components separate from each other over time - The packet spreads as it travels</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-physical-picture","title":"The Physical Picture","text":"<p>Imagine your wave packet at t = 0 as a bell curve. This bell curve is actually the SUM of many sine waves (Fourier components):</p> Text Only<pre><code>\u03c8(x,0) = \u222b A(k) e^(ikx) dk\n         \u2191\n         Sum of many plane waves with different k\n</code></pre> <p>At t = 0, all these sine waves add up constructively at the center, creating the bell curve.</p> <p>Now time evolves:</p> <ul> <li>The component with momentum \\(p_1\\) propagates as: \\(e^{i(k_1 x - \\omega_1 t)}\\), moving at \\(v_1 = p_1/m\\)</li> <li>The component with momentum \\(p_2\\) propagates as: \\(e^{i(k_2 x - \\omega_2 t)}\\), moving at \\(v_2 = p_2/m\\)</li> <li>The component with momentum \\(p_3\\) propagates as: \\(e^{i(k_3 x - \\omega_3 t)}\\), moving at \\(v_3 = p_3/m\\)</li> <li>...and so on for all components</li> </ul> <p>Since \\(v_1 \\neq v_2 \\neq v_3\\) (because they have different momenta), these sine waves gradually get out of sync\u2014they dephase.</p> <ul> <li>Initially, they all interfered constructively to make a tight packet</li> <li>Over time, fast components move ahead, slow components lag behind</li> <li>The packet spreads because the constructive interference that created the localized \"bump\" falls apart</li> </ul> <p>The center of mass of the packet still moves at \\(v_0 = p_0/m\\) (the \"main\" group velocity), but the packet gets wider and wider.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#a-better-analogy","title":"A Better Analogy","text":"<p>Think of marathon runners (the Fourier components): - All start together at the starting line (t = 0, constructive interference creates localized packet) - They run at slightly different speeds (different momenta \u2192 different velocities v = p/m) - The center of the pack moves at the average speed (this is \"the\" group velocity of the packet) - But the pack spreads out over time as faster runners pull ahead and slower runners fall behind - After an hour, the pack is stretched over a much longer distance</p> <p>The spread rate depends on: - How different the runners' speeds are (velocity spread \u0394v) - For quantum particles: \u0394v = \u0394p/m, so lighter particles spread faster</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#summary-of-the-resolution","title":"Summary of the Resolution","text":"<p>You thought: \"The packet has one group velocity and just spreads out somehow.\" Actually: The packet's peak moves at one group velocity, BUT the packet is made of components that each move at their own group velocities. These components separate, causing the spread.</p> <p>The correct picture: - The packet's center moves at \\(v_{\\text{center}} = p_0/m\\) (this is \"the\" group velocity you learned about) - But the packet is built from components with momenta near \\(p_0\\) - These components have velocities \\(v(p) = p/m\\), which differ by \\(\\Delta v \\approx \\Delta p/m\\) - As time passes, the components separate, and the packet width grows</p> <p>For a free quantum particle, group velocity DEPENDS on momentum: \\(v = p/m\\). This makes quantum mechanics dispersive, which is why wave packets spread!</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-mathematics-dispersion-relation-for-free-particles","title":"The Mathematics: Dispersion Relation for Free Particles","text":"<p>Now let's make this rigorous. For a free particle (V = 0), the dispersion relation is:</p> \\[E = \\frac{p^2}{2m} = \\frac{\\hbar^2 k^2}{2m}\\] <p>Therefore: \\(\\omega = E/\\hbar = \\hbar k^2/(2m)\\)</p> <p>The group velocity for a plane wave component with wave number k is:</p> \\[v_{\\text{group}}(k) = \\frac{d\\omega}{dk} = \\frac{d}{dk}\\left(\\frac{\\hbar k^2}{2m}\\right) = \\frac{\\hbar k}{m} = \\frac{p}{m}\\] <p>Key observation: This group velocity depends on k (or p)!</p> <ul> <li>A component with momentum \\(p\\) travels at \\(v = p/m\\)</li> <li>A component with momentum \\(p + \\delta p\\) travels at \\(v + \\delta v = (p + \\delta p)/m\\)</li> <li>They separate at rate \\(\\delta v = \\delta p / m\\)</li> </ul> <p>This is dispersion! Different frequency components travel at different speeds.</p> <p>Domain of validity: This \\(v = p/m\\) relationship for group velocity applies specifically to free particles (V = 0). In a general potential, the dispersion relation is more complex and \\(v \\neq p/m\\).</p> \\[E = \\frac{p^2}{2m} = \\frac{\\hbar^2 k^2}{2m}\\] <p>Therefore: \\(\\omega = E/\\hbar = \\hbar k^2/(2m)\\)</p> <p>The group velocity (the speed at which that momentum component's contribution to the packet moves) is:</p> \\[v_{\\text{group}} = \\frac{d\\omega}{dk} = \\frac{d}{dk}\\left(\\frac{\\hbar k^2}{2m}\\right) = \\frac{\\hbar k}{m} = \\frac{p}{m}\\] <p>This is the key: When we write \\(v = p/m\\), we're talking about the group velocity of each plane wave component in the free-space wave packet. This is the velocity at which that particular momentum component contributes to the probability distribution.</p> <p>Domain of validity: This \\(v = p/m\\) relationship for group velocity applies specifically to free particles (V = 0). In a general potential, the dispersion relation is more complex and \\(v \\neq p/m\\).</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#defining-momentum-spread-rigorously","title":"Defining Momentum Spread Rigorously","text":"<p>The wave packet has a momentum-space wavefunction \\(\\phi(p)\\). The momentum spread is the standard deviation:</p> \\[\\Delta p = \\sqrt{\\langle p^2 \\rangle - \\langle p \\rangle^2}\\] <p>where: $\\(\\langle p \\rangle = \\int_{-\\infty}^{\\infty} p |\\phi(p)|^2 dp \\quad \\text{(average momentum)}\\)$</p> \\[\\langle p^2 \\rangle = \\int_{-\\infty}^{\\infty} p^2 |\\phi(p)|^2 dp \\quad \\text{(average of momentum squared)}\\] <p>Physical meaning: \\(\\Delta p\\) tells you the \"width\" of the momentum distribution. If the packet has a narrow range of momenta, \\(\\Delta p\\) is small. If it contains a wide spectrum of momenta, \\(\\Delta p\\) is large.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#defining-velocity-spread-rigorously","title":"Defining Velocity Spread Rigorously","text":"<p>Now here's the subtle part. For a free particle, each momentum component travels at group velocity \\(v = p/m\\). We want to know: what is the spread in these group velocities?</p> <p>Since group velocity \\(v = p/m\\), we can think of velocity as a function of momentum: \\(v(p) = p/m\\).</p> <p>The velocity spread \\(\\Delta v\\) is the standard deviation of the group velocity distribution:</p> \\[\\Delta v = \\sqrt{\\langle v^2 \\rangle - \\langle v \\rangle^2}\\] <p>where: $\\(\\langle v \\rangle = \\int_{-\\infty}^{\\infty} \\frac{p}{m} |\\phi(p)|^2 dp = \\frac{1}{m} \\int_{-\\infty}^{\\infty} p |\\phi(p)|^2 dp = \\frac{\\langle p \\rangle}{m}\\)$</p> \\[\\langle v^2 \\rangle = \\int_{-\\infty}^{\\infty} \\left(\\frac{p}{m}\\right)^2 |\\phi(p)|^2 dp = \\frac{1}{m^2} \\int_{-\\infty}^{\\infty} p^2 |\\phi(p)|^2 dp = \\frac{\\langle p^2 \\rangle}{m^2}\\] <p>Therefore: $\\(\\Delta v^2 = \\langle v^2 \\rangle - \\langle v \\rangle^2 = \\frac{\\langle p^2 \\rangle}{m^2} - \\frac{\\langle p \\rangle^2}{m^2} = \\frac{1}{m^2}\\left(\\langle p^2 \\rangle - \\langle p \\rangle^2\\right) = \\frac{(\\Delta p)^2}{m^2}\\)$</p> <p>Taking the square root:</p> \\[\\boxed{\\Delta v = \\frac{\\Delta p}{m}}\\] <p>This is the rigorous derivation. The velocity spread equals the momentum spread divided by mass.</p> <p>What this means physically: - The momentum-space wavefunction \\(\\phi(p)\\) has width \\(\\Delta p\\) (how spread out the momenta are) - Each momentum component moves at group velocity \\(v = p/m\\) - The range of group velocities therefore has width \\(\\Delta v = \\Delta p/m\\) - This is specifically for free particles where the group velocity is p/m</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#why-different-masses-give-different-spreading-rates","title":"Why Different Masses Give Different Spreading Rates","text":"<p>Now we can understand spreading with the correct picture:</p> <p>The wave packet at t = 0: - Centered at position \\(x_0\\) - Peak momentum \\(p_0\\) - Momentum spread \\(\\Delta p\\) (contains components from \\(p_0 - \\Delta p\\) to \\(p_0 + \\Delta p\\))</p> <p>Evolution over time:</p> <p>At t = 0: All Fourier components are in phase \u2192 constructive interference \u2192 localized packet</p> <p>At time t later: - Component with momentum \\(p_0\\) has traveled distance: \\(x_{\\text{center}} = p_0 t/m\\) - Component with momentum \\(p_0 + \\Delta p\\) has traveled: \\(x_{\\text{fast}} = (p_0 + \\Delta p)t/m\\) - Component with momentum \\(p_0 - \\Delta p\\) has traveled: \\(x_{\\text{slow}} = (p_0 - \\Delta p)t/m\\)</p> <p>The packet width is now roughly the separation between fastest and slowest components:</p> \\[\\text{Width}(t) \\approx x_{\\text{fast}} - x_{\\text{slow}} = \\frac{(p_0 + \\Delta p)t}{m} - \\frac{(p_0 - \\Delta p)t}{m} = \\frac{2\\Delta p \\cdot t}{m}\\] <p>The spreading rate is:</p> \\[\\frac{d(\\text{Width})}{dt} \\approx \\frac{2\\Delta p}{m} = 2\\Delta v\\] <p>This grows linearly with time! The packet gets wider and wider as the components separate.</p> <p>For the same momentum distribution (same \\(\\Delta p\\)):</p> Particle Velocity Spread Component Separation After Time t Physical Consequence Light (small m) Large \\(\\Delta v = \\Delta p/m\\) Large: \\(\\approx 2\\Delta p \\cdot t/m\\) Fast spreading - components quickly separate Heavy (large m) Small \\(\\Delta v = \\Delta p/m\\) Small: \\(\\approx 2\\Delta p \\cdot t/m\\) Slow spreading - components stay closer together <p>The concrete mechanism: 1. The momentum spread \\(\\Delta p\\) is a property of the initial wave packet 2. This translates to a velocity spread: \\(\\Delta v = \\Delta p/m\\) 3. Over time t, components with different velocities separate by distance \\(\\sim \\Delta v \\cdot t\\) 4. Mass appears in the denominator, so heavier particles spread slower</p> <p>The packet's center moves at \\(v_{\\text{center}} = p_0/m\\), but the packet width grows as \\(\\sim (\\Delta p/m) \\cdot t\\).</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#connecting-to-the-uncertainty-principle","title":"Connecting to the Uncertainty Principle","text":"<p>Important note: The momentum spread \\(\\Delta p\\) we've been discussing here is the same \\(\\Delta p\\) that appears in the Heisenberg uncertainty principle \\(\\Delta x \\cdot \\Delta p \\geq \\hbar/2\\).</p> <ul> <li>A localized wave packet (small \\(\\Delta x\\)) must have a wide momentum distribution (large \\(\\Delta p\\)) to satisfy the uncertainty principle</li> <li>This wide momentum distribution means wide velocity distribution: \\(\\Delta v = \\Delta p/m\\)</li> <li>The wide velocity distribution is what causes spreading</li> </ul> <p>The full story: 1. Confine particle to small \\(\\Delta x\\) \u2192 forces large \\(\\Delta p\\) (uncertainty principle) 2. Large \\(\\Delta p\\) means large \\(\\Delta v\\) (for light particles especially) 3. Large \\(\\Delta v\\) means rapid spreading (group velocity dispersion)</p> <p>This is why light particles can't stay localized\u2014confinement forces them to have large momentum/velocity spread, which causes them to immediately disperse!</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#race-analogy-now-precise","title":"Race Analogy (Now Precise)","text":"<p>Think of the wave packet's Fourier components as runners in a marathon, where: - Each runner represents a plane wave component with a specific momentum p - The runner's speed is their group velocity: \\(v = p/m\\) - The packet's momentum spread \\(\\Delta p\\) means runners have momenta ranging from \\(p_0 - \\Delta p\\) to \\(p_0 + \\Delta p\\)</p> <p>At the starting gun (t = 0): - All runners line up at the starting line together - This represents all Fourier components being in phase - Their constructive interference creates the localized wave packet</p> <p>During the race (t &gt; 0):</p> <p>Light particle (small m): - A runner with momentum \\(p\\) runs at speed \\(v = p/m\\) - A runner with slightly more momentum \\(p + \\delta p\\) runs at \\(v + \\delta v = (p + \\delta p)/m\\) - Small mass in denominator \u2192 even tiny momentum differences create large velocity differences - Runners with slightly different momenta run at very different speeds - The pack quickly spreads out along the track - After time t, the pack is stretched over distance \\(\\sim (\\Delta p/m) \\cdot t\\)</p> <p>Heavy particle (large m): - Same momentum differences between runners - But now large mass in denominator \u2192 small velocity differences - Runners with different momenta all run at nearly the same speed - The pack stays bunched together much longer - After the same time t, the pack is only stretched over \\(\\sim (\\Delta p/m) \\cdot t\\), which is much smaller</p> <p>The center of the pack moves at the average speed \\(\\bar{v} = p_0/m\\) (this is \"the\" group velocity of the packet). But what we care about for spreading is how much the pack stretches out\u2014and that depends on the spread in individual runner velocities: \\(\\Delta v = \\Delta p/m\\).</p> <p>The key insight: Mass is in the denominator of the velocity formula \\(v = p/m\\). This makes the velocity spread smaller for heavier particles, even when they have the same momentum uncertainty. The marathon runners stay closer together, so the wave packet spreads slower.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#part-7-the-heisenberg-uncertainty-principle","title":"Part 7: The Heisenberg Uncertainty Principle","text":"<p>This principle provides another powerful lens for understanding why mass leads to localization.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-fundamental-relation","title":"The Fundamental Relation","text":"\\[\\boxed{\\Delta x \\cdot \\Delta p \\geq \\frac{\\hbar}{2}}\\] <p>Domain of validity: This is a general principle that applies to ANY quantum state (stationary or time-dependent, free space or in a potential, plane wave or wave packet). It's a fundamental property of the wavefunction.</p> <p>Definition of terms:</p> Symbol Name Precise Meaning \\(\\Delta x\\) Position uncertainty Standard deviation of position distribution; measures how spread out the particle is in space \\(\\Delta p\\) Momentum uncertainty Standard deviation of momentum distribution; measures the range of momenta present in the wavefunction \\(\\hbar/2\\) Minimum product About \\(5.3 \\times 10^{-35}\\) J\u00b7s; a fundamental quantum limit <p>What the principle says: You cannot simultaneously know both position and momentum with arbitrary precision. The more precisely you know one, the less precisely you know the other. The product of uncertainties has a minimum value.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#introducing-velocity-uncertainty-careful-treatment","title":"Introducing Velocity Uncertainty (Careful Treatment)","text":"<p>This section requires care because \"velocity uncertainty\" means something specific in quantum mechanics.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-question-what-is-velocity-in-quantum-mechanics","title":"The Question: What Is \"Velocity\" in Quantum Mechanics?","text":"<p>In classical mechanics, velocity is well-defined: \\(v = p/m\\). But in quantum mechanics, both position and momentum can be uncertain, so what do we mean by \"velocity uncertainty\"?</p> <p>Two approaches to defining velocity uncertainty:</p> <p>Approach 1: Velocity operator (formal quantum mechanics)</p> <p>In quantum mechanics, we can define a velocity operator: $\\(\\hat{v} = \\frac{\\hat{p}}{m}\\)$</p> <p>Just like position and momentum operators, this has a probability distribution with mean and spread: $\\(\\langle v \\rangle = \\frac{\\langle p \\rangle}{m}\\)$ $\\(\\langle v^2 \\rangle = \\frac{\\langle p^2 \\rangle}{m^2}\\)$</p> <p>The uncertainty in velocity (standard deviation) is: $\\(\\Delta v = \\sqrt{\\langle v^2 \\rangle - \\langle v \\rangle^2} = \\sqrt{\\frac{\\langle p^2 \\rangle}{m^2} - \\frac{\\langle p \\rangle^2}{m^2}} = \\frac{1}{m}\\sqrt{\\langle p^2 \\rangle - \\langle p \\rangle^2} = \\frac{\\Delta p}{m}\\)$</p> <p>Approach 2: Physical interpretation for free particles</p> <p>For a free particle (as we discussed in the wave packet section), each momentum component travels at group velocity \\(v = p/m\\). The uncertainty in velocity is the spread in these group velocities:</p> \\[\\Delta v = \\frac{\\Delta p}{m}\\] <p>Both approaches give the same result: The velocity uncertainty equals the momentum uncertainty divided by mass.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-rigorous-relationship","title":"The Rigorous Relationship","text":"<p>Momentum and velocity uncertainties are related by:</p> \\[\\boxed{\\Delta p = m \\cdot \\Delta v}\\] <p>Derivation (using standard deviations):</p> <p>Starting with definitions: - \\(\\Delta p = \\sqrt{\\langle p^2 \\rangle - \\langle p \\rangle^2}\\) (standard deviation of momentum) - \\(\\Delta v = \\sqrt{\\langle v^2 \\rangle - \\langle v \\rangle^2}\\) (standard deviation of velocity)</p> <p>With \\(v = p/m\\): $\\(\\langle v \\rangle = \\frac{\\langle p \\rangle}{m}\\)$ $\\(\\langle v^2 \\rangle = \\left\\langle \\frac{p^2}{m^2} \\right\\rangle = \\frac{\\langle p^2 \\rangle}{m^2}\\)$</p> <p>Therefore: $\\(\\Delta v^2 = \\langle v^2 \\rangle - \\langle v \\rangle^2 = \\frac{\\langle p^2 \\rangle}{m^2} - \\frac{\\langle p \\rangle^2}{m^2} = \\frac{1}{m^2}(\\langle p^2 \\rangle - \\langle p \\rangle^2) = \\frac{(\\Delta p)^2}{m^2}\\)$</p> <p>Taking square root: \\(\\Delta v = \\Delta p/m\\)</p> <p>Multiplying both sides by m: \\(\\Delta p = m \\cdot \\Delta v\\) \u2713</p> <p>What these uncertainties represent: - \\(\\Delta p\\): the spread (standard deviation) of the momentum probability distribution \\(|\\phi(p)|^2\\) - \\(\\Delta v\\): the spread (standard deviation) of the velocity probability distribution - These are NOT the same as \"error bars\" or \"measurement uncertainty\" - they're intrinsic properties of the quantum state</p> <p>Domain of validity: While \\(\\Delta p = m \\cdot \\Delta v\\) is derived using \\(v = p/m\\), it's actually valid for any quantum state, not just free particles. The velocity operator \\(\\hat{v} = \\hat{p}/m\\) is well-defined in general.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#rewriting-the-uncertainty-principle-in-terms-of-velocity","title":"Rewriting the Uncertainty Principle in Terms of Velocity","text":"<p>Substituting \\(\\Delta p = m \\cdot \\Delta v\\) into the standard uncertainty principle:</p> \\[\\Delta x \\cdot \\Delta p \\geq \\frac{\\hbar}{2}\\] <p>becomes:</p> \\[\\Delta x \\cdot (m \\cdot \\Delta v) \\geq \\frac{\\hbar}{2}\\] <p>Rearranging to solve for velocity uncertainty:</p> \\[\\boxed{\\Delta v \\geq \\frac{\\hbar}{2m \\cdot \\Delta x}}\\] <p>This tells us: If you confine a particle to a region of size \\(\\Delta x\\), its velocity must be uncertain by at least \\(\\hbar/(2m \\cdot \\Delta x)\\).</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#what-this-equation-tells-us","title":"What This Equation Tells Us","text":"<p>Physical situation: Confine two different particles to the same spatial region (same \\(\\Delta x\\)). What happens?</p> <p>Why this comparison matters: We're asking \"If I localize two particles to the same degree, how do they behave?\" This directly addresses the localization question.</p> Particle Mass Minimum Velocity Uncertainty Electron \\(m_e\\) \\(\\Delta v_e = \\hbar/(2m_e \\cdot \\Delta x)\\) Proton \\(1836 m_e\\) \\(\\Delta v_p = \\hbar/(2 \\cdot 1836 \\cdot m_e \\cdot \\Delta x) = \\Delta v_e/1836\\) <p>For the same spatial confinement, the proton has 1836 times less velocity uncertainty than the electron!</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#physical-consequences-of-velocity-uncertainty","title":"Physical Consequences of Velocity Uncertainty","text":"<p>Now that we understand \\(\\Delta v\\) rigorously, what does it mean physically?</p> <p>For a wave packet in free space: - The packet is a superposition of plane waves with different momenta - Each plane wave component has momentum p and contributes with group velocity \\(v = p/m\\) - The range of group velocities present is \\(\\Delta v = \\Delta p/m\\) - Components with different velocities will separate over time, causing the packet to spread</p> <p>High velocity uncertainty (\\(\\Delta v\\) large): - The momentum components span a wide range - Their group velocities differ greatly: some parts of the wavefunction's momentum content travel fast, others slow - The packet rapidly disperses - Distance between fastest and slowest components grows as \\(\\Delta v \\cdot t\\)</p> <p>Low velocity uncertainty (\\(\\Delta v\\) small): - The momentum components are clustered in a narrow range - Their group velocities are similar: most of the wavefunction's momentum content travels at nearly the same speed - The packet holds together longer - Components don't separate as quickly</p> <p>What \"different parts moving at different speeds\" means: - It's not that the particle is in multiple places moving differently - Rather, the wave packet's Fourier components (the plane waves that add up to make the packet) each propagate at their own group velocity - When we say \"the electron's velocity uncertainty is 58,000 m/s,\" we mean: the momentum spectrum of the electron's wavefunction is wide enough that the fastest-moving Fourier components outpace the slowest by 58,000 m/s</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#comparison-confined-particles","title":"Comparison: Confined Particles","text":"<p>Confined electron: - High velocity uncertainty: \\(\\Delta v = \\hbar/(2m_e \\Delta x)\\) is large - The momentum spectrum is wide - Group velocities of the Fourier components differ greatly - The packet (if released from confinement) would rapidly fly apart - Analogy: Like confining a hyperactive child who, the moment you let go, scatters in all directions at once</p> <p>Confined proton: - Low velocity uncertainty: \\(\\Delta v = \\hbar/(2m_p \\Delta x)\\) is small - The momentum spectrum is narrow - Group velocities of the Fourier components are similar - The packet (if released) would disperse slowly - Analogy: Like a calm adult who, when released, wanders off slowly and stays roughly localized</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#concrete-example","title":"Concrete Example","text":"<p>Physical situation: Confine both particles to \\(\\Delta x = 1\\) nanometer (\\(10^{-9}\\) m).</p> Particle Position Uncertainty Minimum Velocity Uncertainty Electron 1 nm ~58,000 m/s Proton 1 nm ~32 m/s <p>The electron, when confined to 1 nm, has velocity components spanning ~58,000 m/s in different directions! The proton's velocity spread is only ~32 m/s. No wonder the electron spreads out so much faster.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-self-consistency-of-localization","title":"The Self-Consistency of Localization","text":"<p>Now we can understand why heavy particles \"stay put\" while light particles spread:</p> <ol> <li>Confine a light particle: Small \\(\\Delta x\\) forces large \\(\\Delta v\\) \u2192 particle rapidly escapes confinement \u2192 cannot stay localized</li> <li>Confine a heavy particle: Same \\(\\Delta x\\) requires only small \\(\\Delta v\\) \u2192 particle doesn't have the \"velocity budget\" to escape \u2192 stays localized</li> </ol> <p>Mass acts as a \"brake\" on the velocity uncertainty that confinement creates. Heavy particles can be confined without gaining the velocity spread that would let them escape.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#part-8-the-fourier-transform-perspective","title":"Part 8: The Fourier Transform Perspective","text":"<p>This is the most mathematically elegant view, showing why the uncertainty principle exists.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#position-and-momentum-as-fourier-pairs","title":"Position and Momentum as Fourier Pairs","text":"<p>A fundamental fact of quantum mechanics: the wavefunction in position space \\(\\psi(x)\\) and the wavefunction in momentum space \\(\\phi(p)\\) are related by Fourier transforms:</p> \\[\\phi(p) = \\frac{1}{\\sqrt{2\\pi\\hbar}} \\int \\psi(x) \\exp\\left(-\\frac{ipx}{\\hbar}\\right) dx\\] \\[\\psi(x) = \\frac{1}{\\sqrt{2\\pi\\hbar}} \\int \\phi(p) \\exp\\left(\\frac{ipx}{\\hbar}\\right) dp\\] <p>Domain note: This relationship is completely general\u2014it applies to any quantum state, any potential, any time. It's a mathematical property of how quantum mechanics represents states.</p> <p>What this means: Position and momentum descriptions contain the same information, just encoded differently. They are two \"views\" of the same quantum state, related by a mathematical transformation (the Fourier transform).</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-fourier-uncertainty-principle","title":"The Fourier Uncertainty Principle","text":"<p>A fundamental mathematical property of Fourier transforms (nothing specifically quantum!):</p> <p>A narrow function in one domain must be wide in the conjugate domain.</p> <p>This applies to sound waves, signal processing, image analysis\u2014anywhere Fourier transforms appear. For quantum mechanics:</p> <ul> <li>Narrow \\(\\psi(x)\\) in position \u2192 Wide \\(\\phi(p)\\) in momentum</li> <li>Narrow \\(\\phi(p)\\) in momentum \u2192 Wide \\(\\psi(x)\\) in position</li> </ul> <p>Mathematically: \\(\\Delta x \\cdot \\Delta p \\geq \\hbar/2\\)</p> <p>The Heisenberg uncertainty principle is just the Fourier uncertainty principle applied to quantum wavefunctions!</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#where-does-mass-enter","title":"Where Does Mass Enter?","text":"<p>Mass doesn't appear in the Fourier relationship between x and p. But it crucially appears in time evolution.</p> <p>Each momentum component of the wavefunction evolves in time as:</p> \\[\\exp(-iEt/\\hbar) = \\exp\\left(-i\\frac{p^2 t}{2m\\hbar}\\right)\\] <p>Domain note: This is the time evolution for a free particle (V = 0). In a general potential, the time dependence is more complicated, but the mass-dependence of the kinetic energy term remains.</p> <p>The phase accumulated depends on \\(p^2/m\\). Different momentum components acquire different phases at a rate that depends on mass:</p> <ul> <li>Large m: Phases change slowly \u2192 Components stay in sync \u2192 Packet holds together</li> <li>Small m: Phases change quickly \u2192 Components dephase rapidly \u2192 Packet spreads</li> </ul>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-dephasing-picture","title":"The Dephasing Picture","text":"<p>Think of the wave packet as a choir of singers (each momentum component) trying to sing in harmony. At t = 0, they're all in phase\u2014constructive interference creates the localized packet.</p> <p>As time passes, each singer's pitch drifts at a rate proportional to \\(p^2/m\\):</p> <ul> <li>For a heavy particle (large m), the drift is slow\u2014the choir stays in harmony, the packet stays localized</li> <li>For a light particle (small m), the drift is fast\u2014the harmony breaks down, the packet spreads</li> </ul>"},{"location":"quantum-mechanics/quantum-mass-localization/#part-9-the-inevitable-localization-a-classical-quantum-bridge","title":"Part 9: The Inevitable Localization - A Classical-Quantum Bridge","text":""},{"location":"quantum-mechanics/quantum-mass-localization/#classical-rolling-balls-in-a-u-shaped-valley","title":"Classical: Rolling Balls in a U-Shaped Valley","text":"<p>Imagine two frictionless balls - a marble (electron) and a bowling ball (proton) - rolling in a U-shaped valley:</p> Text Only<pre><code>        \u25cfm\n       /   \\\n      /     \\\n     /       \\\n____/    \u25cfM   \\____\n</code></pre> <p>Both have the same total energy E.</p> <p>At the edges (maximum height):</p> <p>Total energy = potential energy: \\(E = mgh_{max}\\)</p> <p>So maximum height: \\(h_{max} = \\frac{E}{mg}\\)</p> <ul> <li>Heavy ball (M): \\(h_{max} = \\frac{E}{Mg}\\) \u2014 doesn't roll up very high (small \u0394h)</li> <li>Light ball (m): \\(h_{max} = \\frac{E}{mg}\\) \u2014 rolls up much higher (large \u0394h)</li> </ul> <p>The heavier ball stays in the valley; the lighter ball explores more space!</p> <p>At the bottom (center of valley):</p> <p>Total energy = kinetic energy: \\(E = \\frac{1}{2}mv^2\\)</p> <p>So velocity at bottom: \\(v_{max} = \\sqrt{\\frac{2E}{m}}\\)</p> <ul> <li>Heavy ball: \\(v_{max} = \\sqrt{\\frac{2E}{M}}\\) \u2014 moving slower</li> <li>Light ball: \\(v_{max} = \\sqrt{\\frac{2E}{m}}\\) \u2014 moving faster</li> </ul> <p>The lighter ball zips through the center faster!</p> <p>But momentum at bottom: \\(p_{max} = m \\cdot v_{max} = m \\cdot \\sqrt{\\frac{2E}{m}} = \\sqrt{2mE}\\)</p> <ul> <li>Heavy ball: \\(p_{max} = \\sqrt{2ME}\\) \u2014 larger momentum!</li> <li>Light ball: \\(p_{max} = \\sqrt{2mE}\\) \u2014 smaller momentum</li> </ul>"},{"location":"quantum-mechanics/quantum-mass-localization/#why-does-the-heavy-ball-have-more-momentum-despite-moving-slower","title":"Why Does the Heavy Ball Have More Momentum Despite Moving Slower?","text":"<p>The key: Momentum = mass \u00d7 velocity, but energy = mass \u00d7 velocity\u00b2</p> <p>Think of it this way: - To have energy E, you need: velocity = \\(\\sqrt{E/m}\\) - Your momentum is then: \\(p = m \\cdot \\sqrt{E/m} = \\sqrt{mE}\\)</p> <p>As mass increases: - Velocity decreases (as \\(1/\\sqrt{m}\\)) - But mass increases (as \\(m\\)) - Momentum = mass \u00d7 velocity increases (as \\(\\sqrt{m}\\))</p> <p>Intuition: Momentum is about \"how hard to stop.\" Energy is about \"how much work you did.\" A slow-moving truck has more momentum than a fast-moving bicycle even if you put the same amount of work into both!</p> <p>The trade-off: - Heavy: lots of mass, little velocity \u2192 high momentum - Light: little mass, lots of velocity \u2192 low momentum</p> <p>Mass wins because momentum is linear in mass but energy is quadratic in velocity.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#classical-spreads-summary","title":"Classical Spreads: Summary","text":"<p>During oscillation, each ball's position ranges from bottom to \\(h_{max}\\), and momentum ranges from \\(p_{max}\\) to 0:</p> Position Spread (\u0394h) Momentum Spread (\u0394p) Heavy ball Small (E/Mg) Large (\\(\\sqrt{2ME}\\)) Light ball Large (E/mg) Small (\\(\\sqrt{2mE}\\)) <p>Classically: heavy particles stay in a smaller region but carry more momentum!</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#forces-and-energy-the-bridge-to-quantum","title":"Forces and Energy (The Bridge to Quantum)","text":"<p>In classical mechanics, force relates to potential energy:</p> \\[F = -\\frac{dU}{dx}\\] <p>Force is the negative gradient (slope) of energy! At the bottom of the valley: - Large slope \u2192 large restoring force pushing ball back - This is what determines the oscillation</p> <p>Why does this matter for quantum? In quantum mechanics, there are no forces - only the potential energy function U(x). The Schr\u00f6dinger equation uses U(x), not F. But they're related by F = -dU/dx, so the physics is the same: steep potentials (large -dU/dx) confine particles more.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#quantum-the-blur-of-all-classical-paths","title":"Quantum: The Blur of All Classical Paths","text":"<p>Now go quantum. The particle isn't rolling - it's a wavefunction that exists everywhere at once.</p> <p>Think of the wavefunction as a blur photograph of all possible classical trajectories: - Some parts correspond to \"classical ball near the edge\" (high position, low momentum) - Some parts correspond to \"classical ball at the bottom\" (center position, high momentum) - The quantum state contains the full range of classical possibilities</p> <p>The momentum content must span the classical range: - From ~0 (at the edges) to ~\\(p_{max} = \\sqrt{2mE}\\) (at the center) - Momentum spread: \\(\\Delta p \\sim \\sqrt{2mE}\\)</p> <p>Just like classical!</p> <p>Heavy particle: - Classical \\(p_{max}\\) is large \u2192 quantum \u0394p is large - Contains high-momentum components - High momentum \u2192 short wavelength: \\(\\lambda = h/p\\) - Short wavelengths are \"tight\" and don't spread out much - By uncertainty \\(\\Delta x \\cdot \\Delta p \\sim \\hbar\\): large \u0394p forces small \u0394x</p> <p>Light particle: - Classical \\(p_{max}\\) is small \u2192 quantum \u0394p is small - Contains only low-momentum components - Low momentum \u2192 long wavelength - Long wavelengths are inherently \"fuzzy\" - Small \u0394p allows large \u0394x</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-beautiful-inevitability","title":"The Beautiful Inevitability","text":"<p>Classically: Heavy ball doesn't roll as high (small \u0394h) but has more momentum (large \u0394p)</p> <p>Quantum: Same story! - Momentum spread set by classical physics: \\(\\Delta p \\sim \\sqrt{2mE}\\) (large for heavy) - de Broglie: \\(\\lambda = h/p\\) (short for heavy) - Uncertainty: \\(\\Delta x \\cdot \\Delta p \\sim \\hbar\\) (small \u0394x for heavy)</p> <p>Of course it can be no other way!</p> <p>The classical physics already told us: - Heavy stays in smaller region (small \u0394h) - Heavy has larger momentum swing (large \u0394p)</p> <p>Quantum mechanics respects this completely. The wavefunction for a heavy particle: - Has large momentum content (large \u0394p) because classical \\(p_{max}\\) is large - Must be localized (small \u0394x) by uncertainty principle - Lives near the center where large momenta \"belong\"</p> <p>The quantum blur preserves the classical wisdom: Momentum is understood the same way (quantity of motion, mass \u00d7 velocity), and for the same energy, heavy particles occupy less space and carry more momentum - both classically and quantum mechanically.</p> <p>It all makes perfect sense.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#part-10-summary-tables-and-quick-reference","title":"Part 10: Summary Tables and Quick Reference","text":""},{"location":"quantum-mechanics/quantum-mass-localization/#table-1-how-mass-affects-localization","title":"Table 1: How Mass Affects Localization","text":"Phenomenon Light Particle (small m) Heavy Particle (large m) Wavefunction curvature Low (gentle bending) High (sharp bending) Spatial extent Large (spread out) Small (localized) Ground state energy in box High (\"cramped\") Low (\"comfortable\") Wave packet spreading rate Fast Slow Velocity uncertainty High Low de Broglie wavelength Long (fuzzy) Short (sharp)"},{"location":"quantum-mechanics/quantum-mass-localization/#table-2-mass-dependence-in-key-formulas","title":"Table 2: Mass Dependence in Key Formulas","text":"Quantity Formula Mass Dependence Normalized curvature \\((d^2\\psi/dx^2)/\\psi = -(2m/\\hbar^2) \\cdot KE\\) \\(\\propto m\\) Particle in box energy \\(E = \\pi^2\\hbar^2/(2mL^2)\\) \\(\\propto 1/m\\) Harmonic oscillator width \\(\\sigma = \\sqrt{\\hbar/(2m\\omega)}\\) \\(\\propto 1/\\sqrt{m}\\) Wave packet spreading \\(d\\sigma/dt \\sim \\hbar/(2m\\sigma_0)\\) \\(\\propto 1/m\\) Velocity uncertainty \\(\\Delta v = \\Delta p/m \\geq \\hbar/(2m\\Delta x)\\) \\(\\propto 1/m\\) de Broglie wavelength \\(\\lambda = h/p = h/\\sqrt{2mE}\\) \\(\\propto 1/\\sqrt{m}\\)"},{"location":"quantum-mechanics/quantum-mass-localization/#table-3-concrete-numbers-electron-vs-proton","title":"Table 3: Concrete Numbers (Electron vs. Proton)","text":"<p>Conditions: Same energy E = 1 eV, same initial width \\(\\sigma_0 = 1\\) nm, same harmonic frequency \\(\\omega\\)</p> Property Electron Proton Ratio de Broglie wavelength 1.23 nm 0.029 nm 43:1 Ground state width (harmonic) \\(\\sigma\\) \\(\\sigma/43\\) 43:1 Time to double packet width t 1836t 1:1836 Velocity uncertainty (\\(\\Delta x\\)=1nm) 58,000 m/s 32 m/s 1836:1"},{"location":"quantum-mechanics/quantum-mass-localization/#table-4-which-approach-answers-which-question","title":"Table 4: Which Approach Answers Which Question?","text":"Question Best Approach Why do confined particles have different energies? Curvature argument (Part 3) How does the ground state depend on mass? Particle in box / Harmonic oscillator (Parts 4-5) How fast does a free particle spread? Wave packet analysis (Part 6) Why can't particles be perfectly localized? Heisenberg uncertainty (Part 7) Where does the uncertainty principle come from? Fourier analysis (Part 8) How does classical intuition connect to quantum? Classical-quantum bridge (Part 9)"},{"location":"quantum-mechanics/quantum-mass-localization/#table-5-domain-of-validity-summary","title":"Table 5: Domain of Validity Summary","text":"Equation/Concept Applies To Restrictions \\(\\lambda = h/p\\) Exact for plane waves; each momentum component in general states p must be well-defined for that component Time-independent Schr\u00f6dinger equation Stationary states (eigenstates) States with definite energy E Curvature relation Any stationary state, any potential At each point in space Particle in box solutions Confined particles with hard walls Boundary conditions \\(\\psi(0) = \\psi(L) = 0\\) Wave packet spreading formula Free particles (\\(V = 0\\)), time-dependent Initial Gaussian packet \\(v_{\\text{group}} = p/m\\) Free particles only (\\(V = 0\\)) Group velocity of plane wave components \\(\\Delta v = \\Delta p/m\\) Any quantum state Relates standard deviations via \\(\\hat{v} = \\hat{p}/m\\) Uncertainty principle ANY quantum state General, fundamental limit Fourier transform relation ANY quantum state Mathematical property of wavefunctions"},{"location":"quantum-mechanics/quantum-mass-localization/#table-6-what-different-velocities-mean","title":"Table 6: What Different \"Velocities\" Mean","text":"Term Definition When It Applies Physical Meaning Classical velocity \\(v = p/m\\) Single particle with definite p How fast particle moves Group velocity \\(v_g = d\\omega/dk = p/m\\) Free particle plane wave component Speed of wave packet envelope; equals p/m for free particles Phase velocity \\(v_{\\phi} = \\omega/k\\) Plane wave Speed of wave crests (not probability) Velocity uncertainty \\(\\Delta v = \\Delta p/m\\) Any quantum state Spread in velocity operator eigenvalues Expectation value \\(\\langle v \\rangle = \\langle p \\rangle/m\\) Any quantum state Average velocity of ensemble"},{"location":"quantum-mechanics/quantum-mass-localization/#part-11-the-deep-unity","title":"Part 11: The Deep Unity","text":""},{"location":"quantum-mechanics/quantum-mass-localization/#why-all-roads-lead-to-the-same-answer","title":"Why All Roads Lead to the Same Answer","text":"<p>We've attacked the localization question from five different angles. Every approach gives the same answer because they're all manifestations of one underlying truth:</p> <p>Mass determines how \"quantum\" a particle behaves.</p> <p>The quantum scale is set by Planck's constant \\(\\hbar\\). The combination \\(\\hbar/m\\) determines:</p> <ul> <li>How much the wavefunction spreads (\\(\\propto \\hbar/m\\))</li> <li>How uncertain the velocity is (\\(\\propto \\hbar/m\\))</li> <li>How fast quantum phases dephase (\\(\\propto \\hbar/m\\))</li> <li>How \"expensive\" in energy terms curvature is (\\(\\propto \\hbar^2/m\\))</li> </ul>"},{"location":"quantum-mechanics/quantum-mass-localization/#the-classical-limit","title":"The Classical Limit","text":"<p>For everyday objects (\\(m \\sim 1\\) kg), \\(\\hbar/m \\sim 10^{-34}\\) m\u00b2/s. Quantum effects are utterly negligible. A baseball has such enormous mass that its wavefunction is essentially a point\u2014it behaves classically.</p> <p>For electrons (\\(m \\sim 10^{-30}\\) kg), \\(\\hbar/m \\sim 10^{-4}\\) m\u00b2/s. Quantum effects dominate at atomic scales.</p> <p>The proton isn't \"classical\"\u2014it's still fully quantum mechanical. But it's 1836 times closer to classical behavior than the electron.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#final-intuition-check","title":"Final Intuition Check","text":"<p>Ask yourself: \"Why do more massive particles stay localized?\"</p> <p>One-sentence answer: Because mass suppresses velocity uncertainty\u2014for the same spatial confinement, heavy particles have less \"spread\" in their velocities, so they hold together instead of flying apart.</p> <ul> <li>From the Schr\u00f6dinger equation: Curvature \\(\\propto m\\), so heavy particles bend back quickly and don't extend far</li> <li>From the uncertainty principle: \\(\\Delta v = \\Delta p/m = \\hbar/(2m\\Delta x)\\), so large m means small \\(\\Delta v\\)</li> <li>From wave packet physics: Dispersion \\(\\propto 1/m\\), so heavy particle packets spread slowly</li> </ul> <p>All the same physics. All the same answer.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#why-this-matters","title":"Why This Matters","text":"<p>In atomic physics: Electrons orbit far from the nucleus (large wavefunctions). Protons and neutrons stay tightly packed in the nucleus. This is why atoms are mostly empty space!</p> <p>In chemistry: Electrons delocalize across molecules (enabling bonding and tunneling). Nuclei stay put. This separation of scales is the Born-Oppenheimer approximation\u2014the foundation of computational chemistry.</p> <p>In solid-state physics: Electrons in crystals are described by Bloch waves that extend over many atoms. The light electron mass is what makes metals metallic\u2014electrons can spread throughout the material and conduct electricity.</p>"},{"location":"quantum-mechanics/quantum-mass-localization/#final-notes-on-equation-validity","title":"Final Notes on Equation Validity","text":"<p>Throughout this document, we've been careful to specify when each equation applies. Here's a summary of the most commonly confused points:</p> <ol> <li>de Broglie wavelength \\(\\lambda = h/p\\):</li> <li>Exact for plane waves with definite momentum</li> <li>For wave packets: applies to each momentum component individually</li> <li>Works in any potential (free space or confined)</li> <li> <p>The momentum p may vary with position in non-uniform potentials</p> </li> <li> <p>Group velocity \\(v = p/m\\):</p> </li> <li>Only exact for free particles where \\(V = 0\\) and \\(\\omega = \\hbar k^2/(2m)\\)</li> <li>Important: Two different meanings!<ul> <li>Packet's group velocity: The speed at which the packet's peak moves = \\(p_0/m\\) for packet centered at \\(p_0\\)</li> <li>Component group velocities: Each Fourier component with momentum p propagates at \\(v(p) = p/m\\)</li> </ul> </li> <li>The packet spreads because different components have different group velocities (dispersion)</li> <li>In non-dispersive media (where all components have the same group velocity), packets don't spread</li> <li>In quantum mechanics for free particles, the medium IS dispersive because \\(v = p/m\\) depends on p</li> <li> <p>In general potentials, the relationship between group velocity and momentum is more complex</p> </li> <li> <p>Velocity uncertainty \\(\\Delta v = \\Delta p/m\\):</p> </li> <li>This relationship is general - applies to any quantum state, not just free particles</li> <li>Derived from the velocity operator \\(\\hat{v} = \\hat{p}/m\\)</li> <li>\\(\\Delta v\\) and \\(\\Delta p\\) are standard deviations: \\(\\Delta v = \\sqrt{\\langle v^2 \\rangle - \\langle v \\rangle^2}\\)</li> <li>For free particles, \\(\\Delta v\\) equals the spread in group velocities of the momentum components</li> <li> <p>For confined particles, \\(\\Delta v\\) still measures the velocity operator's uncertainty, but may not correspond to actual spatial motion</p> </li> <li> <p>Time-independent vs time-dependent Schr\u00f6dinger equation:</p> </li> <li>Time-independent: for stationary states with definite energy</li> <li>Time-dependent: for general evolving states</li> <li> <p>Stationary states are special solutions of the time-dependent equation</p> </li> <li> <p>Free space vs confined:</p> </li> <li>\"Free particle\" means \\(V = 0\\) everywhere</li> <li>\"Confined\" means there's a potential well or barriers</li> <li> <p>The curvature argument works in both cases\u2014it's the most general approach</p> </li> <li> <p>Plane waves vs wave packets:</p> </li> <li>Plane waves have definite wavelength, extend infinitely</li> <li>Wave packets are superpositions of plane waves, are localized</li> <li> <p>Real particles are always wave packets, never pure plane waves</p> </li> <li> <p>What \"momentum spread causes spreading\" really means:</p> </li> <li>A wave packet with momentum spread \\(\\Delta p\\) contains components with different momenta</li> <li>In free space, each component travels at group velocity \\(v = p/m\\)</li> <li>The range of group velocities is \\(\\Delta v = \\Delta p/m\\)</li> <li>Over time \\(t\\), the fastest and slowest components separate by distance \\(\\approx \\Delta v \\cdot t\\)</li> <li>This is the physical mechanism of wave packet spreading</li> <li> <p>Important distinction:</p> <ul> <li>The center of the packet moves at \\(v_{\\text{center}} = p_0/m\\) (where \\(p_0\\) is the average momentum)</li> <li>The width of the packet grows as \\(\\sim (\\Delta p/m) \\cdot t\\) because components separate</li> <li>A packet can move AND spread simultaneously\u2014these are independent phenomena</li> </ul> </li> <li> <p>Dispersion vs non-dispersion:</p> </li> <li>Dispersive: Group velocity depends on frequency/momentum \u2192 different components travel at different speeds \u2192 packets spread</li> <li>Non-dispersive: Group velocity same for all frequencies \u2192 all components travel together \u2192 packets maintain shape</li> <li>Quantum mechanics for free particles is dispersive because \\(v = p/m\\) depends on p</li> <li>This is why quantum wave packets inevitably spread in free space</li> </ol> <p>Understanding these distinctions will help you avoid confusion as you study quantum mechanics further!</p> <p>This page was written with the assistance of Claude (Anthropic).</p>"},{"location":"quantum-mechanics/quantum-multi-particle/","title":"Multi-Particle Quantum Mechanics","text":"<p>OUTDATED CONTENT</p> <p>This page contains draft/outdated material and is not currently maintained.</p> <p>Prerequisites: This builds on Quantum Foundations, Basis Representation, and Operators. Make sure you're comfortable with single-particle wavefunctions, superposition, and measurement before diving in!</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#from-one-to-two-extending-the-wavefunction","title":"From One to Two: Extending the Wavefunction","text":"<p>Everything we've learned so far described one particle. The wavefunction \\(\\psi(x,t)\\) told us the amplitude for finding that particle at position \\(x\\) at time \\(t\\).</p> <p>What happens with two particles?</p> <p>The wavefunction becomes a function of both positions:</p> \\[ \\psi(x_1, x_2, t) \\] <p>This is a fundamentally different beast! Here's what it means:</p> <ul> <li>\\(x_1\\) = position of particle 1</li> <li>\\(x_2\\) = position of particle 2</li> <li>\\(\\psi(x_1, x_2, t)\\) = amplitude for finding particle 1 at \\(x_1\\) AND particle 2 at \\(x_2\\) simultaneously</li> </ul>"},{"location":"quantum-mechanics/quantum-multi-particle/#probability-interpretation-joint-probability","title":"Probability Interpretation: Joint Probability","text":"\\[ P(x_1, x_2) = |\\psi(x_1, x_2)|^2 dx_1 dx_2 \\] <p>This is the probability of finding particle 1 at \\(x_1\\) AND particle 2 at \\(x_2\\) at the same time.</p> <p>Important: The probability at any single point is vanishingly small (technically zero for continuous distributions). In practice, you integrate over a region to find the probability that particle 1 is in some interval and particle 2 is in some interval.</p> <p>Analogy - Rolling Two Dice:</p> <p>Think of rolling a red die and a blue die. The outcome is a pair of numbers: (red=3, blue=5).</p> <p>One die: Probability \\(P(\\text{red}=3)\\) tells you about just the red die Two dice (joint): Probability \\(P(\\text{red}=3 \\text{ AND blue}=5)\\) tells you about both outcomes together</p> <p>For quantum mechanics:</p> <ul> <li>One particle: \\(|\\psi(x)|^2dx\\) = probability particle is at position \\(x\\)</li> <li>Two particles (joint): \\(|\\psi(x_1,x_2)|^2dx_1dx_2\\) = probability particle 1 at \\(x_1\\) AND particle 2 at \\(x_2\\)</li> </ul> <p>Key insight: This is a joint probability distribution. The wavefunction doesn't just tell you about each particle separately; it encodes correlations between them!</p> <p>Just like knowing the red die rolled 3 might affect what the blue die shows (if the dice are rigged together), knowing where particle 1 is can affect where particle 2 is likely to be!</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#the-dimension-explosion","title":"The Dimension Explosion","text":"<p>With one particle in 1D, \\(\\psi(x)\\) is a function of 1 variable. With two particles:</p> <p>\u2022 Two particles in 1D: \\(\\psi(x_1, x_2)\\) (function of 2 variables)</p> <p>\u2022 Two particles in 3D: \\(\\psi(\\vec{r}_1, \\vec{r}_2)\\) (function of 6 variables)</p> <p>\u2022 Three particles in 3D: \\(\\psi(\\vec{r}_1, \\vec{r}_2, \\vec{r}_3)\\) (function of 9 variables)</p> <p>The complexity grows explosively! This is why quantum chemistry is so computationally hard.</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#normalization-why-does-it-equal-1","title":"Normalization: Why Does It Equal 1?","text":"<p>One particle: The particle must be somewhere, so:</p> \\[ \\int_{-\\infty}^{\\infty} |\\psi(x)|^2 dx = 1 \\] <p>Add up probabilities over all possible positions \u2192 get 100%.</p> <p>Two particles: The system must exist in some configuration, so:</p> \\[ \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} |\\psi(x_1, x_2)|^2 dx_1 dx_2 = 1 \\] <p>What does this mean? The double integral sums over ALL possible configurations.</p> <p>A configuration is a pair of positions: (particle 1 here, particle 2 there).</p> <p>Concrete example: Imagine two particles constrained to a discrete grid with just 3 positions: x = 0, 1, 2.</p> <p>Possible configurations (9 total):</p> <ol> <li>Both at 0: \\((x_1=0, x_2=0)\\)</li> <li>Particle 1 at 0, particle 2 at 1: \\((x_1=0, x_2=1)\\)</li> <li>Particle 1 at 0, particle 2 at 2: \\((x_1=0, x_2=2)\\)</li> <li>Particle 1 at 1, particle 2 at 0: \\((x_1=1, x_2=0)\\)</li> <li>Both at 1: \\((x_1=1, x_2=1)\\)</li> <li>Particle 1 at 1, particle 2 at 2: \\((x_1=1, x_2=2)\\)</li> <li>Particle 1 at 2, particle 2 at 0: \\((x_1=2, x_2=0)\\)</li> <li>Particle 1 at 2, particle 2 at 1: \\((x_1=2, x_2=1)\\)</li> <li>Both at 2: \\((x_1=2, x_2=2)\\)</li> </ol> <p>Normalization says: Add probabilities for all 9 configurations \u2192 must equal 1 (100%).</p> \\[ \\sum_{x_1=0}^{2} \\sum_{x_2=0}^{2} |\\psi(x_1, x_2)|^2 = 1 \\] <p>The system is definitely in one of these 9 configurations!</p> <p>Continuous case: Same idea, but infinitely many positions. The double integral adds up probabilities for every pair \\((x_1, x_2)\\):</p> <ul> <li>Particle 1 anywhere from \\(-\\infty\\) to \\(+\\infty\\)</li> <li>Particle 2 anywhere from \\(-\\infty\\) to \\(+\\infty\\)</li> <li>Total configurations: infinitely many</li> </ul> <p>The system must be in some configuration, so total probability = 1.</p> <p>Key point: It's not \"particle 1 must be somewhere\" AND separately \"particle 2 must be somewhere.\" It's \"the two-particle system must be in some configuration\" (and a configuration is a pair \\((x_1, x_2)\\) specifying where both particles are simultaneously).</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#independent-particles-product-states","title":"Independent Particles: Product States","text":"<p>Let's start with the simplest case: two particles that don't interact.</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#the-setup","title":"The Setup","text":"<p>Suppose we have:</p> <ul> <li>Particle 1 in state \\(\\psi_a(x_1)\\)</li> <li>Particle 2 in state \\(\\psi_b(x_2)\\)</li> <li>No interaction between them</li> </ul> <p>The two-particle wavefunction is just the product:</p> \\[ \\psi(x_1, x_2) = \\psi_a(x_1) \\psi_b(x_2) \\] <p>This is called a separable or product state.</p> <p>Physical meaning: The particles are completely independent. The probability of finding particle 1 at \\(x_1\\) doesn't depend on where particle 2 is:</p> \\[ |\\psi(x_1, x_2)|^2 = |\\psi_a(x_1)|^2 \\cdot |\\psi_b(x_2)|^2 \\] <p>The joint probability factorizes (exactly like independent random variables in probability theory)!</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#schrodingers-equation-for-two-particles","title":"Schr\u00f6dinger's Equation for Two Particles","text":"<p>The Hamiltonian for two non-interacting particles is:</p> \\[ \\hat{H} = \\underbrace{\\hat{H}_1}_{\\text{particle 1}} + \\underbrace{\\hat{H}_2}_{\\text{particle 2}} \\] <p>where each \\(\\hat{H}_i = -\\frac{\\hbar^2}{2m_i}\\frac{\\partial^2}{\\partial x_i^2} + V_i(x_i)\\).</p> <p>For a product state \\(\\psi(x_1, x_2) = \\psi_a(x_1)\\psi_b(x_2)\\), let's apply the Hamiltonian:</p> \\[ \\hat{H}\\psi = \\hat{H}_1[\\psi_a(x_1)\\psi_b(x_2)] + \\hat{H}_2[\\psi_a(x_1)\\psi_b(x_2)] \\] <p>The key: \\(\\hat{H}_1\\) only acts on \\(x_1\\) (treats \\(\\psi_b(x_2)\\) as a constant), and \\(\\hat{H}_2\\) only acts on \\(x_2\\):</p> \\[ = \\psi_b(x_2) \\hat{H}_1\\psi_a(x_1) + \\psi_a(x_1) \\hat{H}_2\\psi_b(x_2) \\] <p>If each particle is in an energy eigenstate (\\(\\hat{H}_1\\psi_a = E_a\\psi_a\\) and \\(\\hat{H}_2\\psi_b = E_b\\psi_b\\)):</p> \\[ = E_a\\psi_a(x_1)\\psi_b(x_2) + E_b\\psi_a(x_1)\\psi_b(x_2) = (E_a + E_b)\\psi(x_1, x_2) \\] <p>Result: The product state is an energy eigenstate with total energy = sum of individual energies!</p> \\[ \\boxed{\\hat{H}\\psi = (E_a + E_b)\\psi} \\]"},{"location":"quantum-mechanics/quantum-multi-particle/#concrete-example-two-electrons-in-separate-boxes","title":"Concrete Example: Two Electrons in Separate Boxes","text":"<p>Consider two electrons, each in its own infinite potential well:</p> <ul> <li>Electron 1 in box of length \\(L_1 = 1\\) nm</li> <li>Electron 2 in box of length \\(L_2 = 2\\) nm</li> </ul> <p>Both in ground state (\\(n=1\\)):</p> \\[ E_1 = \\frac{\\pi^2\\hbar^2}{2m_e L_1^2} \\approx 0.38 \\text{ eV} \\] \\[ E_2 = \\frac{\\pi^2\\hbar^2}{2m_e L_2^2} \\approx 0.095 \\text{ eV} \\] <p>Total energy: \\(E_{total} = 0.38 + 0.095 = 0.475\\) eV</p> <p>Wavefunction:</p> \\[ \\psi(x_1, x_2) = \\sqrt{\\frac{2}{L_1}}\\sin\\left(\\frac{\\pi x_1}{L_1}\\right) \\cdot \\sqrt{\\frac{2}{L_2}}\\sin\\left(\\frac{\\pi x_2}{L_2}\\right) \\] <p>Measurement independence: If you measure the position of electron 1, it doesn't affect the probability distribution for electron 2. They're completely separate systems.</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#coupling-particles-interaction-potentials","title":"Coupling Particles: Interaction Potentials","text":"<p>Now let's make it interesting: what if the particles interact?</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#adding-interaction","title":"Adding Interaction","text":"<p>The Hamiltonian becomes:</p> \\[ \\hat{H} = \\hat{H}_1 + \\hat{H}_2 + \\underbrace{V(x_1, x_2)}_{\\text{interaction}} \\] <p>The interaction potential \\(V(x_1, x_2)\\) depends on both positions (this is what couples the particles!).</p> <p>Common examples:</p> <p>\u2022 Coulomb repulsion (two electrons): \\(V(x_1, x_2) = \\frac{ke^2}{|x_1 - x_2|}\\)</p> <p>\u2022 Coulomb attraction (electron-proton): \\(V(x_1, x_2) = -\\frac{ke^2}{|x_1 - x_2|}\\)</p> <p>\u2022 Spring coupling: \\(V(x_1, x_2) = \\frac{1}{2}k(x_1 - x_2)^2\\)</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#product-states-no-longer-work","title":"Product States No Longer Work!","text":"<p>Let's see what happens when we apply \\(\\hat{H}\\) to a product state:</p> \\[ \\hat{H}\\psi = (\\hat{H}_1 + \\hat{H}_2 + V(x_1,x_2))[\\psi_a(x_1)\\psi_b(x_2)] \\] <p>The first two terms still give us \\(E_a + E_b\\) times \\(\\psi\\), but the interaction term:</p> \\[ V(x_1, x_2)\\psi_a(x_1)\\psi_b(x_2) \\] <p>This doesn't factor! You can't write it as \\((E_a + E_b + \\text{something})\\psi\\).</p> <p>Result: Product states are not energy eigenstates when particles interact!</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#operators-for-two-particle-systems","title":"Operators for Two-Particle Systems","text":"<p>Before we calculate energies, let's understand how operators work with two particles. This is crucial for understanding expectation values!</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#single-particle-operators-acting-on-one-particle-at-a-time","title":"Single-Particle Operators: Acting on One Particle at a Time","text":"<p>For two particles, you have separate operators for each particle:</p> <p>Momentum operators:</p> <p>\\(\\hat{p}_1 = -i\\hbar\\frac{\\partial}{\\partial x_1}\\) (acts only on particle 1's position) \\(\\hat{p}_2 = -i\\hbar\\frac{\\partial}{\\partial x_2}\\) (acts only on particle 2's position)</p> <p>Position operators:</p> <p>\\(\\hat{x}_1\\) (multiplies by \\(x_1\\)) \\(\\hat{x}_2\\) (multiplies by \\(x_2\\))</p> <p>Key point: Each operator acts on its own variable only. When \\(\\hat{p}_1\\) takes a derivative, it treats \\(x_2\\) as a constant!</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#example-momentum-of-particle-1","title":"Example: Momentum of Particle 1","text":"<p>Apply \\(\\hat{p}_1\\) to a two-particle wavefunction:</p> \\[ \\hat{p}_1 \\psi(x_1, x_2) = -i\\hbar\\frac{\\partial}{\\partial x_1}\\psi(x_1, x_2) \\] <p>For a product state \\(\\psi(x_1, x_2) = \\psi_a(x_1)\\psi_b(x_2)\\):</p> \\[ \\hat{p}_1[\\psi_a(x_1)\\psi_b(x_2)] = \\psi_b(x_2) \\cdot \\left(-i\\hbar\\frac{\\partial\\psi_a}{\\partial x_1}\\right) = \\psi_b(x_2) \\cdot \\hat{p}_1\\psi_a(x_1) \\] <p>The operator \"sees\" \\(\\psi_b(x_2)\\) as just a constant (doesn't depend on \\(x_1\\)), so it factors out!</p> <p>Expected value of particle 1's momentum:</p> \\[ \\langle p_1 \\rangle = \\int\\int \\psi^* \\hat{p}_1 \\psi \\, dx_1 dx_2 \\] <p>This gives you the average momentum of just particle 1, regardless of what particle 2 is doing.</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#why-does-potential-energy-depend-on-both-particles","title":"Why Does Potential Energy Depend on Both Particles?","text":"<p>Here's the key difference between kinetic and potential energy:</p> <p>Kinetic energy (independent):</p> <p>Particle 1's kinetic energy: \\(\\frac{\\hat{p}_1^2}{2m_1}\\) (depends only on \\(x_1\\)) Particle 2's kinetic energy: \\(\\frac{\\hat{p}_2^2}{2m_2}\\) (depends only on \\(x_2\\)) Total: \\(\\hat{T} = \\frac{\\hat{p}_1^2}{2m_1} + \\frac{\\hat{p}_2^2}{2m_2}\\)</p> <p>Potential energy (can couple!):</p> <p>Coulomb repulsion: \\(V(x_1, x_2) = \\frac{ke^2}{|x_1 - x_2|}\\) (depends on both positions!) The interaction energy depends on the distance between particles</p> <p>Physical meaning:</p> <ul> <li>Kinetic energy is \"local\" (each particle has its own motion)</li> <li>Interaction energy is \"nonlocal\" (depends on relative positions)</li> </ul> <p>This is why \\(V(x_1, x_2)\\) has both variables (the repulsion depends on how far apart the electrons are!).</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#concrete-example-two-electrons-in-one-box","title":"Concrete Example: Two Electrons in One Box","text":"<p>Consider two electrons in the same box of length \\(L = 1\\) nm, with Coulomb repulsion between them.</p> <p>Without interaction (pretending they don't repel): \u2022 Both in ground state: \\(E = 2E_1 = 2(0.38) = 0.76\\) eV</p> <p>With Coulomb repulsion:</p> <p>The interaction raises the energy. How much? We need to calculate the expected value (average value) of the repulsion energy.</p> <p>Connection to Operators section: Remember from Operators and Measurement that the expected value of an operator \\(\\hat{A}\\) is:</p> \\[ \\langle A \\rangle = \\langle\\psi|\\hat{A}|\\psi\\rangle \\] <p>This is the average value you'd get if you measured \\(A\\) many times on identically prepared systems.</p> <p>For potential energy: The potential \\(V(x_1, x_2)\\) is just a function (not a derivative), so the operator \\(\\hat{V}\\) acts by multiplication:</p> \\[ \\hat{V}\\psi(x_1, x_2) = V(x_1, x_2) \\psi(x_1, x_2) \\] <p>In position representation: The expectation value \\(\\langle\\psi|\\hat{V}|\\psi\\rangle\\) becomes an integral:</p> \\[ \\langle V \\rangle = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\psi^*(x_1, x_2) \\cdot V(x_1, x_2) \\cdot \\psi(x_1, x_2) \\, dx_1 dx_2 \\] <p>For real wavefunctions (like particle in a box), \\(\\psi^* = \\psi\\), so \\(\\psi^* \\psi = |\\psi|^2\\):</p> \\[ \\langle V \\rangle = \\int \\int |\\psi(x_1, x_2)|^2 V(x_1, x_2) dx_1 dx_2 \\] <p>Physical meaning: Weight the potential energy \\(V(x_1, x_2)\\) by the probability \\(|\\psi(x_1, x_2)|^2\\) of finding the electrons at those positions, then average over all configurations.</p> <p>It's the same expectation value formula, just extended to two particles!</p> <p>For our example: Both electrons in ground state \\(\\psi_1(x)\\), so \\(\\psi(x_1, x_2) = \\psi_1(x_1)\\psi_1(x_2)\\):</p> \\[ \\langle V \\rangle = \\int_0^L \\int_0^L |\\psi_1(x_1)|^2 |\\psi_1(x_2)|^2 \\frac{ke^2}{|x_1 - x_2|} dx_1 dx_2 \\] <p>For our 1 nm box with both electrons in ground state:</p> \\[ \\langle V \\rangle \\approx 1.4 \\text{ eV} \\] <p>New ground state energy: \\(E \\approx 0.76 + 1.4 = 2.2\\) eV</p> <p>The interaction adds significant energy! And the true ground state wavefunction is not simply \\(\\psi_1(x_1)\\psi_1(x_2)\\) (it gets distorted by the repulsion).</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#entanglement-emerges-non-separable-states","title":"Entanglement Emerges: Non-Separable States","text":"<p>When particles interact, something profound happens: the wavefunction becomes non-separable.</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#what-is-a-non-separable-state","title":"What Is a Non-Separable State?","text":"<p>A state \\(\\psi(x_1, x_2)\\) is separable if you can write it as:</p> \\[ \\psi(x_1, x_2) = \\psi_a(x_1) \\psi_b(x_2) \\] <p>for some single-particle states \\(\\psi_a\\) and \\(\\psi_b\\).</p> <p>If you can't write it this way, the state is entangled (or non-separable).</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#a-simple-entangled-state","title":"A Simple Entangled State","text":"<p>Consider this superposition:</p> \\[ \\psi(x_1, x_2) = \\frac{1}{\\sqrt{2}}\\left[\\psi_a(x_1)\\psi_b(x_2) + \\psi_b(x_1)\\psi_a(x_2)\\right] \\] <p>Try to factor it: Can we write this as \\(f(x_1) \\cdot g(x_2)\\) for any functions \\(f\\) and \\(g\\)?</p> <p>No! The two terms are \"mixed\" (you can't separate \\(x_1\\) from \\(x_2\\)).</p> <p>Physical interpretation:</p> <ul> <li>Either \"particle 1 in state \\(a\\) AND particle 2 in state \\(b\\)\"</li> <li>Or \"particle 1 in state \\(b\\) AND particle 2 in state \\(a\\)\"</li> </ul> <p>The particles are correlated. You can't describe them independently!</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#conditional-probabilities-the-key-to-entanglement","title":"Conditional Probabilities: The Key to Entanglement","text":"<p>Here's what makes entanglement weird. For the entangled state:</p> \\[ \\psi(x_1, x_2) = \\frac{1}{\\sqrt{2}}[\\psi_a(x_1)\\psi_b(x_2) + \\psi_b(x_1)\\psi_a(x_2)] \\] <p>Let's ask: \"What's the probability distribution for particle 2's position?\"</p> <p>It depends on what you measure for particle 1!</p> <p>Before measuring particle 1: Integrate over \\(x_1\\) to get the marginal probability for particle 2:</p> \\[ P(x_2) = \\int |\\psi(x_1, x_2)|^2 dx_1 \\] <p>Expanding \\(|\\psi|^2\\):</p> \\[ |\\psi(x_1, x_2)|^2 = \\frac{1}{2}\\left[|\\psi_a(x_1)|^2|\\psi_b(x_2)|^2 + |\\psi_b(x_1)|^2|\\psi_a(x_2)|^2 + 2\\text{Re}(\\psi_a^*(x_1)\\psi_b^*(x_2)\\psi_b(x_1)\\psi_a(x_2))\\right] \\] <p>Integrating over \\(x_1\\) (using normalization \\(\\int |\\psi_a|^2 dx_1 = \\int |\\psi_b|^2 dx_1 = 1\\)):</p> \\[ P(x_2) = \\frac{1}{2}\\left[|\\psi_b(x_2)|^2 + |\\psi_a(x_2)|^2\\right] + \\text{cross terms} \\] <p>What are the cross terms? They come from the interference term in \\(|\\psi|^2\\):</p> \\[ \\text{cross terms} = \\text{Re}\\int \\psi_a^*(x_1)\\psi_b^*(x_2)\\psi_b(x_1)\\psi_a(x_2) dx_1 \\] <p>For real wavefunctions, this becomes:</p> \\[ = \\int \\psi_a(x_1)\\psi_b(x_1) dx_1 \\cdot \\psi_a(x_2)\\psi_b(x_2) \\] <p>This is the overlap integral \\(\\langle\\psi_a|\\psi_b\\rangle\\) times the product \\(\\psi_a(x_2)\\psi_b(x_2)\\). If states \\(a\\) and \\(b\\) are orthogonal (no overlap), the cross terms vanish! Otherwise, you get quantum interference that affects particle 2's probability distribution.</p> <p>Particle 2 has contributions from both states \\(a\\) and \\(b\\), plus interference from the cross terms!</p> <p>After measuring particle 1 at position \\(x_1^0\\): The wavefunction \"collapses\" and particle 2's distribution becomes:</p> \\[ P(x_2 | x_1 = x_1^0) \\propto |\\psi(x_1^0, x_2)|^2 = \\frac{1}{2}\\left|\\psi_a(x_1^0)\\psi_b(x_2) + \\psi_b(x_1^0)\\psi_a(x_2)\\right|^2 \\] <p>This depends on the specific value \\(x_1^0\\) where you found particle 1!</p> <p>Let's be crystal clear about what this means:</p> <ul> <li> <p>If you DON'T measure particle 1 (no knowledge about its position): Particle 2's probability is \\(P(x_2) = \\frac{1}{2}[|\\psi_b(x_2)|^2 + |\\psi_a(x_2)|^2] + \\text{cross terms}\\). This is the marginal probability (averaging over all possible particle 1 positions).</p> </li> <li> <p>If you DO measure particle 1 and find it at \\(x_1^0\\): Particle 2's probability changes to \\(P(x_2|x_1=x_1^0)\\), which depends on where you found particle 1! This is the conditional probability.</p> </li> </ul> <p>The measurement of particle 1 gives you new information that changes your prediction for particle 2. That's entanglement!</p> <p>Example: Suppose \\(\\psi_a\\) is localized on the left and \\(\\psi_b\\) is localized on the right.</p> <ul> <li> <p>If you find particle 1 on the left (where \\(|\\psi_a(x_1^0)|^2 \\gg |\\psi_b(x_1^0)|^2\\)), then \\(\\psi_a(x_1^0)\\psi_b(x_2)\\) dominates \u2192 particle 2 is likely on the right!</p> </li> <li> <p>If you find particle 1 on the right (where \\(|\\psi_b(x_1^0)|^2 \\gg |\\psi_a(x_1^0)|^2\\)), then \\(\\psi_b(x_1^0)\\psi_a(x_2)\\) dominates \u2192 particle 2 is likely on the left!</p> </li> </ul> <p>Key insight: Measuring particle 1 instantly changes the probability distribution for particle 2, even if they're far apart!</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#concrete-example-which-box","title":"Concrete Example: Which Box?","text":"<p>Imagine two boxes separated by a large distance. You have one particle and you prepare the state:</p> \\[ \\psi(x_1, x_2) = \\frac{1}{\\sqrt{2}}[\\psi_{\\text{left}}(x_1)\\psi_{\\text{right}}(x_2) + \\psi_{\\text{right}}(x_1)\\psi_{\\text{left}}(x_2)] \\] <p>Before measurement: Each particle has 50% chance of being in the left box, 50% in the right box.</p> <p>Measure particle 1 \u2192 find it in the left box:</p> <p>Instantly, particle 2 is definitely in the right box! The measurement of particle 1 \"collapsed\" particle 2's state.</p> <p>This is entanglement. Einstein called it \"spooky action at a distance.\"</p> <p>Note: We'll introduce spin (an important quantum property) later when we need it for understanding Bell's theorem and EPR paradox. For now, let's continue with spatial wavefunctions only!</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#identical-particles-symmetric-and-antisymmetric-wavefunctions","title":"Identical Particles: Symmetric and Antisymmetric Wavefunctions","text":"<p>Now for something truly fundamental about quantum mechanics: identical particles are indistinguishable.</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#the-problem-with-distinguishable-particles","title":"The Problem with Distinguishable Particles","text":"<p>Classically, even if two electrons are identical, you can \"paint one red\" (track it over time). You always know which is which.</p> <p>Quantum mechanically, this is impossible! If you have two electrons, there's no way to tell \"electron 1\" from \"electron 2.\" They're fundamentally indistinguishable.</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#what-indistinguishable-means","title":"What \"Indistinguishable\" Means","text":"<p>If you exchange the two particles (swap labels \\(1 \\leftrightarrow 2\\)), physics shouldn't change. Observable quantities like \\(|\\psi|^2\\) must stay the same:</p> \\[ |\\psi(x_2, x_1)|^2 = |\\psi(x_1, x_2)|^2 \\] <p>This means the wavefunctions can only differ by a phase: \u03c8(x\u2082, x\u2081) = e^(i\u03c6)\u03c8(x\u2081, x\u2082) for some phase \u03c6.</p> <p>Let's figure out what \u03c6 must be. Exchange twice gets you back to the original state. Let's do this step by step:</p> <p>Step 1: Start with the original state \u03c8(x\u2081, x\u2082)</p> <p>Step 2: Exchange once (swap 1 \u2194 2). This gives us \u03c8(x\u2082, x\u2081), which by the equation above equals:</p> \\[ \\psi(x_2, x_1) = e^{i\\phi}\\psi(x_1, x_2) \\] <p>Step 3: Exchange again (swap 1 \u2194 2 again). Starting from \u03c8(x\u2082, x\u2081), swapping gives us \u03c8(x\u2081, x\u2082). But we can also apply our exchange rule to \u03c8(x\u2082, x\u2081):</p> \\[ \\text{Exchanging } \\psi(x_2, x_1) \\text{ gives: } e^{i\\phi}\\psi(x_2, x_1) \\] <p>Step 4: But we know from Step 2 that \u03c8(x\u2082, x\u2081) = e^(i\u03c6)\u03c8(x\u2081, x\u2082), so substitute:</p> \\[ e^{i\\phi}\\psi(x_2, x_1) = e^{i\\phi} \\cdot e^{i\\phi}\\psi(x_1, x_2) = e^{2i\\phi}\\psi(x_1, x_2) \\] <p>Step 5: But exchanging twice must give back the original state \u03c8(x\u2081, x\u2082)! So:</p> \\[ \\psi(x_1, x_2) = e^{2i\\phi}\\psi(x_1, x_2) \\] <p>This is only true if \\(e^{2i\\phi} = 1\\), which means \\(\\phi = 0\\) or \\(\\phi = \\pi\\).</p> <p>Result: Only two possibilities!</p> \\[ \\psi(x_2, x_1) = +\\psi(x_1, x_2) \\quad \\text{(symmetric)} \\] \\[ \\psi(x_2, x_1) = -\\psi(x_1, x_2) \\quad \\text{(antisymmetric)} \\]"},{"location":"quantum-mechanics/quantum-multi-particle/#fermions-vs-bosons","title":"Fermions vs Bosons","text":"<p>Nature uses both:</p> <p>Fermions (antisymmetric): \u2022 Electrons, protons, neutrons \u2022 All spin-1/2 particles \u2022 More generally: spin = 1/2, 3/2, 5/2, ... (half-integer)</p> <p>Bosons (symmetric): \u2022 Photons, gluons \u2022 Helium-4 atoms \u2022 More generally: spin = 0, 1, 2, ... (integer)</p> <p>Spin-statistics theorem: The connection between spin and symmetry is deep (it comes from combining quantum mechanics with special relativity).</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#constructing-an-antisymmetric-wavefunction","title":"Constructing an Antisymmetric Wavefunction","text":"<p>Suppose we want to put two electrons in states \\(\\psi_a(x)\\) and \\(\\psi_b(x)\\).</p> <p>Wrong: \\(\\psi(x_1, x_2) = \\psi_a(x_1)\\psi_b(x_2)\\) (not antisymmetric!)</p> <p>Right: We need to antisymmetrize:</p> \\[ \\psi(x_1, x_2) = \\frac{1}{\\sqrt{2}}\\left[\\psi_a(x_1)\\psi_b(x_2) - \\psi_a(x_2)\\psi_b(x_1)\\right] \\] <p>Check exchange symmetry:</p> \\[ \\psi(x_2, x_1) = \\frac{1}{\\sqrt{2}}\\left[\\psi_a(x_2)\\psi_b(x_1) - \\psi_a(x_1)\\psi_b(x_2)\\right] = -\\psi(x_1, x_2) \\quad \\checkmark \\] <p>Perfect! The \\(1/\\sqrt{2}\\) ensures normalization.</p> <p>Physical meaning: The wavefunction is a superposition:</p> <ul> <li>\"Electron 1 in state \\(a\\), electron 2 in state \\(b\\)\"</li> <li>minus \"Electron 1 in state \\(b\\), electron 2 in state \\(a\\)\"</li> </ul> <p>But remember: these labels are arbitrary! The particles are truly indistinguishable.</p>"},{"location":"quantum-mechanics/quantum-multi-particle/#slater-determinant-notation","title":"Slater Determinant Notation","text":"<p>For two particles, we can write the antisymmetric state as a determinant:</p> \\[ \\psi(x_1, x_2) = \\frac{1}{\\sqrt{2}}\\begin{vmatrix} \\psi_a(x_1) &amp; \\psi_b(x_1) \\\\ \\psi_a(x_2) &amp; \\psi_b(x_2) \\end{vmatrix} = \\frac{1}{\\sqrt{2}}[\\psi_a(x_1)\\psi_b(x_2) - \\psi_a(x_2)\\psi_b(x_1)] \\] <p>This generalizes beautifully to N particles (just use an N\u00d7N determinant, called a Slater determinant).</p> <p>Next: Continue to The Pauli Exclusion Principle to see the profound consequences of antisymmetry for atomic structure and the periodic table!</p>"},{"location":"quantum-mechanics/quantum-operators/","title":"Quantum Operators and Measurement","text":""},{"location":"quantum-mechanics/quantum-operators/#the-momentum-operator-and-plane-waves","title":"The Momentum Operator and Plane Waves","text":"<p>Let's start where we left off in Foundations: plane waves. A plane wave solution to Schr\u00f6dinger's equation with definite momentum \\(p = \\hbar k\\) and energy \\(E = \\hbar\\omega\\) looks like:</p> \\[ \\psi(x,t) = A e^{i(kx - \\omega t)} \\] <p>We can use the exponential property to write this as a product of spatial and temporal parts:</p> \\[ \\psi(x,t) = \\underbrace{A e^{ikx}}_{\\text{spatial part}} \\cdot \\underbrace{e^{-i\\omega t}}_{\\text{time part}} \\] <p>This separation is crucial! The spatial part encodes momentum information, while the time part encodes energy information. They're independent \u2014 a key feature of stationary states.</p> <p>What's a stationary state? A stationary state is a quantum state with definite energy (an energy eigenstate). The \"stationary\" name comes from the fact that all measurable properties (like probability density \\(|\\psi|^2\\)) are constant in time. The wavefunction oscillates as \\(e^{-iEt/\\hbar}\\), but this is just a global phase that doesn't affect any measurements.</p> <p>Stationary states arise when the system's Hamiltonian doesn't depend on time \u2014 meaning all potentials are time-independent: \\(V(x)\\) not \\(V(x,t)\\). This includes free particles (no potential), particles in static external fields, and even interacting particles (like two electrons) as long as all interactions are time-independent (e.g., Coulomb repulsion between electrons doesn't change with time). Plane waves are examples of stationary states for free particles.</p> <p>Looking ahead: Later we'll see that taking the time derivative \u2202\u03c8/\u2202t extracts energy from the time part, just like the spatial derivative extracts momentum! But for now, let's focus on momentum.</p>"},{"location":"quantum-mechanics/quantum-operators/#extracting-momentum-the-spatial-derivative","title":"Extracting Momentum: The Spatial Derivative","text":"<p>Focus on the spatial part:</p> \\[ \\psi(x) = A e^{ikx} \\] <p>This is what we derived from de Broglie's relation. Now let's ask: what happens when we take the spatial derivative?</p> <p>Key property of exponentials: Taking the derivative with respect to \\(x\\) brings down the exponent:</p> \\[ \\frac{d}{dx}e^{ikx} = (ik) \\cdot e^{ikx} \\] <p>The factor \\(ik\\) (whatever was multiplying \\(x\\) in the exponent) gets pulled out front. This simple property is what makes the momentum operator work!</p> <p>Applying this to our plane wave:</p> \\[ \\frac{d\\psi}{dx} = A(ik)e^{ikx} = ik\\psi = \\frac{i}{\\hbar}p\\psi \\] <p>Multiply both sides by \\(-i\\hbar\\):</p> \\[ -i\\hbar\\frac{d\\psi}{dx} = p\\psi \\] <p>This is huge! Look at what just happened:</p> <p>\u2022 We applied the mathematical operation \\(-i\\hbar\\frac{d}{dx}\\) to the wavefunction \u2022 We got back the momentum \\(p\\) times the wavefunction</p> <p>We call this operation \"applying the momentum operator\" and give it the symbol \\(\\hat{p}\\). So we can write:</p> \\[ \\hat{p}\\psi(x) = -i\\hbar\\frac{d\\psi}{dx} = p\\psi \\] <p>Important distinction: This form \\(-i\\hbar\\frac{d}{dx}\\) is specific to position representation (when the wavefunction is written as \\(\\psi(x)\\)). It tells you how the momentum operator acts on functions of \\(x\\).</p> <p>More generally, without referring to a specific representation, we write this in abstract notation (ket notation):</p> \\[ \\hat{p}|\\psi\\rangle = p|\\psi\\rangle \\] <p>This says the same physics \u2014 \"applying the momentum operator gives back the state multiplied by the momentum value \\(p\\)\" \u2014 but now without specifying how you calculate it.</p> <p>Important: This equation \\(\\hat{p}|\\psi\\rangle = p|\\psi\\rangle\\) is NOT true for all states! It only holds for special wavefunctions called momentum eigenstates (like plane waves). For general superpositions, applying \\(\\hat{p}\\) gives a more complicated result \u2014 we'll see examples of this below.</p> <p>The key point: \\(\\hat{p}\\) is the operator itself (abstract concept), while \\(-i\\hbar\\frac{d}{dx}\\) is how that operator acts when you're working with wavefunctions in position space \\(\\psi(x)\\).</p> <p>Connection to real measurements: In the lab, one straightforward way to measure momentum is to measure wavelength \\(\\lambda\\) (via diffraction or interference patterns). For a free particle, de Broglie tells us \\(p = h/\\lambda = \\hbar k\\) where \\(k = 2\\pi/\\lambda\\). So wavelength describes \"how rapidly the wave oscillates in space,\" which is exactly what the spatial derivative \\(d/dx\\) captures. The operator \\(-i\\hbar\\frac{d}{dx}\\) mathematically packages up this measurement process: different wavelengths \u2192 different \\(k\\) values \u2192 different rates of spatial oscillation \u2192 different values when you take \\(d/dx\\).</p> <p>When a state satisfies this equation, we say it's a momentum eigenstate with eigenvalue \\(p\\).</p>"},{"location":"quantum-mechanics/quantum-operators/#example-1-single-plane-wave-eigenstate","title":"Example 1: Single Plane Wave (Eigenstate)","text":"<p>Consider a particle in a single plane wave state:</p> \\[ \\psi(x) = A e^{ikx} \\quad \\text{where } k = 5\\text{ nm}^{-1} \\] <p>Apply the momentum operator:</p> \\[ \\hat{p}\\psi = -i\\hbar\\frac{d}{dx}\\left(Ae^{ikx}\\right) = -i\\hbar(ik)Ae^{ikx} = \\hbar k\\psi = p\\psi \\] <p>We get back the same wavefunction multiplied by a number (\\(p = \\hbar k\\)).</p> <p>Physical meaning: If you measure momentum, you get \\(p = 5\\hbar\\) nm\u207b\u00b9 with 100% certainty. Every single measurement gives the same result. The particle has definite momentum.</p>"},{"location":"quantum-mechanics/quantum-operators/#example-2-superposition-of-two-plane-waves-not-an-eigenstate","title":"Example 2: Superposition of Two Plane Waves (NOT an Eigenstate)","text":"<p>Now consider a particle in a superposition of two plane waves:</p> \\[ \\psi(x) = \\frac{1}{\\sqrt{2}}\\left(e^{ik_1 x} + e^{ik_2 x}\\right) \\] <p>Let's say \\(k_1 = 3\\) nm\u207b\u00b9 and \\(k_2 = 7\\) nm\u207b\u00b9. Apply the momentum operator:</p> \\[ \\hat{p}\\psi = -i\\hbar\\frac{d}{dx}\\left[\\frac{1}{\\sqrt{2}}\\left(e^{ik_1 x} + e^{ik_2 x}\\right)\\right] \\] \\[ = \\frac{1}{\\sqrt{2}}\\left(\\hbar k_1 e^{ik_1 x} + \\hbar k_2 e^{ik_2 x}\\right) \\] \\[ = \\frac{1}{\\sqrt{2}}\\left(p_1 e^{ik_1 x} + p_2 e^{ik_2 x}\\right) \\] <p>This is NOT equal to \\(p\\psi\\) for any single number \\(p\\)! We don't get the same wavefunction back.</p> <p>Physical meaning: This state is not a momentum eigenstate. If you measure momentum, you get:</p> <p>\u2022 \\(p_1 = 3\\hbar\\) nm\u207b\u00b9 with probability \\(|\\frac{1}{\\sqrt{2}}|^2 = 50\\%\\) \u2022 \\(p_2 = 7\\hbar\\) nm\u207b\u00b9 with probability \\(|\\frac{1}{\\sqrt{2}}|^2 = 50\\%\\)</p> <p>The particle does not have a definite momentum! This is why quantum measurements are probabilistic.</p>"},{"location":"quantum-mechanics/quantum-operators/#another-superposition-unequal-amplitudes","title":"Another Superposition: Unequal Amplitudes","text":"<p>Let's make it more interesting with unequal amplitudes:</p> \\[ \\psi(x) = \\frac{1}{2}e^{ik_1 x} + \\frac{\\sqrt{3}}{2}e^{ik_2 x} \\] <p>Now the probabilities are:</p> <p>\u2022 \\(p_1 = k_1\\hbar\\) with probability \\(|\\frac{1}{2}|^2 = 25\\%\\) \u2022 \\(p_2 = k_2\\hbar\\) with probability \\(|\\frac{\\sqrt{3}}{2}|^2 = 75\\%\\)</p> <p>The key insight: When \\(\\psi\\) is a superposition, applying \\(\\hat{p}\\) gives different eigenvalues multiplied by their respective components. The measurement outcome is probabilistic, with probabilities given by \\(|c_n|^2\\) where \\(c_n\\) is the coefficient of each momentum eigenstate.</p>"},{"location":"quantum-mechanics/quantum-operators/#expected-value-the-average-measurement","title":"Expected Value: The Average Measurement","text":"<p>What if we repeat the momentum measurement many times on identically prepared systems? We get an average or expected value.</p> <p>For the unequal superposition above:</p> \\[ \\langle p \\rangle = 25\\% \\cdot p_1 + 75\\% \\cdot p_2 = \\frac{1}{4}(k_1\\hbar) + \\frac{3}{4}(k_2\\hbar) \\] <p>There's a beautiful mathematical formula for this. For a superposition \\(\\psi = c_1\\psi_1 + c_2\\psi_2\\):</p> \\[ \\langle p \\rangle = \\langle\\psi|\\hat{p}|\\psi\\rangle = (c_1^*\\langle\\psi_1| + c_2^*\\langle\\psi_2|) \\hat{p} (c_1|\\psi_1\\rangle + c_2|\\psi_2\\rangle) \\] <p>Expanding this out gives four terms:</p> \\[ \\langle p \\rangle = |c_1|^2 \\langle\\psi_1|\\hat{p}|\\psi_1\\rangle + |c_2|^2 \\langle\\psi_2|\\hat{p}|\\psi_2\\rangle + c_1^*c_2 \\langle\\psi_1|\\hat{p}|\\psi_2\\rangle + c_2^*c_1 \\langle\\psi_2|\\hat{p}|\\psi_1\\rangle \\] <p>The last two terms are cross terms (mixing different states). Here's the key: when \\(\\psi_1\\) and \\(\\psi_2\\) are momentum eigenstates, these cross terms vanish:</p> \\[ \\langle\\psi_1|\\hat{p}|\\psi_2\\rangle = \\langle\\psi_1|p_2|\\psi_2\\rangle = p_2\\langle\\psi_1|\\psi_2\\rangle = 0 \\] <p>The eigenvalue \\(p_2\\) comes out, leaving just the overlap \\(\\langle\\psi_1|\\psi_2\\rangle = 0\\) (eigenstates are orthogonal).</p> <p>Let's see this explicitly: For plane waves \\(\\psi_1 = e^{ik_1 x}\\) and \\(\\psi_2 = e^{ik_2 x}\\):</p> \\[ \\langle\\psi_1|\\hat{p}|\\psi_2\\rangle = \\int_{-\\infty}^{\\infty} e^{-ik_1 x} \\cdot \\left(-i\\hbar\\frac{d}{dx}e^{ik_2 x}\\right) dx = -i\\hbar(ik_2) \\int_{-\\infty}^{\\infty} e^{-ik_1 x} e^{ik_2 x} dx \\] \\[ = \\hbar k_2 \\int_{-\\infty}^{\\infty} e^{i(k_2-k_1)x} dx = 0 \\quad \\text{(for } k_1 \\neq k_2\\text{)} \\] <p>The integral vanishes because plane waves with different wavenumbers are orthogonal. So we get:</p> \\[ \\langle p \\rangle = |c_1|^2 p_1 + |c_2|^2 p_2 \\] <p>In position space, this becomes:</p> \\[ \\langle p \\rangle = \\int_{-\\infty}^{\\infty} \\psi^*(x)\\left(-i\\hbar\\frac{d\\psi}{dx}\\right)dx \\] <p>Why this form? Break it down:</p> <ol> <li>\\(-i\\hbar\\frac{d\\psi}{dx}\\) applies the momentum operator to \\(\\psi\\)</li> <li>Multiply by \\(\\psi^*(x)\\) (gives \\(|\\text{amplitude}|^2\\) weighting)</li> <li>Integrate over all space (average over the distribution)</li> </ol> <p>In abstract notation (works in any basis):</p> \\[ \\langle p \\rangle = \\langle\\psi|\\hat{p}|\\psi\\rangle \\] <p>This is called the expectation value or quantum average of the operator \\(\\hat{p}\\) in state \\(|\\psi\\rangle\\).</p> <p>Two cases to remember:</p> <p>\u2022 Eigenstate (definite momentum): If \\(|\\psi\\rangle\\) is a momentum eigenstate with momentum \\(p\\), then \\(\\langle\\psi|\\hat{p}|\\psi\\rangle = p\\). You get exactly the eigenvalue \u2014 100% certainty.</p> <p>\u2022 Superposition (indefinite momentum): If \\(|\\psi\\rangle = c_1|\\psi_1\\rangle + c_2|\\psi_2\\rangle\\) is a superposition of different momentum eigenstates, then \\(\\langle\\psi|\\hat{p}|\\psi\\rangle = |c_1|^2 p_1 + |c_2|^2 p_2\\) is a weighted average. Individual measurements give either \\(p_1\\) or \\(p_2\\), but the average over many measurements is this probabilistic mix.</p> <p>Key point: Even though individual measurements give discrete eigenvalues (\\(p_1\\) or \\(p_2\\)), the average over many measurements can be any value between them!</p>"},{"location":"quantum-mechanics/quantum-operators/#momentum-space-flipping-the-picture","title":"Momentum Space: Flipping the Picture","text":"<p>So far we've worked in position space where \\(\\psi(x)\\) tells us about position. But we can also work in momentum space where \\(\\tilde{\\psi}(p)\\) tells us about momentum!</p>"},{"location":"quantum-mechanics/quantum-operators/#position-space-vs-momentum-space","title":"Position Space vs Momentum Space","text":"<p>Remember from Foundations that a wave packet can be written as:</p> \\[ \\psi(x) = \\int_{-\\infty}^{\\infty} \\tilde{\\psi}(p) e^{ipx/\\hbar} \\frac{dp}{2\\pi\\hbar} \\] <p>Here \\(\\tilde{\\psi}(p)\\) is the momentum space wavefunction. It tells you \"how much of momentum \\(p\\)\" is in your state.</p> <p>Connection to plane waves: Each \\(e^{ipx/\\hbar}\\) is a momentum eigenstate. We're building \\(\\psi(x)\\) by adding up momentum eigenstates weighted by \\(\\tilde{\\psi}(p)\\).</p>"},{"location":"quantum-mechanics/quantum-operators/#momentum-eigenstates-in-momentum-space","title":"Momentum Eigenstates in Momentum Space","text":"<p>For a momentum eigenstate (single plane wave), what does \\(\\tilde{\\psi}(p)\\) look like?</p> <p>If \\(\\psi(x) = Ae^{ip_0 x/\\hbar}\\) (definite momentum \\(p_0\\)), then:</p> \\[ \\tilde{\\psi}(p) = A' \\delta(p - p_0) \\] <p>A delta function! All the \"weight\" is concentrated at exactly \\(p = p_0\\).</p> <p>Physical meaning: Measuring momentum gives \\(p_0\\) with 100% certainty. The momentum space wavefunction is infinitely sharp at that value.</p> <p>Compare to position space: A plane wave \\(e^{ipx/\\hbar}\\) is spread uniformly over all space (definite \\(p\\), completely uncertain \\(x\\)). Its momentum representation is a delta spike (completely certain \\(p\\)).</p> <p>This is the uncertainty principle in action: sharp in one space means spread in the other!</p>"},{"location":"quantum-mechanics/quantum-operators/#the-momentum-operator-in-momentum-space","title":"The Momentum Operator in Momentum Space","text":"<p>Here's something beautiful. The same operator \\(\\hat{p}\\) acts differently depending on which representation you use.</p> <p>In position representation:</p> \\[ \\hat{p}\\psi(x) = -i\\hbar\\frac{d\\psi}{dx} \\] <p>The operator acts as a derivative.</p> <p>In momentum representation:</p> \\[ \\hat{p}\\tilde{\\psi}(p) = p\\tilde{\\psi}(p) \\] <p>The operator just multiplies by \\(p\\). No derivatives needed!</p> <p>Why the difference? We're working in the momentum basis, where momentum eigenstates are the basis states \\(|p\\rangle\\) themselves. Acting on an eigenstate just returns its eigenvalue.</p> <p>Compare to position: in position space, \\(\\hat{x}\\psi(x) = x\\psi(x)\\) (multiply by \\(x\\)), but in momentum space, \\(\\hat{x}\\) would be \\(i\\hbar\\frac{d}{dp}\\) (a derivative!).</p> <p>The key insight: The abstract operator \\(\\hat{p}\\) is always the same physics. How it acts mathematically depends on which basis you've chosen to work in.</p>"},{"location":"quantum-mechanics/quantum-operators/#superposition-in-momentum-space","title":"Superposition in Momentum Space","text":"<p>For the two-plane-wave superposition:</p> \\[ \\psi(x) = c_1 e^{ip_1 x/\\hbar} + c_2 e^{ip_2 x/\\hbar} \\] <p>The momentum space wavefunction is:</p> \\[ \\tilde{\\psi}(p) = c_1\\delta(p-p_1) + c_2\\delta(p-p_2) \\] <p>Two delta spikes! One at \\(p_1\\) with weight \\(c_1\\), one at \\(p_2\\) with weight \\(c_2\\).</p> <p>Apply the momentum operator in momentum space:</p> \\[ \\hat{p}\\tilde{\\psi}(p) = p_1 c_1\\delta(p-p_1) + p_2 c_2\\delta(p-p_2) \\] <p>Different eigenvalues (\\(p_1\\) and \\(p_2\\)) multiply their respective components. This is exactly what we saw in position space, but now it's crystal clear: the delta spikes are the momentum eigenstates!</p> <p>The expected value becomes:</p> \\[ \\langle p \\rangle = \\int_{-\\infty}^{\\infty} \\tilde{\\psi}^*(p) \\cdot p \\cdot \\tilde{\\psi}(p) dp = |c_1|^2 p_1 + |c_2|^2 p_2 \\] <p>The delta functions do the integration for us, picking out the eigenvalues weighted by their probabilities.</p>"},{"location":"quantum-mechanics/quantum-operators/#summary-operators-extract-observable-information","title":"Summary: Operators Extract Observable Information","text":"<p>What we've learned:</p> <ol> <li> <p>Operators extract information: \\(\\hat{p} = -i\\hbar\\frac{d}{dx}\\) extracts momentum from \\(\\psi(x)\\)</p> </li> <li> <p>Eigenstates have definite values: If \\(\\hat{p}|\\psi\\rangle = p|\\psi\\rangle\\), measuring momentum gives \\(p\\) with 100% certainty</p> </li> <li> <p>Superpositions give probabilistic results: If \\(|\\psi\\rangle = c_1|p_1\\rangle + c_2|p_2\\rangle\\), you get \\(p_1\\) or \\(p_2\\) with probabilities \\(|c_1|^2\\) and \\(|c_2|^2\\)</p> </li> <li> <p>Expected values average over many measurements: \\(\\langle p\\rangle = \\langle\\psi|\\hat{p}|\\psi\\rangle\\)</p> </li> <li> <p>Different bases, same physics: In momentum space, eigenstates are delta functions \\(\\delta(p-p_0)\\) and \\(\\hat{p}\\) simply multiplies by \\(p\\)</p> </li> </ol> <p>This framework applies to all quantum operators: position, energy, angular momentum, etc. The mathematics is always the same \u2014 only the specific operator and basis change!</p>"},{"location":"quantum-mechanics/quantum-operators/#the-energy-operator-and-schrodingers-equation","title":"The Energy Operator and Schr\u00f6dinger's Equation","text":""},{"location":"quantum-mechanics/quantum-operators/#schrodingers-equation-as-an-eigenvalue-problem","title":"Schr\u00f6dinger's Equation as an Eigenvalue Problem","text":"<p>Remember from Foundations we derived Schr\u00f6dinger's equation from the time and spatial derivatives of a plane wave. We found:</p> \\[ i\\hbar\\frac{\\partial\\psi}{\\partial t} = \\hat{H}\\psi \\] <p>where the Hamiltonian operator \\(\\hat{H}\\) represents the total energy:</p> \\[ \\hat{H} = \\underbrace{\\frac{\\hat{p}^2}{2m}}_{\\text{kinetic}} + \\underbrace{V(x)}_{\\text{potential}} \\] <p>Why \\(\\hat{H}\\) instead of \\(\\hat{E}\\)? Pure historical convention from classical mechanics, where the Hamiltonian function represents total energy. We could have used \\(\\hat{E}\\) just like we use \\(\\hat{p}\\) for momentum, but \"Hamiltonian\" (named after physicist William Hamilton) became the standard term.</p> <p>What's the difference between \\(\\hat{H}\\) and \\(E\\)?</p> <p>\u2022 \\(\\hat{H}\\) is an operator \u2014 it acts on wavefunctions. When \\(\\psi\\) is an energy eigenstate, applying \\(\\hat{H}\\) gives \\(\\hat{H}\\psi = E\\psi\\) (the wavefunction back times a number). Like \\(\\hat{p}\\), it's used to calculate expectation values: \\(\\langle E \\rangle = \\langle\\psi|\\hat{H}|\\psi\\rangle\\).</p> <p>\u2022 \\(E\\) is a scalar (just a number) \u2014 the energy eigenvalue. It's the actual total energy value you'd measure.</p> <p>Important: When you see \"energy \\(E\\)\" in quantum mechanics, it means total energy (kinetic + potential) unless otherwise specified. This is different from classical mechanics where we often track kinetic and potential energy separately.</p> <p>For a free particle (no forces, so \\(V = 0\\)), the energy is purely kinetic:</p> \\[ \\hat{H} = \\frac{\\hat{p}^2}{2m} = -\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2} \\] <p>This is why plane wave energy \\(E = \\frac{\\hbar^2k^2}{2m}\\) is just the kinetic energy formula \\(E = \\frac{p^2}{2m}\\) \u2014 there's nothing else!</p>"},{"location":"quantum-mechanics/quantum-operators/#the-key-insight-eigenvalue-equation-for-time-independent-potentials","title":"The Key Insight: Eigenvalue Equation for Time-Independent Potentials","text":"<p>Here's what we didn't emphasize in Foundations: for time-independent potentials (systems where \\(V(x)\\) doesn't change with time), we can separate variables:</p> \\[ \\psi(x,t) = \\psi(x)e^{-iEt/\\hbar} \\] <p>Plugging this into Schr\u00f6dinger's equation, the time part cancels out and we get:</p> \\[ \\hat{H}\\psi(x) = E\\psi(x) \\] <p>This is literally an eigenvalue equation!</p> <p>Think about what you're solving for:</p> <p>\u2022 You know the operator: \\(\\hat{H}\\) (you specify the potential \\(V(x)\\) and mass \\(m\\)) \u2022 You don't know the function: \\(\\psi(x)\\) (this is what you're solving for!) \u2022 You don't know the constant: \\(E\\) (what energies are allowed?)</p> <p>You're looking for functions \\(\\psi(x)\\) that, when you apply \\(\\hat{H}\\) to them, give back the same function multiplied by a constant. Those special functions are the eigenfunctions (energy eigenstates) and those special constants are the eigenvalues (energy levels).</p> <p>What \"solving Schr\u00f6dinger's equation\" really means: You're finding eigenfunctions and eigenvalues of the Hamiltonian operator! Every solution \\(\\psi(x)\\) you find is automatically an energy eigenstate with a definite energy eigenvalue \\(E\\).</p> <p>For a free particle: Plane waves \\(\\psi(x) = Ae^{ikx}\\) are the energy eigenfunctions with eigenvalues \\(E = \\frac{\\hbar^2k^2}{2m}\\). The energy spectrum is continuous \u2014 any \\(E \\geq 0\\) is allowed.</p>"},{"location":"quantum-mechanics/quantum-operators/#example-1-single-plane-wave-energy-eigenstate","title":"Example 1: Single Plane Wave (Energy Eigenstate)","text":"<p>Consider an electron (mass \\(m_e = 9.11 \\times 10^{-31}\\) kg) in a single plane wave state:</p> \\[ \\psi(x) = A e^{ikx} \\quad \\text{where } k = 5 \\times 10^{10}\\text{ m}^{-1} \\] <p>The energy is:</p> \\[ E = \\frac{\\hbar^2 k^2}{2m_e} = \\frac{(1.055 \\times 10^{-34})^2 (5 \\times 10^{10})^2}{2(9.11 \\times 10^{-31})} \\approx 1.53 \\times 10^{-18}\\text{ J} \\approx 9.6\\text{ eV} \\] <p>Apply the Hamiltonian operator:</p> \\[ \\hat{H}\\psi = -\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2}\\left(Ae^{ikx}\\right) = -\\frac{\\hbar^2}{2m}(ik)^2 Ae^{ikx} = \\frac{\\hbar^2k^2}{2m}\\psi = E\\psi \\] <p>We get back the same wavefunction multiplied by the energy eigenvalue \\(E\\).</p> <p>Physical meaning: If you measure energy, you get \\(E = 9.6\\) eV with 100% certainty. Every single measurement gives the same result. The particle has definite energy.</p>"},{"location":"quantum-mechanics/quantum-operators/#example-2-superposition-of-two-plane-waves-not-an-eigenstate_1","title":"Example 2: Superposition of Two Plane Waves (NOT an Eigenstate)","text":"<p>Now consider a superposition of two plane waves with different wavenumbers:</p> \\[ \\psi(x) = \\frac{1}{\\sqrt{2}}\\left(e^{ik_1 x} + e^{ik_2 x}\\right) \\] <p>Let's use \\(k_1 = 3 \\times 10^{10}\\) m\u207b\u00b9 and \\(k_2 = 7 \\times 10^{10}\\) m\u207b\u00b9. These give different energies:</p> \\[ E_1 = \\frac{\\hbar^2k_1^2}{2m_e} \\approx 3.5\\text{ eV} \\] \\[ E_2 = \\frac{\\hbar^2k_2^2}{2m_e} \\approx 18.8\\text{ eV} \\] <p>Apply the Hamiltonian:</p> \\[ \\hat{H}\\psi = \\frac{1}{\\sqrt{2}}\\left(\\hat{H}e^{ik_1 x} + \\hat{H}e^{ik_2 x}\\right) = \\frac{1}{\\sqrt{2}}\\left(E_1 e^{ik_1 x} + E_2 e^{ik_2 x}\\right) \\] <p>This is NOT equal to \\(E\\psi\\) for any single energy \\(E\\)! Different eigenvalues (\\(E_1\\) and \\(E_2\\)) multiply their respective components.</p> <p>Physical meaning: This state is not an energy eigenstate. If you measure energy, you get:</p> <p>\u2022 \\(E_1 = 3.5\\) eV with probability \\(|\\frac{1}{\\sqrt{2}}|^2 = 50\\%\\) \u2022 \\(E_2 = 18.8\\) eV with probability \\(|\\frac{1}{\\sqrt{2}}|^2 = 50\\%\\)</p> <p>The particle does not have definite energy! This is the same probabilistic measurement story we saw with the momentum operator.</p> <p>Let's calculate the expected value explicitly using the expectation value formula. For our superposition \\(\\psi = \\frac{1}{\\sqrt{2}}|E_1\\rangle + \\frac{1}{\\sqrt{2}}|E_2\\rangle\\):</p> \\[ \\langle E \\rangle = \\langle\\psi|\\hat{H}|\\psi\\rangle = \\left(\\frac{1}{\\sqrt{2}}\\langle E_1| + \\frac{1}{\\sqrt{2}}\\langle E_2|\\right) \\hat{H} \\left(\\frac{1}{\\sqrt{2}}|E_1\\rangle + \\frac{1}{\\sqrt{2}}|E_2\\rangle\\right) \\] <p>Expanding this gives four terms:</p> \\[ \\langle E \\rangle = \\frac{1}{2}\\langle E_1|\\hat{H}|E_1\\rangle + \\frac{1}{2}\\langle E_2|\\hat{H}|E_2\\rangle + \\frac{1}{2}\\langle E_1|\\hat{H}|E_2\\rangle + \\frac{1}{2}\\langle E_2|\\hat{H}|E_1\\rangle \\] <p>The cross terms vanish (just like the momentum case):</p> \\[ \\langle E_1|\\hat{H}|E_2\\rangle = \\langle E_1|E_2|E_2\\rangle = E_2 \\langle E_1|E_2\\rangle = 0 \\] <p>So we're left with:</p> \\[ \\langle E \\rangle = \\frac{1}{2}(3.5\\text{ eV}) + \\frac{1}{2}(18.8\\text{ eV}) = 11.15\\text{ eV} \\] <p>Even though individual measurements give either 3.5 eV or 18.8 eV, the average is 11.15 eV!</p>"},{"location":"quantum-mechanics/quantum-operators/#confined-particles-and-discrete-energy-levels","title":"Confined Particles and Discrete Energy Levels","text":""},{"location":"quantum-mechanics/quantum-operators/#from-continuous-to-discrete","title":"From Continuous to Discrete","text":"<p>So far, we've looked at free particles where the energy spectrum is continuous \u2014 any energy \\(E \\geq 0\\) is allowed. But what happens when we confine the particle?</p> <p>Boundary conditions change everything:</p> <p>\u2022 Add walls (particle in a box) \u2022 Add a potential well \u2022 Confine in an atom or molecule</p> <p>When you confine a particle, the wavefunction must satisfy boundary conditions (like being zero at walls). This restricts which wavelengths \u2014 and therefore which wavenumbers \\(k\\) \u2014 are allowed.</p> <p>Result: Only certain discrete values of \\(k\\) are permitted, which means only certain energies are allowed!</p> \\[ E_1, E_2, E_3, \\ldots \\quad \\text{(a discrete ladder of energy levels)} \\]"},{"location":"quantum-mechanics/quantum-operators/#example-particle-in-a-box","title":"Example: Particle in a Box","text":"<p>The simplest confined system is a particle trapped in a 1D box of length \\(L\\) with infinitely high walls. The boundary conditions force the wavefunction to be zero at the walls.</p> <p>Energy eigenvalues:</p> \\[ E_n = \\frac{n^2\\pi^2\\hbar^2}{2mL^2}, \\quad n = 1, 2, 3, \\ldots \\] <p>Energy eigenfunctions:</p> \\[ \\psi_n(x) = \\sqrt{\\frac{2}{L}}\\sin\\left(\\frac{n\\pi x}{L}\\right), \\quad 0 \\leq x \\leq L \\] <p>The quantum number \\(n\\) labels the energy levels. The first few levels:</p> <p>\u2022 \\(n=1\\) (ground state): \\(E_1 = \\frac{\\pi^2\\hbar^2}{2mL^2}\\) \u2022 \\(n=2\\) (first excited state): \\(E_2 = 4E_1\\) \u2022 \\(n=3\\) (second excited state): \\(E_3 = 9E_1\\)</p> <p>Notice the energies grow as \\(n^2\\) \u2014 the spacing between levels gets larger as you go up!</p> <p>For a concrete example: An electron in a box of length \\(L = 1\\) nm:</p> \\[ E_1 = \\frac{\\pi^2(1.055 \\times 10^{-34})^2}{2(9.11 \\times 10^{-31})(10^{-9})^2} \\approx 6.0 \\times 10^{-20}\\text{ J} \\approx 0.38\\text{ eV} \\] <p>Then \\(E_2 \\approx 1.5\\) eV, \\(E_3 \\approx 3.4\\) eV, etc.</p> <p>Key insight: Confinement creates discrete energy levels. The smaller the box, the larger the energy spacing!</p>"},{"location":"quantum-mechanics/quantum-operators/#matrix-representation-of-operators","title":"Matrix Representation of Operators","text":""},{"location":"quantum-mechanics/quantum-operators/#discrete-basis-operators-become-matrices","title":"Discrete Basis \u2192 Operators Become Matrices","text":"<p>We've now seen two very different situations:</p> <p>Continuous basis (position \\(x\\), momentum \\(p\\)): \u2022 Basis labels are continuous variables \u2022 States are functions of continuous variables: \\(\\psi(x)\\) or \\(\\tilde{\\psi}(p)\\) \u2022 Operators are differential operators (involve derivatives)</p> <p>Discrete basis (energy levels, quantum numbers): \u2022 Basis labels are discrete: \\(n = 1, 2, 3, \\ldots\\) \u2022 States written as sums over discrete basis states: \\(|E_1\\rangle, |E_2\\rangle, |E_3\\rangle, \\ldots\\) \u2022 Common examples: energy levels, spin states (\u2191/\u2193), photon polarization (H/V) \u2022 Operators become matrices!</p> <p>Why matrices? Any quantum state can be written as a superposition of basis states:</p> \\[ |\\psi\\rangle = c_1|E_1\\rangle + c_2|E_2\\rangle + c_3|E_3\\rangle + \\cdots \\] <p>We can represent this state as a column vector of coefficients:</p> \\[ |\\psi\\rangle \\leftrightarrow \\begin{pmatrix} c_1 \\\\ c_2 \\\\ c_3 \\\\ \\vdots \\end{pmatrix} \\] <p>When an operator acts on a state, it becomes matrix multiplication!</p>"},{"location":"quantum-mechanics/quantum-operators/#the-hamiltonian-in-the-energy-basis","title":"The Hamiltonian in the Energy Basis","text":"<p>Here's something beautiful: in its own eigenbasis, an operator is always diagonal.</p> <p>The Hamiltonian in the energy basis is:</p> \\[ \\hat{H} = \\begin{pmatrix} E_1 &amp; 0 &amp; 0 &amp; \\cdots \\\\ 0 &amp; E_2 &amp; 0 &amp; \\cdots \\\\ 0 &amp; 0 &amp; E_3 &amp; \\cdots \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots \\end{pmatrix} \\] <p>The energy eigenvalues sit on the diagonal. All off-diagonal elements are zero.</p> <p>Why? When you apply \\(\\hat{H}\\) to an eigenstate \\(|E_n\\rangle\\), you get \\(E_n|E_n\\rangle\\) \u2014 it just multiplies by the eigenvalue. No mixing with other states!</p>"},{"location":"quantum-mechanics/quantum-operators/#example-two-level-system","title":"Example: Two-Level System","text":"<p>Let's work with the simplest discrete system: two energy levels. This could represent:</p> <p>\u2022 The first two levels of a particle in a box \u2022 Two electronic states in an atom \u2022 A spin-1/2 particle (coming later!)</p> <p>Setup: Two energy eigenstates with energies \\(E_1 = 1\\) eV and \\(E_2 = 3\\) eV.</p> <p>Basis states: \\(|E_1\\rangle\\) and \\(|E_2\\rangle\\)</p> <p>The Hamiltonian as a 2\u00d72 matrix:</p> \\[ \\hat{H} = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 3 \\end{pmatrix} \\text{ eV} \\] <p>Example A: Energy Eigenstate</p> <p>Consider the particle in the second excited state:</p> \\[ |\\psi\\rangle = |E_2\\rangle \\] <p>As a column vector:</p> \\[ |\\psi\\rangle = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\] <p>Apply the Hamiltonian (matrix multiplication):</p> \\[ \\hat{H}|\\psi\\rangle = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix} = 3 \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = E_2|\\psi\\rangle \\] <p>We get back the same state multiplied by the eigenvalue \\(E_2 = 3\\) eV.</p> <p>Physical meaning: Measure energy \u2192 get 3 eV with 100% certainty.</p> <p>Example B: Superposition State</p> <p>Now consider an equal superposition:</p> \\[ |\\psi\\rangle = \\frac{1}{\\sqrt{2}}|E_1\\rangle + \\frac{1}{\\sqrt{2}}|E_2\\rangle \\] <p>As a column vector:</p> \\[ |\\psi\\rangle = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\] <p>Apply the Hamiltonian:</p> \\[ \\hat{H}|\\psi\\rangle = \\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; 3 \\end{pmatrix} \\cdot \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} \\] <p>This is NOT equal to \\(E|\\psi\\rangle\\) for any single energy! The components get multiplied by different eigenvalues (1 and 3).</p> <p>Physical meaning: This is not an energy eigenstate. If you measure energy, you get:</p> <p>\u2022 \\(E_1 = 1\\) eV with probability \\(|\\frac{1}{\\sqrt{2}}|^2 = 50\\%\\) \u2022 \\(E_2 = 3\\) eV with probability \\(|\\frac{1}{\\sqrt{2}}|^2 = 50\\%\\)</p> <p>The expected value (average over many measurements):</p> \\[ \\langle E \\rangle = 50\\% \\cdot 1 + 50\\% \\cdot 3 = 2\\text{ eV} \\] <p>Individual measurements give 1 eV or 3 eV, but the average is 2 eV!</p> <p>Using the matrix formula:</p> \\[ \\langle E \\rangle = \\langle\\psi|\\hat{H}|\\psi\\rangle = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 &amp; 1 \\end{pmatrix} \\cdot \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\frac{1}{2}(1 + 3) = 2\\text{ eV} \\] <p>The row vector \\(\\langle\\psi|\\) is the conjugate transpose of \\(|\\psi\\rangle\\).</p>"},{"location":"quantum-mechanics/quantum-operators/#example-position-operator-in-energy-basis","title":"Example: Position Operator in Energy Basis","text":"<p>We saw that the Hamiltonian is diagonal in the energy basis. But what about other operators?</p> <p>Key principle: An operator is diagonal only in its own eigenbasis. In other bases, it has off-diagonal elements.</p> <p>Let's see this by calculating the expected position for a particle in a box in a superposition of energy states:</p> \\[ |\\psi\\rangle = \\frac{1}{\\sqrt{2}}|E_1\\rangle + \\frac{1}{\\sqrt{2}}|E_2\\rangle \\] <p>Using the expectation value formula (same structure as before):</p> \\[ \\langle x \\rangle = \\langle\\psi|\\hat{x}|\\psi\\rangle = \\frac{1}{2}\\langle E_1|\\hat{x}|E_1\\rangle + \\frac{1}{2}\\langle E_2|\\hat{x}|E_2\\rangle + \\frac{1}{2}\\langle E_1|\\hat{x}|E_2\\rangle + \\frac{1}{2}\\langle E_2|\\hat{x}|E_1\\rangle \\] <p>But this time the cross terms don't vanish! Why? Because \\(|E_1\\rangle\\) and \\(|E_2\\rangle\\) are not position eigenstates. We can't pull out an eigenvalue, so orthogonality doesn't help us.</p> <p>What is \\(\\langle E_m|\\hat{x}|E_n\\rangle\\) mathematically?</p> <p>Here's the key: We're working in the energy basis (discrete, column vectors), but to calculate the matrix elements, we need to convert to position representation where we know the explicit forms of the energy eigenstates.</p> <p>The energy eigenstate \\(|E_n\\rangle\\) (an abstract ket in energy basis) corresponds to a specific wavefunction \\(\\psi_n(x)\\) in position representation. For particle in a box:</p> \\[ \\psi_n(x) = \\sqrt{\\frac{2}{L}}\\sin\\left(\\frac{n\\pi x}{L}\\right) \\] <p>These are the same states \u2014 just written in different representations! To calculate the matrix element \\(\\langle E_m|\\hat{x}|E_n\\rangle\\), we convert to position space:</p> \\[ \\langle E_m|\\hat{x}|E_n\\rangle = \\int_0^L \\psi_m^*(x) \\cdot x \\cdot \\psi_n(x) \\, dx = \\frac{2}{L}\\int_0^L x \\sin\\left(\\frac{m\\pi x}{L}\\right) \\sin\\left(\\frac{n\\pi x}{L}\\right) dx \\] <p>Result of the integrals:</p> <p>\u2022 Diagonal elements (\\(m=n\\)): By symmetry, \\(\\langle E_n|\\hat{x}|E_n\\rangle = \\frac{L}{2}\\) (expected position at center of box)</p> <p>\u2022 Off-diagonal elements (\\(m \\neq n\\)): These integrals are generally non-zero! Each integral gives you a single number.</p> <p>These numbers become the entries in our matrix when we return to the energy basis representation (column vectors).</p> <p>Contrast with the Hamiltonian:</p> \\[ \\langle E_m|\\hat{H}|E_n\\rangle = \\int_0^L \\psi_m^*(x) \\hat{H}\\psi_n(x) \\, dx = E_n \\int_0^L \\psi_m^*(x) \\psi_n(x) \\, dx = E_n \\cdot 0 = 0 \\] <p>The eigenvalue \\(E_n\\) comes out, leaving orthogonality \u2192 zero. But for \\(\\hat{x}\\), there's no eigenvalue to extract!</p> <p>Matrix representation: These \\(\\langle E_m|\\hat{x}|E_n\\rangle\\) values are the matrix elements. For a 2-level system:</p> \\[ \\hat{x} \\approx \\begin{pmatrix} L/2 &amp; x_{12} \\\\ x_{21} &amp; L/2 \\end{pmatrix} \\] <p>where \\(x_{12}\\) and \\(x_{21}\\) are the off-diagonal matrix elements.</p> <p>The key insight: Every operator is diagonal in its own eigenbasis, but appears as a more complex matrix in other bases.</p>"},{"location":"quantum-mechanics/quantum-operators/#summary-energy-schrodingers-equation-and-matrices","title":"Summary: Energy, Schr\u00f6dinger's Equation, and Matrices","text":"<p>What we've learned:</p> <ol> <li> <p>Time derivative extracts energy: Just like spatial derivative \\(\\frac{\\partial}{\\partial x}\\) extracts momentum, time derivative \\(\\frac{\\partial}{\\partial t}\\) extracts energy: \\(i\\hbar\\frac{\\partial\\psi}{\\partial t} = E\\psi\\)</p> </li> <li> <p>Hamiltonian = total energy operator: \\(\\hat{H} = \\frac{\\hat{p}^2}{2m} + V(x)\\) represents kinetic + potential energy. Energy eigenvalue \\(E\\) always means total energy (unlike classical mechanics where we often separate KE and PE).</p> </li> <li> <p>Schr\u00f6dinger's equation is an eigenvalue equation: For time-independent potentials, \"solving Schr\u00f6dinger's equation\" means finding energy eigenfunctions and eigenvalues: \\(\\hat{H}\\psi(x) = E\\psi(x)\\)</p> </li> <li> <p>Free particles: continuous spectrum: Plane wave solutions \\(\\psi(x) = Ae^{ikx}\\) with \\(E = \\frac{\\hbar^2k^2}{2m}\\) (purely kinetic). Any energy \\(E \\geq 0\\) is allowed.</p> </li> <li> <p>Confined particles: discrete spectrum: Boundary conditions restrict allowed wavelengths, creating discrete energy levels \\(E_1, E_2, E_3, \\ldots\\) (Example: particle in box with \\(E_n = \\frac{n^2\\pi^2\\hbar^2}{2mL^2}\\))</p> </li> <li> <p>Discrete basis \u2192 matrix representation: When working with discrete energy levels:    \u2022 States become column vectors: \\(|\\psi\\rangle = c_1|E_1\\rangle + c_2|E_2\\rangle \\rightarrow \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}\\)    \u2022 Operators become matrices    \u2022 Operator action = matrix multiplication</p> </li> <li> <p>Operators are diagonal in their own eigenbasis: \\(\\hat{H}\\) in energy basis has eigenvalues on diagonal. In other bases (like position operator in energy basis), matrices have off-diagonal elements.</p> </li> <li> <p>Same probabilistic measurement framework: Whether continuous (plane waves) or discrete (energy levels), superposition states give probabilistic measurement outcomes with probabilities \\(|c_n|^2\\).</p> </li> </ol> <p>The big picture: Quantum operators extract observable quantities through eigenvalue equations. The mathematical form changes (derivatives vs matrices) depending on whether you're working with continuous or discrete bases, but the physics \u2014 eigenstates, eigenvalues, and probabilistic measurements \u2014 remains the same!</p>"},{"location":"quantum-mechanics/quantum-operators/#what-changes-with-problem-vs-basis","title":"What Changes With Problem vs Basis?","text":"<p>The Hamiltonian \\(\\hat{H}\\) changes for different physical systems (free particle, hydrogen atom, etc.), giving different energy eigenvalues. Other operators like \\(\\hat{x}\\) and \\(\\hat{p}\\) are the same for all problems, but their mathematical form changes depending on which basis you choose:</p> Operator Position Basis \\(\\psi(x)\\) Momentum Basis \\(\\tilde{\\psi}(p)\\) Energy Basis \\(c_n\\) (discrete) \\(\\hat{x}\\) multiply by \\(x\\) \\(i\\hbar\\frac{d}{dp}\\) off-diagonal matrix \\(\\hat{p}\\) \\(-i\\hbar\\frac{d}{dx}\\) multiply by \\(p\\) off-diagonal matrix \\(\\hat{H}\\) \\(-\\frac{\\hbar^2}{2m}\\frac{d^2}{dx^2} + V(x)\\) \\(\\frac{p^2}{2m} + V(i\\hbar\\frac{d}{dp})\\) diagonal matrix (eigenvalues \\(E_n\\)) <p>Same operator, different mathematical form. The physics doesn't change\u2014only how you calculate it. Pick whichever basis makes your calculation easiest!</p>"},{"location":"quantum-mechanics/quantum-pauli-exclusion/","title":"The Pauli Exclusion Principle","text":"<p>OUTDATED CONTENT</p> <p>This page contains draft/outdated material and is not currently maintained.</p> <p>Prerequisites: This builds on Multi-Particle Quantum Mechanics. Make sure you understand identical particles, fermions vs bosons, and antisymmetric wavefunctions before continuing!</p>"},{"location":"quantum-mechanics/quantum-pauli-exclusion/#quick-recap-identical-fermions-must-be-antisymmetric","title":"Quick Recap: Identical Fermions Must Be Antisymmetric","text":"<p>In the previous section, we learned that identical fermions (like electrons) must have antisymmetric wavefunctions:</p> \\[ \\psi(x_2, x_1) = -\\psi(x_1, x_2) \\] <p>When we put two fermions in states \\(\\psi_a\\) and \\(\\psi_b\\), we construct the antisymmetric wavefunction:</p> \\[ \\psi(x_1, x_2) = \\frac{1}{\\sqrt{2}}\\left[\\psi_a(x_1)\\psi_b(x_2) - \\psi_a(x_2)\\psi_b(x_1)\\right] \\] <p>Now let's see what happens when we try to put both fermions in the same state...</p>"},{"location":"quantum-mechanics/quantum-pauli-exclusion/#pauli-exclusion-principle","title":"Pauli Exclusion Principle","text":"<p>The antisymmetry requirement leads directly to one of the most important principles in quantum mechanics.</p>"},{"location":"quantum-mechanics/quantum-pauli-exclusion/#what-happens-if-a-b","title":"What Happens If \\(a = b\\)?","text":"<p>Suppose we try to put both electrons in the same state: \\(\\psi_a = \\psi_b\\).</p> <p>The antisymmetric wavefunction becomes:</p> \\[ \\psi(x_1, x_2) = \\frac{1}{\\sqrt{2}}[\\psi_a(x_1)\\psi_a(x_2) - \\psi_a(x_2)\\psi_a(x_1)] = 0 \\] <p>The wavefunction vanishes! This state doesn't exist.</p> <p>Pauli Exclusion Principle: Two identical fermions cannot occupy the same quantum state.</p> <p>Alternative statement: If two fermions are in the same position (\\(x_1 = x_2 = x\\)):</p> \\[ \\psi(x, x) = -\\psi(x, x) = 0 \\] <p>The antisymmetry forces \\(\\psi(x,x) = 0\\). Fermions avoid each other!</p>"},{"location":"quantum-mechanics/quantum-pauli-exclusion/#including-spin-the-complete-picture","title":"Including Spin: The Complete Picture","text":"<p>The complete wavefunction must be antisymmetric under exchange of all quantum numbers (spatial + spin):</p> \\[ \\Psi(x_1, x_2) = \\psi_{\\text{spatial}}(x_1, x_2) \\times \\psi_{\\text{spin}} \\] <p>Key insight: If the spatial part is symmetric, the spin part must be antisymmetric (and vice versa).</p> <p>Example 1: Spatial symmetric \u2192 spin antisymmetric</p> \\[ \\Psi = \\underbrace{\\frac{1}{\\sqrt{2}}[\\psi_a(x_1)\\psi_b(x_2) + \\psi_a(x_2)\\psi_b(x_1)]}_{\\text{symmetric}} \\times \\underbrace{\\frac{1}{\\sqrt{2}}(|\\uparrow\\downarrow\\rangle - |\\downarrow\\uparrow\\rangle)}_{\\text{antisymmetric (singlet)}} \\] <p>Example 2: Spatial antisymmetric \u2192 spin symmetric</p> \\[ \\Psi = \\underbrace{\\frac{1}{\\sqrt{2}}[\\psi_a(x_1)\\psi_b(x_2) - \\psi_a(x_2)\\psi_b(x_1)]}_{\\text{antisymmetric}} \\times \\underbrace{|\\uparrow\\uparrow\\rangle}_{\\text{symmetric}} \\] <p>Result: Two electrons can be in the same spatial state if they have opposite spins!</p> <p>The complete state (spatial \u00d7 spin) is still antisymmetric overall:</p> \\[ \\psi_a(x_1)\\psi_a(x_2) \\times \\frac{1}{\\sqrt{2}}(|\\uparrow\\downarrow\\rangle - |\\downarrow\\uparrow\\rangle) \\] <p>This is what allows two electrons to occupy the \"same orbital\" in atoms (they must have opposite spins).</p>"},{"location":"quantum-mechanics/quantum-pauli-exclusion/#electrons-in-a-box-energy-levels-fill-up","title":"Electrons in a Box: Energy Levels Fill Up","text":"<p>Let's see Pauli exclusion in action by adding electrons to a 1D infinite potential well one at a time.</p>"},{"location":"quantum-mechanics/quantum-pauli-exclusion/#energy-levels-reminder","title":"Energy Levels (Reminder)","text":"<p>The single-particle energy levels are:</p> \\[ E_n = \\frac{n^2\\pi^2\\hbar^2}{2mL^2}, \\quad n = 1, 2, 3, ... \\] <p>For a box of length \\(L = 1\\) nm:</p> <p>\u2022 \\(E_1 \\approx 0.38\\) eV (ground state) \u2022 \\(E_2 = 4E_1 \\approx 1.5\\) eV \u2022 \\(E_3 = 9E_1 \\approx 3.4\\) eV \u2022 \\(E_4 = 16E_1 \\approx 6.0\\) eV</p>"},{"location":"quantum-mechanics/quantum-pauli-exclusion/#adding-electrons-one-by-one","title":"Adding Electrons One by One","text":"<p>One electron:</p> <p>Goes into the ground state: \\(n=1\\), either spin up or down.</p> \\[ \\Psi = \\psi_1(x_1)|\\uparrow\\rangle \\] <p>Total energy: \\(E = E_1 \\approx 0.38\\) eV</p> <p>Two electrons:</p> <p>Both can go into \\(n=1\\) level if they have opposite spins!</p> \\[ \\Psi = \\psi_1(x_1)\\psi_1(x_2) \\times \\frac{1}{\\sqrt{2}}(|\\uparrow\\downarrow\\rangle - |\\downarrow\\uparrow\\rangle) \\] <p>Spatial part is symmetric, spin part is antisymmetric \u2192 total antisymmetric \u2713</p> <p>Total energy: \\(E = 2E_1 \\approx 0.76\\) eV</p> <p>(We're ignoring Coulomb repulsion for now to keep it simple.)</p> <p>Three electrons:</p> <p>Now we have a problem! The \\(n=1\\) level is \"full\" (two electrons with opposite spins).</p> <p>Pauli exclusion: The third electron must go into the next level up!</p> \\[ \\Psi = \\psi_1(x_1)\\psi_1(x_2)\\psi_2(x_3) \\times (\\text{properly antisymmetrized spin state}) \\] <p>Total energy: \\(E = 2E_1 + E_2 = 2(0.38) + 1.5 = 2.3\\) eV</p> <p>Big jump! The third electron forced into a higher energy state.</p> <p>Four electrons:</p> <p>Two in \\(n=1\\) (opposite spins), two in \\(n=2\\) (opposite spins):</p> <p>Total energy: \\(E = 2E_1 + 2E_2 = 2(0.38) + 2(1.5) = 3.8\\) eV</p> <p>Five electrons:</p> \\[ E = 2E_1 + 2E_2 + E_3 = 0.76 + 3.0 + 3.4 = 7.2 \\text{ eV} \\] <p>The pattern: Energy levels fill up like stairs. Each spatial state holds at most 2 electrons (spin up and spin down).</p>"},{"location":"quantum-mechanics/quantum-pauli-exclusion/#connection-to-the-periodic-table","title":"Connection to the Periodic Table","text":"<p>This is exactly how electrons fill atomic orbitals!</p> <p>\u2022 Each orbital (characterized by quantum numbers \\(n, \\ell, m\\)) can hold 2 electrons</p> <p>\u2022 Electrons fill from lowest to highest energy</p> <p>\u2022 This explains the structure of the periodic table</p> <p>Examples:</p> <p>\u2022 Helium (2 electrons): 1s\u00b2 (both in ground state, opposite spins)</p> <p>\u2022 Lithium (3 electrons): 1s\u00b2 2s\u00b9 (third electron forced to next shell)</p> <p>\u2022 Neon (10 electrons): 1s\u00b2 2s\u00b2 2p\u2076 (first two shells completely filled)</p> <p>The Pauli exclusion principle is why matter is stable and has structure!</p>"},{"location":"quantum-mechanics/quantum-pauli-exclusion/#coulomb-repulsion-spatial-correlation-example","title":"Coulomb Repulsion: Spatial Correlation Example","text":"<p>Now let's add back the interaction between electrons and see how it creates spatial correlations.</p>"},{"location":"quantum-mechanics/quantum-pauli-exclusion/#two-electrons-in-a-box-with-repulsion","title":"Two Electrons in a Box with Repulsion","text":"<p>Consider two electrons in a 1D box of length \\(L\\), both with the same spin (say, both spin up). By Pauli exclusion, they must be in different spatial states.</p> <p>Simplest antisymmetric state (ignoring spin notation since both are \\(\\uparrow\\)):</p> \\[ \\psi(x_1, x_2) = \\frac{1}{\\sqrt{2}}[\\psi_1(x_1)\\psi_2(x_2) - \\psi_2(x_1)\\psi_1(x_2)] \\] <p>where \\(\\psi_n(x) = \\sqrt{2/L}\\sin(n\\pi x/L)\\).</p> <p>Without interaction: This is exact! Energy = \\(E_1 + E_2 = 5E_1\\).</p> <p>With Coulomb repulsion: \\(V(x_1, x_2) = \\frac{ke^2}{|x_1 - x_2|}\\)</p> <p>The wavefunction gets distorted (electrons avoid each other even more!).</p>"},{"location":"quantum-mechanics/quantum-pauli-exclusion/#probability-distributions","title":"Probability Distributions","text":"<p>Question: If we find electron 1 at position \\(x_1\\), where is electron 2 likely to be?</p> <p>Conditional probability:</p> \\[ P(x_2 | x_1) = \\frac{|\\psi(x_1, x_2)|^2}{\\int |\\psi(x_1, x_2)|^2 dx_2} \\] <p>Let's plug in a specific location: Say \\(x_1 = L/4\\) (left side of box).</p> <p>For the antisymmetric state above:</p> \\[ |\\psi(L/4, x_2)|^2 = \\frac{1}{2}\\left|\\psi_1(L/4)\\psi_2(x_2) - \\psi_2(L/4)\\psi_1(x_2)\\right|^2 \\] <p>Result: Electron 2 is more likely to be found on the right side of the box!</p> <p>The antisymmetry + Coulomb repulsion creates a \"hole\" in the probability distribution near \\(x_2 = x_1\\).</p>"},{"location":"quantum-mechanics/quantum-pauli-exclusion/#expected-separation","title":"Expected Separation","text":"<p>We can calculate the average distance between electrons:</p> \\[ \\langle |x_1 - x_2| \\rangle = \\int_0^L \\int_0^L |x_1 - x_2| |\\psi(x_1, x_2)|^2 dx_1 dx_2 \\] <p>Without Pauli exclusion (if they were distinguishable bosons in the same state):</p> \\[ \\langle |x_1 - x_2| \\rangle \\approx 0.4L \\] <p>With Pauli exclusion (antisymmetric fermion state):</p> \\[ \\langle |x_1 - x_2| \\rangle \\approx 0.5L \\] <p>Fermions stay further apart on average! Antisymmetry creates an effective \"repulsion\" even before considering Coulomb forces.</p> <p>With Coulomb repulsion included:</p> \\[ \\langle |x_1 - x_2| \\rangle \\approx 0.6L \\] <p>The interaction pushes them even further apart.</p> <p>Key insight: Finding one electron at a particular location changes the probability distribution for the other (they're entangled!).</p>"},{"location":"quantum-mechanics/quantum-pauli-exclusion/#summary-why-pauli-exclusion-matters","title":"Summary: Why Pauli Exclusion Matters","text":"<p>The Pauli exclusion principle isn't just a rule - it's a fundamental consequence of fermion antisymmetry that has profound effects:</p> <p>1. Atoms have structure: Without it, all electrons would collapse into the ground state. No chemistry, no life!</p> <p>2. Matter is stable: Pauli pressure prevents white dwarfs and neutron stars from collapsing further.</p> <p>3. Electrons stay apart: Even without Coulomb repulsion, antisymmetry creates \"exchange repulsion.\"</p> <p>4. The periodic table: Explains why elements have their chemical properties (valence electrons, noble gases, etc.).</p> <p>5. Conductors vs insulators: Band structure and electrical conductivity depend on how energy levels fill up.</p> <p>Next: Continue to Spin, Entanglement, and Bell's Theorem to explore the deeper quantum correlations and experimental tests of quantum mechanics!</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/","title":"Entanglement Isn't Just for Spin","text":"<p>This page highlights some of my favorite insights from Daniel V. Schroeder's paper [1]:</p> <p>\"Entanglement isn't just for spin\", American Journal of Physics 85, 812 (2017) arXiv: 1703.10620</p> <p>The paper shows that quantum entanglement occurs not just in spin systems, but also in the spatial wave functions of particles. You can see entanglement in simple 2D systems that are easy to visualize.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#key-pedagogical-insight","title":"Key Pedagogical Insight","text":"<p>A powerful insight from the paper is that you can first study a single particle in 2D, with a wavefunction \\(\\psi(x, y)\\), and learn to see the difference between separable states (which factor into \\(X(x)Y(y)\\)) and non-separable states (whose x- and y-shapes depend on each other).</p> <p>Once students can visually recognize non-separability in 2D plots, you then make the simple substitution \\((x,y) \\to (x_1, x_2)\\). Instantly, the same mathematics describes a two-particle system, and the same non-separability now becomes true entanglement between particle 1 and particle 2.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#one-particle-in-two-dimensions","title":"One Particle in Two Dimensions","text":"<p>Consider a particle in a 2D square box (infinite square well), where both x and y go from 0 to L.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#example-1-a-separable-wave-function","title":"Example 1: A Separable Wave Function","text":"<p>The simplest solutions have the form:</p> \\[\\psi(x,y) = \\sin\\left(\\frac{n_x \\pi x}{L}\\right) \\sin\\left(\\frac{n_y \\pi y}{L}\\right)\\] <p>This is called separable because it factors into a function of x times a function of y:</p> \\[\\psi(x,y) = f(x) \\cdot g(y)\\] <p>Here's what the state with \\(n_x = 2, n_y = 3\\) looks like:</p> <p></p> <p>Figure 1 (from [1]): A separable wave function. Red = positive, cyan = negative, black = zero. The 1D functions along the top and right show how this factors into \\(f(x) \\times g(y)\\).</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#example-2-a-non-separable-wave-function","title":"Example 2: A Non-Separable Wave Function","text":"<p>Now consider a superposition of two energy eigenstates:</p> \\[\\psi(x,y) = \\sin\\left(\\frac{2\\pi x}{L}\\right)\\sin\\left(\\frac{3\\pi y}{L}\\right) + \\frac{1}{2}\\sin\\left(\\frac{3\\pi x}{L}\\right)\\sin\\left(\\frac{2\\pi y}{L}\\right)\\] <p>Even though this is built from separable pieces, the sum itself is not separable - you cannot write it as a single function of x times a single function of y.</p> <p></p> <p>Figure 2 (from [1]): Non-separable wave functions. (a) A superposition of two states. (b) Superposition with three states. (c) A \"cat state\" with isolated peaks at opposite corners.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#how-to-see-the-difference-slice-the-wave-function","title":"How to See the Difference: Slice the Wave Function","text":"<p>The key to understanding separability is to slice the 2D wave function at different values of x and look at what y-dependence you get.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#for-a-separable-wave-function","title":"For a Separable Wave Function:","text":"<p>If you fix x at some value \\(x_0\\), you get:</p> \\[\\psi(x_0, y) = f(x_0) \\cdot g(y)\\] <p>The y-dependence is always \\(g(y)\\) - the same function shape - just scaled by the constant \\(f(x_0)\\).</p> <p>Key point: Different slices (different values of \\(x_0\\)) give the same y-pattern, just with different amplitudes.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#for-a-non-separable-wave-function","title":"For a Non-Separable Wave Function:","text":"<p>If you fix x at value \\(x_0\\), you get some function of y. But if you fix x at a different value \\(x_0'\\), you get a different shaped function of y.</p> <p>Key point: Different slices give different y-patterns - not just different amplitudes, but different shapes entirely.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#measuring-one-coordinate-affects-the-other","title":"Measuring One Coordinate Affects the Other","text":"<p>The physical difference shows up in measurements:</p> <p>Before measuring x, the probability distribution for y is:</p> \\[P(y) = \\int_0^L |\\psi(x,y)|^2 \\, dx\\] <p>Geometric interpretation: - This integral sums along a horizontal line at height y - You're adding up the brightness across the entire row - \\(P(y)\\) is literally the \"horizontal brightness profile\"</p> <p>Why this matters: - If a horizontal slice is mostly dark \u2192 the integral adds up small numbers \u2192 \\(P(y)\\) is small - If a horizontal slice is bright across many x-values \u2192 the integral adds up larger numbers \u2192 \\(P(y)\\) is large</p> <p>After measuring x and getting result \\(x_0\\), the wave function \"collapses\" to:</p> \\[P(y|x_0) = |\\psi(x_0, y)|^2\\] <p>Now you're looking at a vertical line at \\(x_0\\) - only that x-value survives.</p> <p>Quick summary: - Before x-measurement: \\(P(y) = \\int |\\psi(x,y)|^2 dx\\) \u2192 horizontal line \u2192 sum over all x - After x-measurement: \\(P(y|x_0) = |\\psi(x_0, y)|^2\\) \u2192 vertical line \u2192 only that x survives</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#example-the-non-separable-state","title":"Example: The Non-Separable State","text":"<p>For the wave function in Figure 2(a), here's what happens:</p> <p></p> <p>Figure 3 (from [1]): Probability distribution for measuring y. Green (solid) = before measuring x. Purple (dashed) = after measuring x and getting \\(x = L/4\\). The measurement changes the distribution!</p> <p>Before measuring x: The green curve shows three peaks with certain relative heights.</p> <p>After measuring x = L/4: The purple curve shows the same three peaks, but now the peak at \\(y = L/4\\) is much more likely, and the peak at \\(y = 3L/4\\) is much less likely.</p> <p>The measurements are correlated! Measuring x changes what we expect to see when we measure y.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#how-interactions-create-entanglement","title":"How Interactions Create Entanglement","text":"<p>So far we've seen examples of entangled states, but how do particles become entangled? The answer: interactions between particles.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#two-particles-with-repulsive-interaction-in-a-box-at-steady-state","title":"Two Particles with Repulsive Interaction in a box at steady state","text":"<p>Consider two particles in a 1D infinite square well that repel each other. The interaction potential is:</p> \\[V(x_1, x_2) = V_0 e^{-(x_2-x_1)^2/a^2}\\] <p>What this means physically:</p> <ul> <li>When \\(x_1 \\approx x_2\\) (particles close together) \u2192 exponential is large \u2192 strong repulsion</li> <li>When \\(x_1\\) and \\(x_2\\) are far apart \u2192 exponential is tiny \u2192 almost no repulsion</li> </ul> <p>The particles \"hate\" being near each other. Their lowest-energy arrangement keeps them far apart.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#reading-the-ground-state-wave-function","title":"Reading the Ground State Wave Function","text":"<p>Figure 5 (from [1]): Ground state wave function for two repelling particles in a 1D box (0 to L). Horizontal axis = position of particle 1 (\\(x_1\\)). Vertical axis = position of particle 2 (\\(x_2\\)). Bright = high probability, dark = low probability. \"Left\" = 0 &lt; x &lt; L/2, \"Right\" = L/2 &lt; x &lt; L.</p> <p>Important: This shows what the wave function looks like IF you prepare the system in the ground state. The Schr\u00f6dinger equation conserves energy - it doesn't push particles toward lower energy states. If you prepare particles in an excited state or superposition, they stay in that state (though superpositions will have oscillating phases). This ground state is just one possible stationary solution, not something the system naturally evolves into.</p> <p>Key features:</p> <ul> <li>The diagonal \\(x_1 = x_2\\) is dark - particles strongly avoid being in the same place (where repulsion is largest)</li> <li>Two bright peaks - when you measure the particles, you're most likely to find:<ul> <li>One particle on the left, the other on the right (top-left corner)</li> <li>One particle on the right, the other on the left (bottom-right corner)</li> </ul> </li> <li>These are exactly the arrangements where particles stay far apart!</li> </ul>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#dynamics-watching-entanglement-form-in-real-time","title":"Dynamics: Watching Entanglement Form in Real Time","text":"<p>The ground state above is stationary - the probability distribution doesn't change with time. But what if the particles aren't in an energy eigenstate? What if they're moving?</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#a-scattering-event","title":"A Scattering Event","text":"<p>Two equal-mass particles in 1D start as separated Gaussian wave packets moving toward each other. They interact via a short-range rectangular barrier:</p> \\[V(x_1, x_2) = \\begin{cases} V_0 &amp; \\text{if } |x_2 - x_1| &lt; a \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] <p>The parameters V\u2080 and a are chosen so transmission and reflection probabilities are approximately equal.</p> <p></p> <p>Figure 6 (from [1]): Time evolution of a two-particle scattering event. Three snapshots: Before, During, and After the interaction. The diagonal gray line shows where x\u2081 = x\u2082. Rainbow colors indicate the phase of the wave function (arrows show direction of motion). Brightness shows the magnitude (probability density).</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#understanding-configuration-space","title":"Understanding Configuration Space","text":"<p>Critical: Don't confuse the map with the territory</p> <p>This plot shows configuration space (x\u2081, x\u2082), not real space. Each point represents where BOTH particles are simultaneously.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#what-youre-seeing","title":"What you're seeing:","text":"<p>\u2022 Before: ONE blob in upper-left corner   - Particle 1 on left, particle 2 on right (definite configuration)   - In real space: two separated wave packets moving toward each other   - Separable (product) state</p> <p>\u2022 During: The wave packets collide and scatter via repulsion</p> <p>\u2022 After: TWO blobs appear   - Top-left blob: particle 1 left, particle 2 right (reflection - bounced off each other)   - Bottom-right blob: particle 1 right, particle 2 left (transmission - passed through)   - Why two blobs? When wave packets collide, there are two possible outcomes: they can bounce back (reflection) or pass through each other (transmission). Quantum mechanics keeps both outcomes in superposition - that's why you see two blobs instead of one   - The rainbow stripes show both blobs moving apart (arrows show direction of motion)   - Note: There are still tiny probability tails near the diagonal (where particles would be close together), but the probability is extremely small compared to the two dominant peaks</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#testing-for-entanglement-the-slicing-method","title":"Testing for Entanglement: The Slicing Method","text":""},{"location":"quantum-mechanics/quantum-spatial-entanglement/#the-before-state-is-not-entangled","title":"The \"Before\" state is NOT entangled","text":"<p>The Before state is separable: \u03c8(x\u2081,x\u2082) = \u03c8\u2081(x\u2081)\u00b7\u03c8\u2082(x\u2082). This means:</p> <p>\u2022 The slicing test: Draw a vertical line anywhere (fix x\u2081). The distribution you see for x\u2082 always has the same shape - just a different height \u2022 Measuring particle 1's position doesn't change the shape of particle 2's distribution \u2022 Geometrically: the blob is round/oval - not tilted, not split</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#the-after-state-is-entangled","title":"The \"After\" state IS entangled","text":"<p>The After state is entangled: \u03c8(x\u2081,x\u2082) \u2260 \u03c8\u2081(x\u2081)\u00b7\u03c8\u2082(x\u2082). Now:</p> <p>\u2022 The slicing test fails: Draw a vertical line on the left \u2192 you see a bump at the top (x\u2082 large). Draw one on the right \u2192 you see a bump at the bottom (x\u2082 small) \u2022 Measuring particle 1's position completely changes particle 2's distribution \u2022 Geometrically: the blob is split diagonally - two separate peaks along the anti-diagonal</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#summary-the-visual-test","title":"Summary: The visual test","text":"<p>The test for entanglement: Slide a vertical line across the plot. If every slice gives the same shape in x\u2082 \u2192 separable. If different slices give different shapes \u2192 entangled.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#why-fig-5-is-frozen-but-fig-6-evolves","title":"Why Fig. 5 is Frozen But Fig. 6 Evolves","text":"<p>Fig. 5 (ground state) is an energy eigenstate. Time evolution only adds a global phase: \u03a8(x\u2081,x\u2082,t) = e^(-iEt/\u210f)\u03a8(x\u2081,x\u2082,0). The phase factor cancels when you calculate |\u03a8|\u00b2, so the probability distribution never changes. The pattern is frozen.</p> <p>Fig. 6 (scattering) starts with moving Gaussian wave packets - NOT energy eigenstates. Each packet is a superposition of many momenta and energies. Different energy components oscillate at different rates, causing the packets to move, spread, collide, and interfere. Wave packets must evolve in time.</p> <p>The rainbow colors in Fig. 6 show the phase gradient (momentum direction). Arrows point in the direction of motion: initially toward each other, then apart after scattering.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#why-cant-we-see-this-in-real-space","title":"Why Can't We See This in Real Space?","text":"<p>In real 1D physical space, you always see two bumps (the two particles) - before, during, and after the collision. After scattering, both outcomes (reflection and transmission) look identical in real space: one bump moving left, one moving right. Real space cannot distinguish which particle is which or whether they bounced vs. passed through.</p> <p>Configuration space reveals what real space hides: The two blobs in Fig. 6 show that reflection and transmission exist in superposition. One blob represents \"particle 1 left, particle 2 right\" and the other represents \"particle 1 right, particle 2 left\" - these are correlated outcomes, not just two independent particles. This correlation is entanglement, and it's only visible when you plot both particle positions simultaneously in 2D configuration space.</p> <p>The key difference: Real space shows where particles are. Configuration space shows which combinations of positions are correlated - that's where entanglement lives.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#application-why-electrons-act-like-wave-packets-in-real-metals","title":"Application: Why Electrons Act Like Wave Packets in Real Metals","text":"<p>In a perfect crystal, electrons should be extended Bloch waves spread across the entire metal (like particle in a box eigenstates). But in real metals, we treat electrons as localized wave packets moving with definite velocities. Why?</p> <p>The answer: entanglement with heavy ions.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#configuration-space-picture","title":"Configuration Space Picture","text":"<p>Consider an electron-ion interaction in configuration space (x_electron, x_ion):</p> Text Only<pre><code>BEFORE INTERACTION:\nx_ion\n  ^\n  |\nx\u2080|======================  \u2190 long horizontal stripe\n  |                         (electron delocalized everywhere,\n  |                          ion localized at x\u2080)\n  +-----------------------&gt; x_electron\n\nAFTER INTERACTION:\nx_ion\n  ^\n  |\nx\u2080| ***          ***  \u2190 TWO short horizontal segments\n  |                    (electron localized into wave packets)\n  +-----------------------&gt; x_electron\n</code></pre> <p>What happened: The interaction V(x_e - x_ion) creates entanglement, just like in Fig. 6. The time-dependent Schr\u00f6dinger equation evolves the initial separable state into a non-separable superposition.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#why-heavy-ions-localize-electrons","title":"Why Heavy Ions Localize Electrons","text":"<p>Heavy ions have: - Tiny de Broglie wavelength (\u03bb = h/mv, huge m \u2192 tiny \u03bb) - Very sharp position localization - Essentially fixed position (act like delta functions)</p> <p>When the electron interacts with a well-localized ion, the conditional electron wavefunction \u03c8_e(x_e | x_ion \u2248 x\u2080) \u221d \u03a8(x_e, x\u2080) becomes a localized wave packet instead of an extended Bloch wave. This is exactly the \"slicing test\" from the paper: measuring (or knowing) one particle's position changes the other particle's distribution.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#repeated-scattering-creates-effective-wave-packets","title":"Repeated Scattering Creates Effective Wave Packets","text":"<p>In real metals, electrons constantly scatter off vibrating ions (phonons), impurities, and defects. Each interaction: - Creates entanglement between electron and ion - Destroys long-range coherence of the Bloch wave - Effectively localizes the electron over a finite region (mean free path)</p> <p>The result: Electrons behave as quasiparticle wave packets with finite spatial extent and definite momentum, not as extended eigenstates of the perfect crystal. This is why electron transport is diffusive (resistivity) rather than ballistic, and why we can think of electrons as localized packets moving through the metal.</p> <p>The heavy ion acts like a continuous measurement - not a sharp collapse, but decoherence through entanglement that gives electrons their packet-like character.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#connection-to-spin-systems","title":"Connection to Spin Systems","text":"<p>The same mathematics applies to two spin-1/2 particles. Instead of plotting \u03c8(x\u2081, x\u2082), you plot the four spin state amplitudes in a 2\u00d72 grid, where the axes represent the spin z-components (\u2191 or \u2193) of each particle.</p> <p></p> <p>Figure 7 (from [1]): 2\u00d72 density plots for singlet and triplet states of two spin-1/2 particles. The horizontal axis shows particle 1's spin (\u2212\u210f/2 or +\u210f/2), the vertical axis shows particle 2's spin. Bright colors show larger amplitudes, black shows zero. The singlet state (top) is entangled - measuring one spin immediately determines the other. The triplet states show different correlation patterns.</p> <p>The visual pattern is identical to the spatial case: entangled states show correlations (anti-diagonal pattern for singlet), while separable states have simpler structures. This correspondence helps students recognize entanglement across different quantum systems.</p>"},{"location":"quantum-mechanics/quantum-spatial-entanglement/#references","title":"References","text":"<p>[1] D. V. Schroeder, \"Entanglement isn't just for spin,\" Am. J. Phys., vol. 85, no. 11, pp. 812\u2013820, Nov. 2017, doi: 10.1119/1.5003808. Available: https://arxiv.org/abs/1703.10620</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/","title":"Spin, Entanglement, and Bell's Theorem","text":"<p>OUTDATED CONTENT</p> <p>This page contains draft/outdated material and is not currently maintained.</p> <p>Prerequisites: This builds on Multi-Particle Quantum Mechanics and The Pauli Exclusion Principle. Make sure you understand entangled states and identical particles before continuing!</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#from-position-to-spin-a-new-quantum-property","title":"From Position to Spin: A New Quantum Property","text":"<p>So far, we've focused on spatial entanglement (correlations in position). Now we'll introduce spin - an intrinsic quantum property that provides the cleanest examples of entanglement and led to experimental tests that shook our understanding of reality.</p> <p>Quick recap of what we know about entanglement:</p> <p>In Multi-Particle Quantum Mechanics, we learned that entangled states cannot be factored:</p> \\[ \\psi(x_1, x_2) \\neq \\psi_a(x_1) \\psi_b(x_2) \\] <p>And we saw that measuring one particle instantly affects the probability distribution of the other. Now we'll see this gets even stranger with spin...</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#spin-a-brief-introduction","title":"Spin: A Brief Introduction","text":"<p>Before we go further, we need to introduce spin (an intrinsic property of particles like electrons).</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#what-is-spin","title":"What Is Spin?","text":"<p>Spin is angular momentum that particles have even when sitting still. It's not actually spinning (electrons aren't little balls), but it acts like angular momentum in many ways.</p> <p>For electrons: Spin-1/2, meaning it comes in two states:</p> <ul> <li>Spin up: \\(|\\uparrow\\rangle\\) or \\(|+\\rangle\\)</li> <li>Spin down: \\(|\\downarrow\\rangle\\) or \\(|-\\rangle\\)</li> </ul> <p>These are the only two options. Measure electron spin along any axis (say, the z-axis) and you get one of these two results.</p> <p>Important: \"Up\" and \"down\" are relative to your measurement direction, not absolute directions in space! There's no universal \"up\" in the universe (is the center of the Milky Way \"down\"?). When we say \"spin up along the z-axis,\" we mean the spin is aligned with the direction you chose to call +z (your detector's orientation). \"Spin down\" means opposite to that direction. You're free to choose any axis you want\u2014the labels just tell you whether the spin points parallel or antiparallel to your chosen measurement direction.</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#spin-states-as-vectors","title":"Spin States as Vectors","text":"<p>We can represent spin states as column vectors in a 2D Hilbert space:</p> \\[ |\\uparrow\\rangle = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad |\\downarrow\\rangle = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\] <p>A general spin state is a superposition:</p> \\[ |\\psi\\rangle = \\alpha|\\uparrow\\rangle + \\beta|\\downarrow\\rangle = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix} \\] <p>where \\(|\\alpha|^2 + |\\beta|^2 = 1\\).</p> <p>Measurement: If you measure spin along the z-axis, you get:</p> <ul> <li>\\(|\\uparrow\\rangle\\) with probability \\(|\\alpha|^2\\)</li> <li>\\(|\\downarrow\\rangle\\) with probability \\(|\\beta|^2\\)</li> </ul>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#complete-state-spatial-spin","title":"Complete State: Spatial + Spin","text":"<p>An electron has two properties: where it is (position) and which way it's spinning (spin). The complete quantum state must specify both!</p> <p>You still have ONE quantum state, but now it has two components (because spin can be \u2191 or \u2193).</p> <p>Analogy: A 2D velocity vector \\(\\vec{v} = (v_x, v_y)\\) is ONE vector, but it has two components (x-component and y-component). Similarly, the electron's quantum state is ONE state, but it has two spin components.</p> <p>The complete state is written:</p> \\[ \\Psi(x) = \\psi_{\\uparrow}(x)|\\uparrow\\rangle + \\psi_{\\downarrow}(x)|\\downarrow\\rangle \\] <p>Wait, what's going on here? Are the kets column vectors? Are they being multiplied?</p> <p>Yes! Let me unpack this confusing notation. The kets ARE still column vectors:</p> \\[ |\\uparrow\\rangle = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad |\\downarrow\\rangle = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\] <p>And \\(\\psi_\\uparrow(x)\\) and \\(\\psi_\\downarrow(x)\\) are just scalar functions (regular numbers that depend on \\(x\\)).</p> <p>So at each position \\(x\\), you multiply scalars times vectors:</p> \\[ \\Psi(x) = \\psi_{\\uparrow}(x) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\psi_{\\downarrow}(x) \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\psi_{\\uparrow}(x) \\\\ \\psi_{\\downarrow}(x) \\end{pmatrix} \\] <p>Key insight: At each position \\(x\\), the wavefunction \u03a8(x) is a 2-component column vector! The top component is \u03c8\u2191(x) and the bottom component is \u03c8\u2193(x).</p> <p>What are these components?</p> <ul> <li>\\(\\psi_\\uparrow(x)\\) = amplitude for electron to be at position \\(x\\) with spin up</li> <li>\\(\\psi_\\downarrow(x)\\) = amplitude for electron to be at position \\(x\\) with spin down</li> </ul> <p>This is the ONE complete wavefunction\u2014it just has two components (a 2D spinor) at each position \\(x\\)!</p> <p>What this means:</p> <ul> <li>If you measure spin and get \u2191, the electron's position is described by \\(\\psi_\\uparrow(x)\\)</li> <li>If you measure spin and get \u2193, the electron's position is described by \\(\\psi_\\downarrow(x)\\)</li> <li>Before measurement, the electron is in a superposition of both spin states</li> </ul> <p>Important: Renormalization after measurement!</p> <p>Before measurement, the full state is normalized:</p> \\[ \\int \\left[|\\psi_\\uparrow(x)|^2 + |\\psi_\\downarrow(x)|^2\\right] dx = 1 \\] <p>But \\(\\int |\\psi_\\uparrow(x)|^2 dx\\) alone equals the probability of measuring spin up (call it \\(P_\\uparrow\\)), which might be less than 1!</p> <p>After you measure spin up, the state collapses to just the spin-up component, but you need to renormalize it:</p> \\[ \\psi_\\uparrow(x) \\to \\frac{\\psi_\\uparrow(x)}{\\sqrt{P_\\uparrow}} \\] <p>Now \\(\\int \\left|\\frac{\\psi_\\uparrow(x)}{\\sqrt{P_\\uparrow}}\\right|^2 dx = \\frac{P_\\uparrow}{P_\\uparrow} = 1\\) \u2713</p> <p>This ensures the collapsed state is still properly normalized (total probability = 1).</p> <p>Probability interpretation:</p> <ul> <li>\\(|\\psi_\\uparrow(x)|^2 dx\\) = probability of finding electron at position \\(x\\) with spin up</li> <li>\\(|\\psi_\\downarrow(x)|^2 dx\\) = probability of finding electron at position \\(x\\) with spin down</li> </ul> <p>The ket notation \\(|\\uparrow\\rangle\\) and \\(|\\downarrow\\rangle\\) is just a compact way to label which spin component you're talking about.</p> <p>Key point: Spin adds an extra \"dimension\" to the wavefunction. Before spin: \u03c8(x) was one number at each position. With spin: \u03a8(x) has two numbers at each position (one for spin-up, one for spin-down). It's still ONE quantum state\u2014just with more structure!</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#two-electrons-position-and-spin","title":"Two Electrons: Position AND Spin","text":"<p>For two electrons in 1D, you have four possible spin combinations. The complete state is actually:</p> \\[ \\Psi(x_1, x_2) = \\psi_{\\uparrow\\uparrow}(x_1, x_2)|\\uparrow\\uparrow\\rangle + \\psi_{\\uparrow\\downarrow}(x_1, x_2)|\\uparrow\\downarrow\\rangle + \\psi_{\\downarrow\\uparrow}(x_1, x_2)|\\downarrow\\uparrow\\rangle + \\psi_{\\downarrow\\downarrow}(x_1, x_2)|\\downarrow\\downarrow\\rangle \\] <p>So yes, it's a column vector with FOUR elements, each multiplied by a spatial wavefunction!</p> <p>At each pair of positions \\((x_1, x_2)\\), the state is:</p> \\[ \\Psi(x_1, x_2) = \\begin{pmatrix} \\psi_{\\uparrow\\uparrow}(x_1, x_2) \\\\ \\psi_{\\uparrow\\downarrow}(x_1, x_2) \\\\ \\psi_{\\downarrow\\uparrow}(x_1, x_2) \\\\ \\psi_{\\downarrow\\downarrow}(x_1, x_2) \\end{pmatrix} \\] <p>What these mean:</p> <ul> <li>\\(\\psi_{\\uparrow\\uparrow}(x_1, x_2)\\) = amplitude for electron 1 at \\(x_1\\) with spin up AND electron 2 at \\(x_2\\) with spin up</li> <li>\\(\\psi_{\\uparrow\\downarrow}(x_1, x_2)\\) = amplitude for electron 1 at \\(x_1\\) with spin up AND electron 2 at \\(x_2\\) with spin down</li> <li>And so on...</li> </ul> <p>Compact notation: We often write this as \\(\\psi(x_1, x_2)|s_1, s_2\\rangle\\) where \\(s_1, s_2 \\in \\{\\uparrow, \\downarrow\\}\\), but that's hiding the fact that there are really four different spatial wavefunctions, one for each spin combination!</p> <p>Important simplification: Often the spin and spatial parts are separable, meaning we can write:</p> \\[ \\Psi(x_1, x_2) = \\psi_{\\text{spatial}}(x_1, x_2) \\times |\\text{spin state}\\rangle \\] <p>For example: \\(\\psi(x_1, x_2)|\\uparrow\\downarrow\\rangle\\) means the same spatial wavefunction \u03c8(x\u2081, x\u2082) applies, with electron 1 having spin up and electron 2 having spin down. This is much simpler and is what we'll use for most of the document!</p> <p>We'll return to spin in much more detail when we discuss Bell's theorem!</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#spin-entanglement-and-the-epr-paradox","title":"Spin Entanglement and the EPR Paradox","text":"<p>Now we're ready for the truly weird stuff: entangled spin states and the EPR paradox that troubled Einstein.</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#the-singlet-state","title":"The Singlet State","text":"<p>The most famous entangled state is the spin singlet:</p> \\[ |\\psi\\rangle = \\frac{1}{\\sqrt{2}}(|\\uparrow\\downarrow\\rangle - |\\downarrow\\uparrow\\rangle) \\] <p>What this means:</p> <ul> <li>Particle 1 spin up AND particle 2 spin down</li> <li>minus particle 1 spin down AND particle 2 spin up</li> </ul> <p>This is a superposition of the two possibilities! You don't know which is which until you measure.</p> <p>Key property: The two spins are anticorrelated. If you measure particle 1 and find spin up, particle 2 is definitely spin down (and vice versa).</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#the-epr-setup-1935","title":"The EPR Setup (1935)","text":"<p>Einstein, Podolsky, and Rosen proposed this thought experiment:</p> <p>Step 1: Create two particles in the singlet state.</p> <p>Step 2: Separate them by a huge distance (say, light-years apart).</p> <p>Step 3: Alice measures particle 1's spin. Bob measures particle 2's spin.</p> <p>What happens:</p> <p>\u2022 Alice measures particle 1 \u2192 finds spin up \u2022 Instantly, particle 2's state becomes spin down \u2022 Bob measures particle 2 \u2192 finds spin down (guaranteed!)</p> <p>They're perfectly anticorrelated, no matter how far apart!</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#einsteins-objection-spooky-action-at-a-distance","title":"Einstein's Objection: \"Spooky Action at a Distance\"","text":"<p>Einstein was deeply troubled by this:</p> <p>Einstein's argument:</p> <ol> <li>The particles are far apart (space-like separated)</li> <li>Special relativity says no signal can travel faster than light</li> <li>Yet Alice's measurement instantly determines Bob's result</li> <li>How can particle 2 \"know\" what Alice measured?</li> </ol> <p>Einstein's conclusion: Quantum mechanics must be incomplete. The particles must have carried predetermined values with them (hidden variables that quantum mechanics doesn't describe).</p> <p>Local realism: Each particle has a definite (but hidden) spin value from the moment they separate. Measurements just reveal pre-existing properties.</p> <p>This seems reasonable! Like drawing colored balls from a hat (they had their colors all along).</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#the-quantum-response","title":"The Quantum Response","text":"<p>Quantum mechanics says: No! The particles don't have definite spins until measured. The singlet state is the complete description.</p> <p>Before measurement:</p> <ul> <li>Particle 1 is in superposition: \\(\\frac{1}{\\sqrt{2}}(|\\uparrow\\rangle - |\\downarrow\\rangle)\\) (if you ignore particle 2)</li> <li>Particle 2 is in superposition: \\(\\frac{1}{\\sqrt{2}}(|\\downarrow\\rangle - |\\uparrow\\rangle)\\) (if you ignore particle 1)</li> <li>But they're entangled (the correlations are built into the joint state)</li> </ul> <p>After Alice measures:</p> <ul> <li>The entangled state \"collapses\"</li> <li>Bob's particle is now in a definite state (opposite to Alice's result)</li> </ul> <p>Is this spooky action at a distance?</p> <p>Sort of! The correlations are nonlocal. But you can't use this to send signals faster than light (no information is transmitted by the collapse).</p> <p>Who's right: Einstein or quantum mechanics?</p> <p>For 30 years this was just philosophy. Then John Bell figured out how to test it experimentally...</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#bells-theorem-deriving-the-inequality","title":"Bell's Theorem: Deriving the Inequality","text":"<p>John Bell (1964) showed that local hidden variable theories make different predictions than quantum mechanics. We can test who's right!</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#the-setup","title":"The Setup","text":"<p>Instead of just measuring spin along one axis (say, z-axis), Alice and Bob can choose different measurement directions.</p> <p>Measurement directions:</p> <ul> <li>Alice chooses angle \\(\\theta_A\\) (direction to measure spin)</li> <li>Bob chooses angle \\(\\theta_B\\)</li> <li>Each gets result: \\(\\pm 1\\) (spin up/down along that axis)</li> </ul> <p>Correlation function: Average product of their results:</p> \\[ E(\\theta_A, \\theta_B) = \\langle A \\cdot B \\rangle \\] <p>where \\(A, B \\in \\{+1, -1\\}\\).</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#local-hidden-variables-prediction","title":"Local Hidden Variables Prediction","text":"<p>Assumption: Each particle carries hidden variable \\(\\lambda\\) that predetermines all measurement outcomes.</p> <p>For any \\(\\lambda\\):</p> <ul> <li>Alice's result is \\(A(\\theta_A, \\lambda) = \\pm 1\\)</li> <li>Bob's result is \\(B(\\theta_B, \\lambda) = \\pm 1\\)</li> </ul> <p>These are predetermined functions (the results exist before measurement!).</p> <p>Averaging over all possible \\(\\lambda\\) (with probability distribution \\(\\rho(\\lambda)\\)):</p> \\[ E(\\theta_A, \\theta_B) = \\int A(\\theta_A, \\lambda) B(\\theta_B, \\lambda) \\rho(\\lambda) d\\lambda \\]"},{"location":"quantum-mechanics/quantum-spin-entanglement/#the-chsh-inequality","title":"The CHSH Inequality","text":"<p>Consider four measurement combinations with angles \\(a, a', b, b'\\):</p> \\[ S = E(a,b) - E(a,b') + E(a',b) + E(a',b') \\] <p>Bell's mathematical result: For any local hidden variable theory:</p> \\[ |S| \\leq 2 \\] <p>This is called the CHSH inequality (Clauser-Horne-Shimony-Holt, a version of Bell's original).</p> <p>Proof sketch: Since \\(A, B \\in \\{-1, +1\\}\\), for any fixed \\(\\lambda\\):</p> \\[ A(a,\\lambda)[B(b,\\lambda) - B(b',\\lambda)] + A(a',\\lambda)[B(b,\\lambda) + B(b',\\lambda)] \\] <p>Notice: \\(B(b) - B(b') \\in \\{-2, 0, +2\\}\\) and \\(B(b) + B(b') \\in \\{-2, 0, +2\\}\\).</p> <p>When \\(B(b) - B(b') = 0\\), we have \\(B(b) = B(b')\\), so \\(|B(b) + B(b')| = 2\\). When \\(B(b) + B(b') = 0\\), we have \\(B(b) = -B(b')\\), so \\(|B(b) - B(b')| = 2\\).</p> <p>In either case: \\(|A(a)[B(b)-B(b')]| + |A(a')[B(b)+B(b')]| \\leq 2\\) (since \\(|A| = 1\\)).</p> <p>Taking absolute value before averaging over \\(\\lambda\\) gives \\(|S| \\leq 2\\).</p> <p>Interpretation: Local realism constrains the correlations. You can't get too correlated across different measurement angles.</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#quantum-mechanics-violates-it","title":"Quantum Mechanics Violates It!","text":"<p>For the singlet state \\(\\frac{1}{\\sqrt{2}}(|\\uparrow\\downarrow\\rangle - |\\downarrow\\uparrow\\rangle)\\), quantum mechanics predicts:</p> \\[ E(\\theta_A, \\theta_B) = -\\cos(\\theta_A - \\theta_B) \\] <p>The correlation depends on the relative angle between measurement directions!</p> <p>Optimal angle choices: \\(a = 0\u00b0\\), \\(a' = 90\u00b0\\), \\(b = 45\u00b0\\), \\(b' = 135\u00b0\\)</p> <p>Calculate each term:</p> <ul> <li>\\(E(a,b) = E(0\u00b0, 45\u00b0) = -\\cos(45\u00b0) = -\\frac{1}{\\sqrt{2}}\\)</li> <li>\\(E(a,b') = E(0\u00b0, 135\u00b0) = -\\cos(135\u00b0) = +\\frac{1}{\\sqrt{2}}\\)</li> <li>\\(E(a',b) = E(90\u00b0, 45\u00b0) = -\\cos(45\u00b0) = -\\frac{1}{\\sqrt{2}}\\)</li> <li>\\(E(a',b') = E(90\u00b0, 135\u00b0) = -\\cos(45\u00b0) = -\\frac{1}{\\sqrt{2}}\\)</li> </ul> \\[ S = -\\frac{1}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}} = -\\frac{4}{\\sqrt{2}} = -2\\sqrt{2} \\approx -2.83 \\] <p>Result: \\(|S| = 2.83 &gt; 2\\) (Bell's inequality is violated!)</p> <p>Quantum mechanics predicts correlations that are impossible for any local hidden variable theory.</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#what-this-means","title":"What This Means","text":"<p>The verdict:</p> <ul> <li>\u2717 Local realism is wrong (Nature doesn't have predetermined values)</li> <li>\u2713 Quantum mechanics is right (The correlations are genuinely nonlocal)</li> </ul> <p>But wait: How do we know quantum mechanics is right? We need to do the experiment!</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#experimental-tests-bells-inequality-in-the-lab","title":"Experimental Tests: Bell's Inequality in the Lab","text":"<p>Bell's theorem moved entanglement from philosophy to experimental science.</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#aspects-experiments-1981-1982","title":"Aspect's Experiments (1981-1982)","text":"<p>Alain Aspect and collaborators performed the definitive early tests:</p> <p>Setup:</p> <ul> <li>Source produces entangled photon pairs (polarization-entangled, analogous to spin-entangled electrons)</li> <li>Photons sent to Alice and Bob (13 meters apart)</li> <li>Each measures polarization along chosen angle</li> <li>Record correlations for different angle combinations</li> </ul> <p>Key innovation: Rapidly switching measurement angles while photons are in flight \u2192 ensures measurements are space-like separated (no communication possible between detectors).</p> <p>Result:</p> \\[ S_{\\text{measured}} = 2.70 \\pm 0.05 \\] <p>Violates Bell's inequality! Quantum mechanics wins, local realism loses.</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#loopholes-and-how-they-were-closed","title":"Loopholes and How They Were Closed","text":"<p>Early experiments had potential loopholes Einstein fans could exploit:</p> <p>1. Detection loophole:</p> <ul> <li>Not all photons are detected (typical efficiency ~20-30%)</li> <li>Maybe hidden variables cleverly avoid being measured?</li> <li>Closed: Use more efficient detectors (&gt;80% efficiency)</li> </ul> <p>2. Locality loophole:</p> <ul> <li>Detectors not far enough apart (maybe communication possible?)</li> <li>Closed: Space-like separated measurements (detection events outside each other's light cones)</li> </ul> <p>3. Freedom-of-choice loophole:</p> <ul> <li>Measurement angles chosen by pseudo-random number generators</li> <li>Maybe hidden variables determine both outcomes and measurement choices?</li> <li>Closed: Use cosmic photons or human choice to set angles</li> </ul>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#loophole-free-experiments-2015","title":"Loophole-Free Experiments (2015)","text":"<p>Three independent groups simultaneously closed all major loopholes:</p> <p>\u2022 Delft (Hanson et al.): Entangled electron spins in diamond, 1.3 km separation</p> <p>\u2022 Vienna (Zeilinger et al.): Entangled photons, 60 meters, &gt;75% efficiency detectors</p> <p>\u2022 NIST (Shalm et al.): Entangled photons, efficient detectors, space-like separation</p> <p>All found: Clear violation of Bell's inequality with all loopholes closed!</p> \\[ |S| &gt; 2 \\quad \\text{(with high statistical significance)} \\]"},{"location":"quantum-mechanics/quantum-spin-entanglement/#the-2022-nobel-prize","title":"The 2022 Nobel Prize","text":"<p>The Nobel Prize in Physics was awarded to:</p> <p>\u2022 Alain Aspect (France): pioneering photon entanglement experiments</p> <p>\u2022 John Clauser (USA): first experimental test of Bell's inequality (1972)</p> <p>\u2022 Anton Zeilinger (Austria): quantum information and loophole-free tests</p> <p>The citation: \"for experiments with entangled photons, establishing the violation of Bell inequalities and pioneering quantum information science\"</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#what-weve-learned","title":"What We've Learned","text":"<p>Nature is fundamentally nonlocal:</p> <ul> <li>Measurement outcomes at one location are instantaneously correlated with distant measurements</li> <li>This isn't due to hidden variables or communication</li> <li>It's genuine quantum entanglement</li> </ul> <p>But information still can't travel faster than light:</p> <ul> <li>Alice's individual results look random (50% up, 50% down)</li> <li>Only when Alice and Bob compare results do they see the correlations</li> <li>Comparing requires classical communication (limited by speed of light)</li> </ul> <p>Einstein was wrong, but in the most interesting way:</p> <ul> <li>Quantum mechanics really is complete (no hidden variables needed)</li> <li>\"Spooky action at a distance\" is real!</li> <li>But it doesn't violate relativity</li> </ul> <p>The universe is even weirder than Einstein thought!</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#matrix-representation-for-discrete-systems","title":"Matrix Representation for Discrete Systems","text":"<p>For systems with discrete spin states, we can represent everything using matrices (making entanglement very concrete).</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#single-spin-2d-hilbert-space","title":"Single Spin: 2D Hilbert Space","text":"<p>A single spin-1/2 particle lives in a 2D Hilbert space:</p> <p>Basis states:</p> \\[ |\\uparrow\\rangle = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad |\\downarrow\\rangle = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\] <p>General state:</p> \\[ |\\psi\\rangle = \\alpha|\\uparrow\\rangle + \\beta|\\downarrow\\rangle = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix} \\]"},{"location":"quantum-mechanics/quantum-spin-entanglement/#two-spins-4d-hilbert-space-tensor-product","title":"Two Spins: 4D Hilbert Space (Tensor Product)","text":"<p>With two particles, the Hilbert space is the tensor product: \\(\\mathcal{H}_1 \\otimes \\mathcal{H}_2\\).</p> <p>Dimension: \\(2 \\times 2 = 4\\) (dimension multiplies!)</p> <p>Basis states:</p> \\[ |\\uparrow\\uparrow\\rangle = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad |\\uparrow\\downarrow\\rangle = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad |\\downarrow\\uparrow\\rangle = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad |\\downarrow\\downarrow\\rangle = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\] <p>Ordering convention: First entry is particle 1, second is particle 2.</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#product-states-vs-entangled-states","title":"Product States vs Entangled States","text":"<p>Product state example: \\(|\\uparrow\\downarrow\\rangle\\)</p> \\[ |\\uparrow\\downarrow\\rangle = |\\uparrow\\rangle \\otimes |\\downarrow\\rangle = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\otimes \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\] <p>This is separable (particle 1 is definitely up, particle 2 is definitely down).</p> <p>Singlet state (entangled):</p> \\[ |\\psi_{\\text{singlet}}\\rangle = \\frac{1}{\\sqrt{2}}(|\\uparrow\\downarrow\\rangle - |\\downarrow\\uparrow\\rangle) = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\\\ 0 \\end{pmatrix} \\] <p>Check if separable: Can we write this as \\((\\alpha_1|\\uparrow\\rangle + \\beta_1|\\downarrow\\rangle) \\otimes (\\alpha_2|\\uparrow\\rangle + \\beta_2|\\downarrow\\rangle)\\)?</p> <p>Expanding the tensor product:</p> \\[ = \\alpha_1\\alpha_2|\\uparrow\\uparrow\\rangle + \\alpha_1\\beta_2|\\uparrow\\downarrow\\rangle + \\beta_1\\alpha_2|\\downarrow\\uparrow\\rangle + \\beta_1\\beta_2|\\downarrow\\downarrow\\rangle \\] <p>For the singlet: coefficient of \\(|\\uparrow\\uparrow\\rangle\\) is 0, so \\(\\alpha_1\\alpha_2 = 0\\).</p> <ul> <li>If \\(\\alpha_1 = 0\\): then coefficient of \\(|\\uparrow\\downarrow\\rangle\\) is 0, so \\(\\alpha_1\\beta_2 = 0\\) \u2713</li> <li>But then coefficient of \\(|\\downarrow\\uparrow\\rangle\\) is \\(\\beta_1\\alpha_2\\), which must equal \\(-1/\\sqrt{2}\\)</li> <li>And coefficient of \\(|\\downarrow\\downarrow\\rangle\\) is \\(\\beta_1\\beta_2\\), which must equal \\(0\\)</li> </ul> <p>So \\(\\beta_2 = 0\\) (to make last term zero), but then \\(\\beta_1\\alpha_2 = -1/\\sqrt{2}\\) requires \\(\\alpha_2 \\neq 0\\)... but we needed \\(\\alpha_1\\alpha_2 = 0\\) with \\(\\alpha_1 = 0\\) \u2713, giving no constraint on \\(\\alpha_2\\).</p> <p>Actually, let's be more careful: if \\(\\alpha_1 = 0\\), then \\(\\beta_1 \\neq 0\\) (normalized). Then \\(\\beta_1\\beta_2 = 0\\) means \\(\\beta_2 = 0\\). But \\(\\alpha_2\\) must be normalized: \\(|\\alpha_2|^2 + |\\beta_2|^2 = 1\\), so \\(\\alpha_2 = e^{i\\phi}\\) for some phase.</p> <p>Then coefficient of \\(|\\downarrow\\uparrow\\rangle\\) is \\(\\beta_1 e^{i\\phi} = -1/\\sqrt{2}\\). And coefficient of \\(|\\uparrow\\downarrow\\rangle\\) should be \\(1/\\sqrt{2}\\), but \\(\\alpha_1\\beta_2 = 0 \\cdot 0 = 0 \\neq 1/\\sqrt{2}\\). Contradiction!</p> <p>Result: The singlet cannot be factored into a product state (it's genuinely entangled!).</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#measuring-entanglement-example-calculation","title":"Measuring Entanglement: Example Calculation","text":"<p>Let's calculate what happens when Alice measures her spin in the singlet state.</p> <p>Before measurement:</p> \\[ |\\psi\\rangle = \\frac{1}{\\sqrt{2}}(|\\uparrow\\downarrow\\rangle - |\\downarrow\\uparrow\\rangle) \\] <p>Alice measures particle 1 along z-axis:</p> <p>Case 1: Alice gets \\(\\uparrow\\)</p> <p>The state collapses to the part with particle 1 = \\(|\\uparrow\\rangle\\):</p> \\[ |\\psi_{\\text{after}}\\rangle = |\\uparrow\\downarrow\\rangle \\] <p>Bob's particle is definitely \\(|\\downarrow\\rangle\\)!</p> <p>Case 2: Alice gets \\(\\downarrow\\)</p> <p>The state collapses to:</p> \\[ |\\psi_{\\text{after}}\\rangle = -|\\downarrow\\uparrow\\rangle \\] <p>Bob's particle is definitely \\(|\\uparrow\\rangle\\)!</p> <p>(The minus sign is an overall phase, physically irrelevant for Bob's local measurements.)</p> <p>Probability: Each outcome occurs with 50% probability:</p> \\[ P(\\uparrow) = \\left|\\frac{1}{\\sqrt{2}}\\right|^2 = \\frac{1}{2}, \\quad P(\\downarrow) = \\left|\\frac{1}{\\sqrt{2}}\\right|^2 = \\frac{1}{2} \\] <p>Key insight: Alice's measurement instantaneously determines Bob's state, even though her result was random (50/50)!</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#operators-as-matrices","title":"Operators as Matrices","text":"<p>For a single spin, the z-component spin operator is:</p> \\[ \\hat{S}_z = \\frac{\\hbar}{2}\\begin{pmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{pmatrix} \\] <p>Eigenvalues: \\(+\\hbar/2\\) (spin up) and \\(-\\hbar/2\\) (spin down).</p> <p>For two spins, we can measure correlations. The operator \\(\\hat{S}_{1z} \\otimes \\hat{S}_{2z}\\) (product of individual spin measurements) becomes a 4\u00d74 matrix:</p> \\[ \\hat{S}_{1z} \\otimes \\hat{S}_{2z} = \\frac{\\hbar^2}{4}\\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix} \\] <p>Calculate expected value for singlet state:</p> \\[ \\langle \\hat{S}_{1z} \\otimes \\hat{S}_{2z} \\rangle = \\langle\\psi|\\hat{S}_{1z} \\otimes \\hat{S}_{2z}|\\psi\\rangle \\] \\[ = \\frac{1}{2}\\begin{pmatrix} 0 &amp; 1 &amp; -1 &amp; 0 \\end{pmatrix} \\frac{\\hbar^2}{4}\\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\\\ 0 \\end{pmatrix} \\] \\[ = \\frac{\\hbar^2}{8}\\begin{pmatrix} 0 &amp; 1 &amp; -1 &amp; 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\frac{\\hbar^2}{8}(-1 - 1) = -\\frac{\\hbar^2}{4} \\] <p>Result: Negative correlation! When particle 1 is up, particle 2 tends to be down (and vice versa).</p> <p>For a product state like \\(|\\uparrow\\uparrow\\rangle\\), you'd get \\(+\\hbar^2/4\\) (positive correlation).</p> <p>The singlet has maximum anticorrelation (the defining feature of this entangled state!).</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#summary-the-big-picture","title":"Summary: The Big Picture","text":"<p>We've journeyed through spin, entanglement, and the experimental tests that revealed the quantum nature of reality:</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#spin-as-a-quantum-property","title":"Spin as a Quantum Property","text":"<p>Intrinsic angular momentum: Electrons have spin-1/2 (two states: \u2191 and \u2193)</p> <p>Complete state: Must specify both position AND spin: \\(\\Psi(x) = \\psi_\\uparrow(x)|\\uparrow\\rangle + \\psi_\\downarrow(x)|\\downarrow\\rangle\\)</p> <p>Two particles: 4D spin space (\u2191\u2191, \u2191\u2193, \u2193\u2191, \u2193\u2193)</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#the-epr-paradox","title":"The EPR Paradox","text":"<p>Singlet state: \\(\\frac{1}{\\sqrt{2}}(|\\uparrow\\downarrow\\rangle - |\\downarrow\\uparrow\\rangle)\\) - perfectly anticorrelated</p> <p>Einstein's objection: Measuring one particle instantly affects the other \u2192 must be hidden variables!</p> <p>Quantum response: No hidden variables - entanglement is real!</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#bells-theorem","title":"Bell's Theorem","text":"<p>Key insight: Local hidden variables predict \\(|S| \\leq 2\\)</p> <p>Quantum mechanics: Predicts \\(|S| = 2\\sqrt{2} \\approx 2.83\\) (violates Bell's inequality!)</p> <p>This is testable: Different theories make different experimental predictions!</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#experimental-verification","title":"Experimental Verification","text":"<p>Early tests (1972-1982): Clauser, Aspect - quantum mechanics wins!</p> <p>Loophole-free (2015): All major loopholes closed - entanglement is real!</p> <p>2022 Nobel Prize: Awarded for establishing quantum nonlocality</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#what-it-all-means","title":"What It All Means","text":"<p>Nature is nonlocal: - Correlations are instantaneous - Not due to hidden variables - Genuine quantum entanglement</p> <p>But no faster-than-light communication: - Individual results are random - Only comparison reveals correlations - Information limited by speed of light</p> <p>Implications: - Quantum mechanics is complete - Reality is fundamentally different from classical intuition - Entanglement enables quantum technologies</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#applications-and-future","title":"Applications and Future","text":"<p>Entanglement isn't just weird\u2014it's useful:</p> <p>\u2022 Quantum computing: Entangled qubits for parallel computation</p> <p>\u2022 Quantum cryptography: Unhackable communication using entangled photons</p> <p>\u2022 Quantum teleportation: Transfer quantum states using entanglement</p> <p>\u2022 Quantum sensing: Entanglement-enhanced measurements</p> <p>The strangeness that troubled Einstein is now the foundation of quantum information science!</p>"},{"location":"quantum-mechanics/quantum-spin-entanglement/#further-reading","title":"Further Reading","text":"<p>Bell's original papers:</p> <ul> <li>\"On the Einstein Podolsky Rosen Paradox\" (1964): the groundbreaking theorem</li> <li>\"Bertlmann's socks and the nature of reality\" (1981): wonderfully accessible explanation</li> </ul> <p>Classic experiments:</p> <ul> <li>Aspect, Dalibard, Roger, \"Experimental test of Bell's inequalities...\" (1982)</li> <li>Hensen et al., \"Loophole-free Bell inequality violation...\" (2015)</li> </ul> <p>Books:</p> <ul> <li>\"Quantum Computation and Quantum Information\" (Nielsen &amp; Chuang)</li> <li>\"Entanglement\" (Amir Aczel, popular science)</li> </ul> <p>Next steps:</p> <ul> <li>Density matrices (describing mixed states and partial information)</li> <li>Quantum information theory (quantifying entanglement)</li> <li>Many-body quantum mechanics (atoms, molecules, solids)</li> </ul>"},{"location":"quantum-mechanics/quantum-velocity/","title":"Velocity in Quantum Mechanics: A Complete Tutorial","text":""},{"location":"quantum-mechanics/quantum-velocity/#introduction-why-is-velocity-confusing-in-quantum-mechanics","title":"Introduction: Why Is Velocity Confusing in Quantum Mechanics?","text":"<p>In classical mechanics, velocity is simple: \\(v = dx/dt\\), a single number telling you how fast something moves. In quantum mechanics, things get messier because particles don't have definite positions or momenta\u2014they have wavefunctions. This leads to multiple velocity concepts, each answering a different question.</p> <p>This tutorial will give you the intuition to immediately recognize which velocity concept applies to any situation. We'll derive everything from first principles so you understand where each definition comes from, not just memorize formulas.</p>"},{"location":"quantum-mechanics/quantum-velocity/#part-1-the-foundationehrenfests-theorem","title":"Part 1: The Foundation\u2014Ehrenfest's Theorem","text":"<p>Everything starts here. Ehrenfest's theorem tells us how expectation values evolve in time, bridging quantum mechanics to classical intuition.</p>"},{"location":"quantum-mechanics/quantum-velocity/#starting-point-the-schrodinger-equation","title":"Starting Point: The Schr\u00f6dinger Equation","text":"<p>The time-dependent Schr\u00f6dinger equation governs quantum evolution:</p> \\[i\\hbar \\frac{\\partial \\psi}{\\partial t} = \\hat{H}\\psi\\] <p>Taking the complex conjugate (and using that \\(\\hat{H}\\) is Hermitian, so \\(\\hat{H}^\\dagger = \\hat{H}\\)):</p> \\[-i\\hbar \\frac{\\partial \\psi^*}{\\partial t} = \\hat{H}\\psi^*\\]"},{"location":"quantum-mechanics/quantum-velocity/#deriving-the-general-form","title":"Deriving the General Form","text":"<p>For any operator \\(\\hat{A}\\), the expectation value is:</p> \\[\\langle A \\rangle = \\int \\psi^* \\hat{A} \\psi \\, dx\\] <p>Taking the time derivative (using the product rule on three terms):</p> \\[\\frac{d\\langle A \\rangle}{dt} = \\int \\left(\\frac{\\partial \\psi^*}{\\partial t} \\hat{A} \\psi + \\psi^* \\hat{A} \\frac{\\partial \\psi}{\\partial t} + \\psi^* \\frac{\\partial \\hat{A}}{\\partial t} \\psi \\right) dx\\] <p>Substituting from the Schr\u00f6dinger equation:</p> \\[\\frac{d\\langle A \\rangle}{dt} = \\int \\left(\\frac{-\\hat{H}\\psi^*}{i\\hbar} \\hat{A} \\psi + \\psi^* \\hat{A} \\frac{\\hat{H}\\psi}{i\\hbar} + \\psi^* \\frac{\\partial \\hat{A}}{\\partial t} \\psi \\right) dx\\] <p>Rearranging:</p> \\[\\frac{d\\langle A \\rangle}{dt} = \\frac{1}{i\\hbar} \\int \\psi^* (\\hat{A}\\hat{H} - \\hat{H}\\hat{A}) \\psi \\, dx + \\left\\langle \\frac{\\partial \\hat{A}}{\\partial t} \\right\\rangle\\] <p>This gives us Ehrenfest's Theorem:</p> \\[\\boxed{\\frac{d\\langle A \\rangle}{dt} = \\frac{1}{i\\hbar}\\langle[\\hat{A}, \\hat{H}]\\rangle + \\left\\langle \\frac{\\partial \\hat{A}}{\\partial t} \\right\\rangle}\\] <p>Physical meaning: The rate of change of any expectation value depends on (1) how the operator fails to commute with the Hamiltonian, and (2) any explicit time dependence of the operator itself.</p>"},{"location":"quantum-mechanics/quantum-velocity/#part-2-deriving-the-velocity-operator","title":"Part 2: Deriving the Velocity Operator","text":"<p>Now let's apply Ehrenfest's theorem to the position operator \\(\\hat{x}\\) to find what \"velocity\" means quantum mechanically.</p>"},{"location":"quantum-mechanics/quantum-velocity/#applying-to-position","title":"Applying to Position","text":"<p>Set \\(\\hat{A} = \\hat{x}\\). Since \\(\\hat{x}\\) has no explicit time dependence (\\(\\frac{\\partial \\hat{x}}{\\partial t} = 0\\)):</p> \\[\\frac{d\\langle x \\rangle}{dt} = \\frac{1}{i\\hbar}\\langle[\\hat{x}, \\hat{H}]\\rangle\\] <p>Now we need to compute the commutator \\([\\hat{x}, \\hat{H}]\\) for different Hamiltonians.</p>"},{"location":"quantum-mechanics/quantum-velocity/#case-1-free-particle-v-0","title":"Case 1: Free Particle (\\(V = 0\\))","text":"<p>For a free particle: \\(\\hat{H} = \\frac{\\hat{p}^2}{2m}\\)</p> <p>Calculate the commutator:</p> \\[[\\hat{x}, \\hat{H}] = \\left[\\hat{x}, \\frac{\\hat{p}^2}{2m}\\right] = \\frac{1}{2m}[\\hat{x}, \\hat{p}^2]\\] <p>To evaluate \\([\\hat{x}, \\hat{p}^2]\\), use the identity \\([\\hat{A}, \\hat{B}\\hat{C}] = \\hat{B}[\\hat{A}, \\hat{C}] + [\\hat{A}, \\hat{B}]\\hat{C}\\):</p> \\[[\\hat{x}, \\hat{p}^2] = [\\hat{x}, \\hat{p}\\cdot\\hat{p}] = \\hat{p}[\\hat{x}, \\hat{p}] + [\\hat{x}, \\hat{p}]\\hat{p}\\] <p>Using the canonical commutation relation \\([\\hat{x}, \\hat{p}] = i\\hbar\\):</p> \\[[\\hat{x}, \\hat{p}^2] = \\hat{p}(i\\hbar) + (i\\hbar)\\hat{p} = 2i\\hbar\\hat{p}\\] <p>Therefore:</p> \\[[\\hat{x}, \\hat{H}] = \\frac{1}{2m}(2i\\hbar\\hat{p}) = \\frac{i\\hbar\\hat{p}}{m}\\] <p>Substituting back into Ehrenfest's theorem:</p> \\[\\frac{d\\langle x \\rangle}{dt} = \\frac{1}{i\\hbar} \\cdot \\frac{i\\hbar\\langle\\hat{p}\\rangle}{m} = \\frac{\\langle\\hat{p}\\rangle}{m}\\]"},{"location":"quantum-mechanics/quantum-velocity/#case-2-particle-in-a-potential-v-neq-0","title":"Case 2: Particle in a Potential (\\(V \\neq 0\\))","text":"<p>For a particle in a potential: \\(\\hat{H} = \\frac{\\hat{p}^2}{2m} + V(\\hat{x})\\)</p> \\[[\\hat{x}, \\hat{H}] = \\left[\\hat{x}, \\frac{\\hat{p}^2}{2m}\\right] + [\\hat{x}, V(\\hat{x})]\\] <p>The first term we already calculated. For the second term:</p> \\[[\\hat{x}, V(\\hat{x})] = \\hat{x}V(\\hat{x}) - V(\\hat{x})\\hat{x} = 0\\] <p>This is zero because \\(\\hat{x}\\) commutes with any function of itself! Position doesn't care what potential it's in\u2014it's still just position.</p> <p>Therefore:</p> \\[[\\hat{x}, \\hat{H}] = \\frac{i\\hbar\\hat{p}}{m}\\] <p>And again:</p> \\[\\frac{d\\langle x \\rangle}{dt} = \\frac{\\langle\\hat{p}\\rangle}{m}\\]"},{"location":"quantum-mechanics/quantum-velocity/#the-velocity-operator","title":"The Velocity Operator","text":"<p>Since \\(\\frac{d\\langle x \\rangle}{dt}\\) represents the average velocity, and we found it equals \\(\\frac{\\langle\\hat{p}\\rangle}{m}\\), we identify the velocity operator:</p> \\[\\boxed{\\hat{v} = \\frac{\\hat{p}}{m}}\\] <p>This is remarkable: The velocity operator is \\(\\hat{p}/m\\) whether the particle is free or in a potential\u2014the potential doesn't appear!</p>"},{"location":"quantum-mechanics/quantum-velocity/#physical-interpretation","title":"Physical Interpretation","text":"<p>This matches classical intuition perfectly:</p> <ul> <li>The velocity at any instant depends only on the momentum at that instant</li> <li>The potential \\(V(x)\\) affects the acceleration (how velocity changes over time), not the instantaneous velocity</li> <li>In classical mechanics: \\(v = p/m\\) regardless of forces. Same in quantum mechanics!</li> </ul>"},{"location":"quantum-mechanics/quantum-velocity/#part-3-wave-velocitiesphase-and-group","title":"Part 3: Wave Velocities\u2014Phase and Group","text":"<p>When we have propagating waves, two more velocity concepts emerge from the wave nature of the solutions.</p>"},{"location":"quantum-mechanics/quantum-velocity/#phase-velocity-speed-of-the-crests","title":"Phase Velocity: Speed of the Crests","text":"<p>For a plane wave \\(\\psi = Ae^{i(kx - \\omega t)}\\), surfaces of constant phase satisfy:</p> \\[kx - \\omega t = \\text{constant}\\] <p>Differentiating: \\(k\\,dx - \\omega\\,dt = 0\\), so:</p> \\[v_{\\text{ph}} = \\frac{dx}{dt} = \\frac{\\omega}{k}\\] <p>What it means: This is the speed at which a particular wave crest (or trough) moves through space.</p> <p>For a free quantum particle with \\(E = \\hbar\\omega\\) and \\(p = \\hbar k\\):</p> \\[\\omega = \\frac{E}{\\hbar} = \\frac{p^2}{2m\\hbar} = \\frac{\\hbar k^2}{2m}\\] <p>So:</p> \\[v_{\\text{ph}} = \\frac{\\omega}{k} = \\frac{\\hbar k}{2m} = \\frac{p}{2m}\\] <p>Note: This is half the classical velocity! Phase velocity alone doesn't tell you how fast the particle moves.</p>"},{"location":"quantum-mechanics/quantum-velocity/#group-velocity-speed-of-the-envelope","title":"Group Velocity: Speed of the Envelope","text":"<p>A wave packet is a superposition of many plane waves:</p> \\[\\psi(x,t) = \\int A(k) e^{i(kx - \\omega(k) t)} dk\\] <p>If the packet is peaked around \\(k_0\\), the envelope moves at:</p> \\[v_g = \\frac{d\\omega}{dk}\\bigg|_{k=k_0}\\] <p>Derivation sketch: Expand \\(\\omega(k)\\) around \\(k_0\\): \\(\\omega(k) \\approx \\omega_0 + \\omega'_0(k-k_0)\\). The phase becomes:</p> \\[kx - \\omega t \\approx k_0 x - \\omega_0 t + (k-k_0)(x - \\omega'_0 t)\\] <p>The envelope (the \\(A(k)\\) contribution) depends on \\((x - \\omega'_0 t)\\), so it moves at speed \\(\\omega'_0 = d\\omega/dk\\).</p> <p>For a free particle:</p> \\[v_g = \\frac{d\\omega}{dk} = \\frac{d}{dk}\\left(\\frac{\\hbar k^2}{2m}\\right) = \\frac{\\hbar k}{m} = \\frac{p}{m}\\] <p>This matches the classical velocity! The group velocity tells you how fast the probability \"bump\" actually moves.</p>"},{"location":"quantum-mechanics/quantum-velocity/#the-key-relationship","title":"The Key Relationship","text":"<p>For a free particle:</p> \\[v_g = 2 \\, v_{\\text{ph}} = \\frac{p}{m} = \\langle \\hat{v} \\rangle\\] <p>The group velocity equals the expectation value of the velocity operator\u2014this is the \"real\" speed of the particle.</p>"},{"location":"quantum-mechanics/quantum-velocity/#part-4-when-does-each-velocity-apply","title":"Part 4: When Does Each Velocity Apply?","text":"<p>This is the crucial practical knowledge. Here's when each concept is meaningful:</p>"},{"location":"quantum-mechanics/quantum-velocity/#velocity-operator-hatv-hatpm","title":"Velocity Operator \\(\\hat{v} = \\hat{p}/m\\)","text":"<p>Always exists. It's an operator\u2014a mathematical object that acts on wavefunctions.</p> <ul> <li>What it does: When acting on \\(\\psi\\), it transforms the wavefunction according to momentum/mass</li> <li>Eigenvalue equation: \\(\\hat{v}\\psi = v\\psi\\) only when \\(\\psi\\) is a momentum eigenstate (plane wave)</li> <li>General case: \\(\\hat{v}\\psi\\) gives some other function, not a multiple of \\(\\psi\\)</li> </ul>"},{"location":"quantum-mechanics/quantum-velocity/#group-velocity-v_g-domegadk","title":"Group Velocity \\(v_g = d\\omega/dk\\)","text":"<p>Requires: Propagating waves with a dispersion relation \\(\\omega(k)\\)</p> <p>Works for: - Free particles (wave packets) - Bloch waves in crystals: \\(v_g = \\frac{1}{\\hbar}\\frac{dE_n}{dk}\\) - Waveguides, phonons, any dispersive medium</p> <p>Does NOT work for: - Standing waves (particle in a box) - Bound states with discrete spectra - Single plane waves (need a packet!)</p>"},{"location":"quantum-mechanics/quantum-velocity/#phase-velocity-v_textph-omegak","title":"Phase Velocity \\(v_{\\text{ph}} = \\omega/k\\)","text":"<p>Requires: A single plane wave with definite \\(k\\)</p> <ul> <li>More of a mathematical curiosity in quantum mechanics</li> <li>Doesn't represent particle motion</li> <li>Can exceed the speed of light without violating relativity (no information transfer)</li> </ul>"},{"location":"quantum-mechanics/quantum-velocity/#expectation-velocity-langle-hatv-rangle-dlangle-x-rangledt","title":"Expectation Velocity \\(\\langle \\hat{v} \\rangle = d\\langle x \\rangle/dt\\)","text":"<p>Requires: Normalizable wavefunction</p> <ul> <li>This is the \"average velocity\" from Ehrenfest's theorem</li> <li>Equals \\(\\langle \\hat{p} \\rangle / m\\) for any potential</li> <li>The closest quantum analog to classical velocity</li> </ul>"},{"location":"quantum-mechanics/quantum-velocity/#part-5-quick-reference-tables","title":"Part 5: Quick Reference Tables","text":""},{"location":"quantum-mechanics/quantum-velocity/#table-1-the-four-velocity-concepts","title":"Table 1: The Four Velocity Concepts","text":"Concept Type Definition Formula Requirements Velocity Operator OPERATOR Heisenberg equation of motion \\(\\hat{v} = \\frac{\\hat{p}}{m}\\) None\u2014always valid Group Velocity NUMBER Envelope speed of wave packet \\(v_g = \\frac{d\\omega}{dk}\\big\\|_{k_0}\\) Dispersion relation, wave packet Phase Velocity NUMBER Crest speed of single wave \\(v_{\\text{ph}} = \\frac{\\omega}{k}\\) Single plane wave Expectation Velocity NUMBER Average position motion \\(v_{\\text{exp}} = \\frac{d\\langle x \\rangle}{dt} = \\frac{\\langle \\hat{p} \\rangle}{m}\\) Normalizable \\(\\psi\\)"},{"location":"quantum-mechanics/quantum-velocity/#table-2-which-velocities-work-in-each-system","title":"Table 2: Which Velocities Work in Each System?","text":"System Wavefunction Operator \\(\\hat{v}\\) Group \\(v_g\\) Phase \\(v_{\\text{ph}}\\) Expectation \\(\\langle \\hat{v} \\rangle\\) Plane wave \\(e^{i(kx-\\omega t)}\\) \u2713 (eigenstate) \u2717 (need packet) \u2713 \u2717 (not normalizable) Free wave packet \\(\\int A(k)e^{ikx}dk\\) \u2713 \u2713 \u2713 (per mode) \u2713 (equals \\(v_g\\)) Particle in box \\(\\sin(n\\pi x/L)\\) \u2713 \u2717 (discrete \\(k\\)) \u2014 \u2713 (equals 0) Harmonic oscillator \\(\\psi_n(x)\\) \u2713 \u2717 (no \\(k\\)) \u2014 \u2713 (equals 0 for \\(\\psi_n\\)) Bloch wave in crystal \\(u_k(x)e^{ikx}\\) \u2713 \u2713 \u2713 \u2713"},{"location":"quantum-mechanics/quantum-velocity/#table-3-concrete-numbers-electron-example","title":"Table 3: Concrete Numbers (Electron Example)","text":"<p>Parameters: \\(m = 9.1 \\times 10^{-31}\\) kg, \\(k = 5 \\times 10^6\\) m\\(^{-1}\\)</p> Quantity Value Calculation Momentum \\(p = \\hbar k = 5.3 \\times 10^{-25}\\) kg\u00b7m/s \\(p = (1.055 \\times 10^{-34})(5 \\times 10^6)\\) Classical/Group velocity \\(v_g = 5.8 \\times 10^5\\) m/s \\(v_g = p/m = \\hbar k/m\\) Phase velocity \\(v_{\\text{ph}} = 2.9 \\times 10^5\\) m/s \\(v_{\\text{ph}} = \\hbar k/(2m) = v_g/2\\) Eigenvalue of \\(\\hat{v}\\) \\(5.8 \\times 10^5\\) m/s Same as \\(v_g\\) (for plane wave)"},{"location":"quantum-mechanics/quantum-velocity/#part-6-deep-divewhat-does-the-velocity-operator-actually-do","title":"Part 6: Deep Dive\u2014What Does the Velocity Operator Actually Do?","text":"<p>This is often confusing. Let's be precise:</p>"},{"location":"quantum-mechanics/quantum-velocity/#acting-on-an-eigenstate","title":"Acting on an Eigenstate","text":"<p>If \\(\\psi\\) is a momentum eigenstate (plane wave \\(e^{ikx}\\)):</p> \\[\\hat{v}\\psi = \\frac{\\hat{p}}{m}\\psi = \\frac{\\hbar k}{m}\\psi = v_{\\text{eigen}}\\psi\\] <p>The wavefunction just gets multiplied by a number\u2014the velocity eigenvalue. This is the only case where the velocity \"is\" a definite number.</p>"},{"location":"quantum-mechanics/quantum-velocity/#acting-on-a-general-state","title":"Acting on a General State","text":"<p>For anything else (Gaussian packet, bound state, etc.):</p> \\[\\hat{v}\\psi = \\frac{\\hat{p}}{m}\\psi = \\frac{-i\\hbar}{m}\\frac{\\partial \\psi}{\\partial x}\\] <p>This gives a different function, not a multiple of \\(\\psi\\). The particle doesn't have a definite velocity!</p>"},{"location":"quantum-mechanics/quantum-velocity/#measurement","title":"Measurement","text":"<p>When you measure velocity: - You get one eigenvalue randomly - Probability of getting \\(v\\) is \\(|\\langle v | \\psi \\rangle|^2\\) - After measurement, the state collapses to that eigenstate - Repeated measurements give the average \\(\\langle \\hat{v} \\rangle\\)</p>"},{"location":"quantum-mechanics/quantum-velocity/#summary","title":"Summary","text":"<p>Think of the velocity operator as a \"velocity measuring machine recipe\": - It transforms wavefunctions according to momentum - Only eigenstates give definite velocity values - For general states, you get probabilities and averages</p>"},{"location":"quantum-mechanics/quantum-velocity/#part-7-intuition-summary","title":"Part 7: Intuition Summary","text":"<p>When you see \"velocity\" in a QM problem, ask:</p> <ol> <li> <p>Is it an operator? \u2192 Then it's \\(\\hat{v} = \\hat{p}/m\\), valid always</p> </li> <li> <p>Is it a number describing wave propagation?</p> </li> <li>Envelope speed \u2192 Group velocity \\(v_g = d\\omega/dk\\)</li> <li> <p>Crest speed \u2192 Phase velocity \\(v_{\\text{ph}} = \\omega/k\\)</p> </li> <li> <p>Is it \"how fast is the particle moving on average\"? \u2192 Expectation value \\(\\langle \\hat{v} \\rangle = \\langle \\hat{p} \\rangle / m\\)</p> </li> <li> <p>Is the particle in a potential? \u2192 Doesn't matter! All these definitions still apply the same way.</p> </li> </ol> <p>The elegant unity: For a free particle wave packet, all the \"number\" velocities agree:</p> \\[v_g = \\langle \\hat{v} \\rangle = \\frac{p_0}{m}\\] <p>And this is exactly what classical mechanics says: \\(v = p/m\\).</p>"},{"location":"quantum-mechanics/quantum-velocity/#appendix-extensions-to-crystals","title":"Appendix: Extensions to Crystals","text":"<p>In a periodic potential (crystal), electrons are described by Bloch waves with band structure \\(E_n(k)\\). The group velocity becomes:</p> \\[v_g = \\frac{1}{\\hbar}\\frac{dE_n}{dk}\\] <p>This is crucial in solid-state physics: - Near band edges, \\(dE/dk \\approx 0\\), so electrons slow down - Effective mass: \\(m^* = \\hbar^2 / (d^2E/dk^2)\\) can be negative! - The velocity operator still applies, but now includes crystal momentum</p> <p>The key insight: group velocity works whenever you have propagating waves with a dispersion relation\u2014not just free particles.</p> <p>This page was written with the assistance of Claude (Anthropic).</p>"}]}